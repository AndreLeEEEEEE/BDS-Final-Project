{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Classification Plan</u>\n",
    "- 1st Training phase: Train a classifier on the first two labeled years of the data\n",
    "- 2nd Training phase: Use the classifier and batch active learning on the rest of the unlabeled data until 2021. Examples that would provide the most information will be chosen to get their true label. The remaining examples will get pseudo-labeled\n",
    "- 1st Evaluate phase: Use the newly trained classifier to evaluate the data from 2021 and 2022\n",
    "- Predict phase: Use time series forecasting (RNN) to predict a country's set of features until 2050\n",
    "- 2nd Evaluate phase: Use the classifier to predict levels of CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Active Learning\n",
    "def train_labeled(classifier, lab_data, lab_label, epoch):\n",
    "    \"\"\"Train on the current labeled dataset\n",
    "\n",
    "    Parameters:\n",
    "    - classifier (classifier type): classifier-in-training\n",
    "    - lab_data (ndarray): current labeled data, shape=(# of years, 83, 9)\n",
    "    - lab_label (ndarray): labels for current labeled data, shape=(# of years, 83)\n",
    "    - epoch (int): cycles to run the training for\n",
    "\n",
    "    Returns:\n",
    "    - classifier (classifier type): trained classifier\n",
    "    \"\"\"\n",
    "    for _ in range(epoch):\n",
    "        # Labeled examples have to be concatenated because not every\n",
    "        # year's worth of data contains examples of every class\n",
    "        # ex. If the classifier trains on data belonging to\n",
    "        # only 8/11 classes, predict_proba() will only return\n",
    "        # probabilities for these 8/11 classes and will ignore\n",
    "        # the possibility of the 3 others\n",
    "        labeled_examples = np.concatenate(lab_data, axis=0)\n",
    "        labels = np.concatenate(lab_label, axis=0)\n",
    "        # labeled_examples.shape = (# of years * 83, 9)\n",
    "        # labels.shape = (# of years * 83,)\n",
    "        classifier.fit(labeled_examples, labels)\n",
    "\n",
    "    return classifier\n",
    "\n",
    "def predict_unlabeled(classifier, batch_data):\n",
    "    \"\"\"Predict on the unlabeled data of a year\n",
    "    \n",
    "    Parameters:\n",
    "    - classifier (classifier type): a trained classifier\n",
    "    - batch_data (ndarray): a year's worth of unlabeled data, shape=(83, 9)\n",
    "\n",
    "    Returns:\n",
    "    - pred_class (ndarray): array of predicted classes, shape=(83,)\n",
    "    - pred_proba (ndarray): array of array of class probabilities, shape=(83, 6)\n",
    "    \"\"\"\n",
    "    pred_class = np.array(classifier.predict(batch_data))\n",
    "    pred_proba = np.array(classifier.predict_proba(batch_data))\n",
    "\n",
    "    return pred_class, pred_proba\n",
    "    \n",
    "def batch_active_learning(classifier, lab_data, lab_label, unlab_data, unlab_label, confident_threshold, epoch):\n",
    "    \"\"\"Train a classifier using batch active learning\n",
    "    \n",
    "    Parameters:\n",
    "    - classifier: a classifier from the scikit-learn (sklearn) module \n",
    "    - lab_data (ndarray): the labeled dataset, inital shape=(3, 83, 9)\n",
    "    - lab_label (ndarray): the labled dataset's labels, inital shape=(3, 83)\n",
    "    - unlab_data (ndarray): the unlabeled dataset, inital shape=(28, 83, 9)\n",
    "    - unlab_label (ndarray): the unlabeled dataset's labels, inital shape=(28, 83)\n",
    "    - confident_threshold (float): threshold for the algorithm to request labels\n",
    "    - epoch (int): number of epoches training will last for\n",
    "\n",
    "    Returns:\n",
    "    - classifier (classifier type): trained classifier\n",
    "    \"\"\"\n",
    "\n",
    "    index = 0\n",
    "    episode = 1\n",
    "    # classifier = train_labeled(classifier, lab_data, lab_label, epoch)\n",
    "    while index < 28:\n",
    "        print(f\"Episode {episode}: \")\n",
    "\n",
    "        classifier = train_labeled(classifier, lab_data, lab_label, epoch)\n",
    "\n",
    "        # Predict on the next batch of unlabeled data\n",
    "        # 1 year is a batch\n",
    "        # 4 batches per episode\n",
    "        batch_data = []\n",
    "        batch_label = []\n",
    "        for modifier in range(4):\n",
    "            batch_data.append(unlab_data[index + modifier])\n",
    "            batch_label.append(unlab_label[index + modifier])\n",
    "\n",
    "        # np.shape(batch_data) = (4, 83, 9)\n",
    "        # np.shape(batch_label) = (4, 83)\n",
    "\n",
    "        pred_class = []\n",
    "        pred_proba = []\n",
    "        for batch in batch_data:\n",
    "            prediction_class, pred_probability = predict_unlabeled(classifier, batch)\n",
    "            pred_class.append(prediction_class)\n",
    "            pred_proba.append(pred_probability)\n",
    "        print(f\"score: {classifier.score(np.concatenate(batch_data, axis=0), np.concatenate(batch_label, axis=0))}\")\n",
    "\n",
    "        # np.shape(pred_class) = (4, 83)\n",
    "        # np.shape(pred_proba) = (4, 83, 6)\n",
    "\n",
    "        # Choose which examples to request a true label for\n",
    "        # For these examples, replace their predicted label with their true label\n",
    "        # Remember that the order of examples in pred_class, pred_proba, batch_data, and batch_label are the same\n",
    "        # Ex. The label information of the example at index 0 of batch_data is found at index 0 of the other arrays\n",
    "        uncertain = 0\n",
    "        # 4 cycles\n",
    "        for i, batch_proba in enumerate(pred_proba):\n",
    "            # 83 cycles\n",
    "            for j, probas in enumerate(batch_proba):\n",
    "                pred = np.max(probas)\n",
    "                if pred < confident_threshold:\n",
    "                    uncertain += 1\n",
    "                    pred_class[i][j] = batch_label[i][j]\n",
    "\n",
    "        print(f\"{uncertain} label request(s) made\")\n",
    "\n",
    "        # Reshape batch_data and pred_class for np.append()\n",
    "        # rbatch_data = np.reshape(batch_data, (1, 83, 9))\n",
    "        # rpred_class = np.reshape(pred_class, (1, 83))\n",
    "\n",
    "        # classifier = train_labeled(classifier, batch_data, pred_class, epoch)\n",
    "\n",
    "        # Add the newly pseudo-labeled, and any true-labeled, examples to the labeled data set\n",
    "        lab_data = np.append(lab_data, batch_data, axis=0)\n",
    "        lab_label = np.append(lab_label, pred_class, axis=0)\n",
    "\n",
    "        index += 4\n",
    "        episode += 1\n",
    "\n",
    "    # Train one last time with all the passed examples, labeled and pseudo-labeled\n",
    "    classifier = train_labeled(classifier, lab_data, lab_label, epoch)\n",
    "        \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch active learning hyperparameters aka model parameters\n",
    "# These are different from real model parameters that are estimated by the model itself\n",
    "\n",
    "n_estimators = 1000\n",
    "max_iter = 1000\n",
    "learning_rate = 0.01\n",
    "max_depth = 50\n",
    "confident_threshold = 0.70\n",
    "epoch = 1\n",
    "n_classes = 11\n",
    "n_queries = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Training phase: Train a classifier on the first two labeled years of the data\n",
    "# 2nd Training phase: Use the classifier and batch active learning on the rest of the unlabeled data until 2021. Examples that would provide the most \n",
    "\n",
    "\n",
    "# Gaussian Naive Bayes isn't an option because the data distribution isn't gaussian/normal due to lacking a \"symmetric bell shape\". \n",
    "# Most of the data labels are on the high end of the scale. Thus, the data's bell shape isn't symmetric\n",
    "# Bernoulli Naive Bayes isn't an option because sample features must be binary-valued (Bernoulli, boolean)\n",
    "# Multinomial, Complement, and Categorical aren't considered  due to data being classified moreso out of probability rather than certainty.\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=n_estimators, criterion=\"log_loss\", max_depth=max_depth)\n",
    "\n",
    "# classifier = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
    "\n",
    "classifier = batch_active_learning(classifier, \n",
    "                                   np.copy(lab_set), np.copy(lab_set_label), \n",
    "                                   np.copy(unlab_set), np.copy(unlab_set_label), \n",
    "                                   confident_threshold, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN classifier class\n",
    "class RecurrentNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecurrentNeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=8, out_features=16)\n",
    "        # nn.ReLU() doesn't need parameters in this case\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(in_features=16, out_features=16)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(in_features=16, out_features=16)\n",
    "        self.activation3 = nn.ReLU()\n",
    "        # self.batchNorm = nn.BatchNorm1d()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        # self.dropout1 = nn.Dropout()\n",
    "        self.dense1 = nn.Linear(in_features=16, out_features=1)\n",
    "        # self.dropout2 = nn.Dropout()\n",
    "        # self.dense2 = nn.Linear()\n",
    "        # self.dropout3 = nn.Dropout()\n",
    "        # self.dense3 = nn.Linear()\n",
    "        # self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.activation3(x)\n",
    "        # x = self.batchNorm(x)\n",
    "        # x = self.flatten(x)\n",
    "        # x = self.dropout1(x)\n",
    "        x = self.dense1(x)\n",
    "        # x = self.dropout2(x)\n",
    "        # x = self.dense2(x)\n",
    "        # x = self.dropout3(x)\n",
    "        # x = self.dense3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[[0, 1],\n",
    "               [2, 3],\n",
    "               [4, 5]],\n",
    "              [[6, 7],\n",
    "               [8, 9],\n",
    "               [10, 11]],\n",
    "              [[12, 13],\n",
    "               [14, 15],\n",
    "               [16, 17]],\n",
    "              [[18, 19],\n",
    "               [20, 21],\n",
    "               [22, 23]],\n",
    "              [[24, 25],\n",
    "               [26, 27],\n",
    "               [28, 29]]])\n",
    "\n",
    "print(a.shape)\n",
    "\n",
    "a = np.transpose(a, [1, 0, 2])\n",
    "\n",
    "print(a.shape)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random((2, 4))\n",
    "print(x)\n",
    "y = x[:, 0:2]\n",
    "print(y)\n",
    "z = x[:, 2:]\n",
    "print(z)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
