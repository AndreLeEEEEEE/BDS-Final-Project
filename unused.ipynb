{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Classification Plan</u>\n",
    "- 1st Training phase: Train a classifier on the first two labeled years of the data\n",
    "- 2nd Training phase: Use the classifier and batch active learning on the rest of the unlabeled data until 2021. Examples that would provide the most information will be chosen to get their true label. The remaining examples will get pseudo-labeled\n",
    "- 1st Evaluate phase: Use the newly trained classifier to evaluate the data from 2021 and 2022\n",
    "- Predict phase: Use time series forecasting (RNN) to predict a country's set of features until 2050\n",
    "- 2nd Evaluate phase: Use the classifier to predict levels of CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Active Learning\n",
    "def train_labeled(classifier, lab_data, lab_label, epoch):\n",
    "    \"\"\"Train on the current labeled dataset\n",
    "\n",
    "    Parameters:\n",
    "    - classifier (classifier type): classifier-in-training\n",
    "    - lab_data (ndarray): current labeled data, shape=(# of years, 83, 9)\n",
    "    - lab_label (ndarray): labels for current labeled data, shape=(# of years, 83)\n",
    "    - epoch (int): cycles to run the training for\n",
    "\n",
    "    Returns:\n",
    "    - classifier (classifier type): trained classifier\n",
    "    \"\"\"\n",
    "    for _ in range(epoch):\n",
    "        # Labeled examples have to be concatenated because not every\n",
    "        # year's worth of data contains examples of every class\n",
    "        # ex. If the classifier trains on data belonging to\n",
    "        # only 8/11 classes, predict_proba() will only return\n",
    "        # probabilities for these 8/11 classes and will ignore\n",
    "        # the possibility of the 3 others\n",
    "        labeled_examples = np.concatenate(lab_data, axis=0)\n",
    "        labels = np.concatenate(lab_label, axis=0)\n",
    "        # labeled_examples.shape = (# of years * 83, 9)\n",
    "        # labels.shape = (# of years * 83,)\n",
    "        classifier.fit(labeled_examples, labels)\n",
    "\n",
    "    return classifier\n",
    "\n",
    "def predict_unlabeled(classifier, batch_data):\n",
    "    \"\"\"Predict on the unlabeled data of a year\n",
    "    \n",
    "    Parameters:\n",
    "    - classifier (classifier type): a trained classifier\n",
    "    - batch_data (ndarray): a year's worth of unlabeled data, shape=(83, 9)\n",
    "\n",
    "    Returns:\n",
    "    - pred_class (ndarray): array of predicted classes, shape=(83,)\n",
    "    - pred_proba (ndarray): array of array of class probabilities, shape=(83, 6)\n",
    "    \"\"\"\n",
    "    pred_class = np.array(classifier.predict(batch_data))\n",
    "    pred_proba = np.array(classifier.predict_proba(batch_data))\n",
    "\n",
    "    return pred_class, pred_proba\n",
    "    \n",
    "def batch_active_learning(classifier, lab_data, lab_label, unlab_data, unlab_label, confident_threshold, epoch):\n",
    "    \"\"\"Train a classifier using batch active learning\n",
    "    \n",
    "    Parameters:\n",
    "    - classifier: a classifier from the scikit-learn (sklearn) module \n",
    "    - lab_data (ndarray): the labeled dataset, inital shape=(3, 83, 9)\n",
    "    - lab_label (ndarray): the labled dataset's labels, inital shape=(3, 83)\n",
    "    - unlab_data (ndarray): the unlabeled dataset, inital shape=(28, 83, 9)\n",
    "    - unlab_label (ndarray): the unlabeled dataset's labels, inital shape=(28, 83)\n",
    "    - confident_threshold (float): threshold for the algorithm to request labels\n",
    "    - epoch (int): number of epoches training will last for\n",
    "\n",
    "    Returns:\n",
    "    - classifier (classifier type): trained classifier\n",
    "    \"\"\"\n",
    "\n",
    "    index = 0\n",
    "    episode = 1\n",
    "    # classifier = train_labeled(classifier, lab_data, lab_label, epoch)\n",
    "    while index < 28:\n",
    "        print(f\"Episode {episode}: \")\n",
    "\n",
    "        classifier = train_labeled(classifier, lab_data, lab_label, epoch)\n",
    "\n",
    "        # Predict on the next batch of unlabeled data\n",
    "        # 1 year is a batch\n",
    "        # 4 batches per episode\n",
    "        batch_data = []\n",
    "        batch_label = []\n",
    "        for modifier in range(4):\n",
    "            batch_data.append(unlab_data[index + modifier])\n",
    "            batch_label.append(unlab_label[index + modifier])\n",
    "\n",
    "        # np.shape(batch_data) = (4, 83, 9)\n",
    "        # np.shape(batch_label) = (4, 83)\n",
    "\n",
    "        pred_class = []\n",
    "        pred_proba = []\n",
    "        for batch in batch_data:\n",
    "            prediction_class, pred_probability = predict_unlabeled(classifier, batch)\n",
    "            pred_class.append(prediction_class)\n",
    "            pred_proba.append(pred_probability)\n",
    "        print(f\"score: {classifier.score(np.concatenate(batch_data, axis=0), np.concatenate(batch_label, axis=0))}\")\n",
    "\n",
    "        # np.shape(pred_class) = (4, 83)\n",
    "        # np.shape(pred_proba) = (4, 83, 6)\n",
    "\n",
    "        # Choose which examples to request a true label for\n",
    "        # For these examples, replace their predicted label with their true label\n",
    "        # Remember that the order of examples in pred_class, pred_proba, batch_data, and batch_label are the same\n",
    "        # Ex. The label information of the example at index 0 of batch_data is found at index 0 of the other arrays\n",
    "        uncertain = 0\n",
    "        # 4 cycles\n",
    "        for i, batch_proba in enumerate(pred_proba):\n",
    "            # 83 cycles\n",
    "            for j, probas in enumerate(batch_proba):\n",
    "                pred = np.max(probas)\n",
    "                if pred < confident_threshold:\n",
    "                    uncertain += 1\n",
    "                    pred_class[i][j] = batch_label[i][j]\n",
    "\n",
    "        print(f\"{uncertain} label request(s) made\")\n",
    "\n",
    "        # Reshape batch_data and pred_class for np.append()\n",
    "        # rbatch_data = np.reshape(batch_data, (1, 83, 9))\n",
    "        # rpred_class = np.reshape(pred_class, (1, 83))\n",
    "\n",
    "        # classifier = train_labeled(classifier, batch_data, pred_class, epoch)\n",
    "\n",
    "        # Add the newly pseudo-labeled, and any true-labeled, examples to the labeled data set\n",
    "        lab_data = np.append(lab_data, batch_data, axis=0)\n",
    "        lab_label = np.append(lab_label, pred_class, axis=0)\n",
    "\n",
    "        index += 4\n",
    "        episode += 1\n",
    "\n",
    "    # Train one last time with all the passed examples, labeled and pseudo-labeled\n",
    "    classifier = train_labeled(classifier, lab_data, lab_label, epoch)\n",
    "        \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch active learning hyperparameters aka model parameters\n",
    "# These are different from real model parameters that are estimated by the model itself\n",
    "\n",
    "n_estimators = 1000\n",
    "max_iter = 1000\n",
    "learning_rate = 0.01\n",
    "max_depth = 50\n",
    "confident_threshold = 0.70\n",
    "epoch = 1\n",
    "n_classes = 11\n",
    "n_queries = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Training phase: Train a classifier on the first two labeled years of the data\n",
    "# 2nd Training phase: Use the classifier and batch active learning on the rest of the unlabeled data until 2021. Examples that would provide the most \n",
    "\n",
    "\n",
    "# Gaussian Naive Bayes isn't an option because the data distribution isn't gaussian/normal due to lacking a \"symmetric bell shape\". \n",
    "# Most of the data labels are on the high end of the scale. Thus, the data's bell shape isn't symmetric\n",
    "# Bernoulli Naive Bayes isn't an option because sample features must be binary-valued (Bernoulli, boolean)\n",
    "# Multinomial, Complement, and Categorical aren't considered  due to data being classified moreso out of probability rather than certainty.\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=n_estimators, criterion=\"log_loss\", max_depth=max_depth)\n",
    "\n",
    "# classifier = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
    "\n",
    "classifier = batch_active_learning(classifier, \n",
    "                                   np.copy(lab_set), np.copy(lab_set_label), \n",
    "                                   np.copy(unlab_set), np.copy(unlab_set_label), \n",
    "                                   confident_threshold, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN classifier class\n",
    "class RecurrentNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecurrentNeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=8, out_features=16)\n",
    "        # nn.ReLU() doesn't need parameters in this case\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(in_features=16, out_features=16)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(in_features=16, out_features=16)\n",
    "        self.activation3 = nn.ReLU()\n",
    "        # self.batchNorm = nn.BatchNorm1d()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        # self.dropout1 = nn.Dropout()\n",
    "        self.dense1 = nn.Linear(in_features=16, out_features=1)\n",
    "        # self.dropout2 = nn.Dropout()\n",
    "        # self.dense2 = nn.Linear()\n",
    "        # self.dropout3 = nn.Dropout()\n",
    "        # self.dense3 = nn.Linear()\n",
    "        # self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.activation3(x)\n",
    "        # x = self.batchNorm(x)\n",
    "        # x = self.flatten(x)\n",
    "        # x = self.dropout1(x)\n",
    "        x = self.dense1(x)\n",
    "        # x = self.dropout2(x)\n",
    "        # x = self.dense2(x)\n",
    "        # x = self.dropout3(x)\n",
    "        # x = self.dense3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[[0, 1],\n",
    "               [2, 3],\n",
    "               [4, 5]],\n",
    "              [[6, 7],\n",
    "               [8, 9],\n",
    "               [10, 11]],\n",
    "              [[12, 13],\n",
    "               [14, 15],\n",
    "               [16, 17]],\n",
    "              [[18, 19],\n",
    "               [20, 21],\n",
    "               [22, 23]],\n",
    "              [[24, 25],\n",
    "               [26, 27],\n",
    "               [28, 29]]])\n",
    "\n",
    "print(a.shape)\n",
    "\n",
    "a = np.transpose(a, [1, 0, 2])\n",
    "\n",
    "print(a.shape)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random((2, 4))\n",
    "print(x)\n",
    "y = x[:, 0:2]\n",
    "print(y)\n",
    "z = x[:, 2:]\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random((2, 3))\n",
    "print(x)\n",
    "y = scaler.fit_transform(x)\n",
    "print(y)\n",
    "z = scaler.inverse_transform(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us = pd.DataFrame(all_tsf[75], columns=[\"Oil\", \"Gas\", \"Coal\", \"Nuclear\", \"Hydro\", \"Solar\", \"Wind\", \"Geothermal\", \"Biofuel\"])\n",
    "x = np.array(us.index) + 1990\n",
    "x2 = np.array(us.index) + 2023\n",
    "y = [36.5] * 33\n",
    "plt.plot(x, us[\"Oil\"])\n",
    "plt.plot(x2, y)\n",
    "# plt.plot(x, us[\"Solar\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.random.random((1, 9))\n",
    "t = np.expand_dims(b, axis=0)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([2, 3, -1, -4, 3])\n",
    "a.clip(min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Andre\\Downloads\\BDS-Final-Project\\env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 251 entries, 0 to 250\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Date       251 non-null    object \n",
      " 1   Open       251 non-null    float64\n",
      " 2   High       251 non-null    float64\n",
      " 3   Low        251 non-null    float64\n",
      " 4   Close      251 non-null    float64\n",
      " 5   Adj Close  251 non-null    float64\n",
      " 6   Volume     251 non-null    int64  \n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 13.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "filename = \"GOOG.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.set_axis(df['Date'])\n",
    "df.drop(columns=['Open', 'High', 'Low', 'Volume'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "close_data = df['Close'].values\n",
    "close_data = close_data.reshape((-1,1))\n",
    "\n",
    "split_percent = 0.80\n",
    "split = int(split_percent*len(close_data))\n",
    "\n",
    "close_train = close_data[:split]\n",
    "close_test = close_data[split:]\n",
    "\n",
    "date_train = df['Date'][:split]\n",
    "date_test = df['Date'][split:]\n",
    "\n",
    "print(len(close_train))\n",
    "print(len(close_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 15\n",
    "\n",
    "train_generator = TimeseriesGenerator(close_train, close_train, length=look_back, batch_size=20)     \n",
    "test_generator = TimeseriesGenerator(close_test, close_test, length=look_back, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Andre\\Downloads\\BDS-Final-Project\\env\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Andre\\Downloads\\BDS-Final-Project\\env\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andre\\AppData\\Local\\Temp\\ipykernel_2168\\3162532282.py:14: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, epochs=num_epochs, verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Andre\\Downloads\\BDS-Final-Project\\env\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "10/10 [==============================] - 1s 3ms/step - loss: 11554.5381\n",
      "Epoch 2/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 11426.9717\n",
      "Epoch 3/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 11293.7666\n",
      "Epoch 4/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 11141.2334\n",
      "Epoch 5/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 10985.4990\n",
      "Epoch 6/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 10815.8350\n",
      "Epoch 7/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 10631.1152\n",
      "Epoch 8/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 10181.3779\n",
      "Epoch 9/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 4897.7959\n",
      "Epoch 10/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 3198.6125\n",
      "Epoch 11/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 8449.4941\n",
      "Epoch 12/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 8191.8394\n",
      "Epoch 13/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 10773.5732\n",
      "Epoch 14/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 5261.1387\n",
      "Epoch 15/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 4056.9382\n",
      "Epoch 16/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 2028.5875\n",
      "Epoch 17/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1388.7838\n",
      "Epoch 18/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1561.7274\n",
      "Epoch 19/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1151.3384\n",
      "Epoch 20/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1195.0916\n",
      "Epoch 21/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 958.1230\n",
      "Epoch 22/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1086.4948\n",
      "Epoch 23/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1017.6371\n",
      "Epoch 24/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1010.2296\n",
      "Epoch 25/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 854.1686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x26477b79ed0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    LSTM(10,\n",
    "        activation='relu',\n",
    "        input_shape=(look_back,1))\n",
    ")\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "num_epochs = 25\n",
    "model.fit_generator(train_generator, epochs=num_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251,)\n",
      "(15,)\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 101.56218719482422\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 86.59120178222656\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 90.86072540283203\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 81.2593994140625\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 91.72211456298828\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 104.33345794677734\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 118.39060974121094\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 137.13905334472656\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 155.01210021972656\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 82.14383697509766\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 69.56270599365234\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 172.47073364257812\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 230.16983032226562\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 215.10752868652344\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 321.79718017578125\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 295.8485107421875\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 342.37359619140625\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 297.29925537109375\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 354.9450988769531\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 164.80857849121094\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 298.98712158203125\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 210.30514526367188\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 417.1631774902344\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 463.4076843261719\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 427.1459655761719\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 493.82073974609375\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 447.45867919921875\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 509.257080078125\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 746.23388671875\n",
      "x.shape: (15,)\n",
      "x.reshape: (1, 15, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1st out.shape: (1, 1)\n",
      "2nd out.shape: ()\n",
      "out: 867.3323974609375\n",
      "prediction_list.shape: (31,)\n"
     ]
    }
   ],
   "source": [
    "close_data = close_data.reshape((-1))\n",
    "\n",
    "print(close_data.shape)\n",
    "\n",
    "def predict(num_prediction, model):\n",
    "    # close_data.shape ~= (500,)\n",
    "    # num_prediction = 30\n",
    "    # look_back = 15\n",
    "    # The last 15 elements of close_data\n",
    "    prediction_list = close_data[-look_back:]\n",
    "    print(prediction_list.shape)\n",
    "    \n",
    "    for _ in range(num_prediction):\n",
    "        x = prediction_list[-look_back:]\n",
    "        print(f\"x.shape: {x.shape}\")\n",
    "        x = x.reshape((1, look_back, 1))\n",
    "        print(f\"x.reshape: {x.shape}\")\n",
    "        out = model.predict(x)\n",
    "        print(f\"1st out.shape: {out.shape}\")\n",
    "        out = out[0][0]\n",
    "        print(f\"2nd out.shape: {out.shape}\")\n",
    "        print(f\"out: {out}\")\n",
    "        prediction_list = np.append(prediction_list, out)\n",
    "    prediction_list = prediction_list[look_back-1:]\n",
    "\n",
    "    print(f\"prediction_list.shape: {prediction_list.shape}\")\n",
    "        \n",
    "    return prediction_list\n",
    "    \n",
    "def predict_dates(num_prediction):\n",
    "    last_date = df['Date'].values[-1]\n",
    "    prediction_dates = pd.date_range(last_date, periods=num_prediction+1).tolist()\n",
    "    return prediction_dates\n",
    "\n",
    "num_prediction = 30\n",
    "forecast = predict(num_prediction, model)\n",
    "forecast_dates = predict_dates(num_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44]\n",
      "[14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37\n",
      " 38 39 40 41 42 43 44]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.arange(15)\n",
    "print(c)\n",
    "c = np.append(c, (np.arange(30) + 15))\n",
    "print(c)\n",
    "print(c[15-1:])\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[2, 3], [4, 5]])\n",
    "np.save(\"data/w\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.load(\"data/w.npy\")\n",
    "print(type(p))\n",
    "print(p.shape)\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
