{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, LSTM\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from modAL.models import ActiveLearner\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into DataFrames\n",
    "\n",
    "# These two groups of data will be used for label making\n",
    "# Emissions data\n",
    "nrg_emi_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"CO2 Emissions from Energy\", header=2, index_col=0)\n",
    "# The sheet called \"Natural Gas Flaring\" is already a part of the calculations for the sheet called \"CO2 from Flaring\"\n",
    "flar_emi_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"CO2 from Flaring\", header=2, index_col=0)\n",
    "equi_emi_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"CO2e Methane, Process emissions\", header=2, index_col=0)\n",
    "\n",
    "# Renewable energy production data\n",
    "hydro_pro_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Hydro Generation - TWh\", header=2, index_col=0)\n",
    "solar_pro_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Solar Generation - TWh\", header=2, index_col=0)\n",
    "wind_pro_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Wind Generation - TWh\", header=2, index_col=0)\n",
    "geo_pro_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Geo Biomass Other - TWh\", header=2, index_col=0)\n",
    "bio_pro_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Biofuels production - PJ\", header=2, index_col=0, nrows=47)\n",
    "\n",
    "# These three groups of data will be used for the feature sets\n",
    "# Renewable energy consumption data\n",
    "hydro_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Hydro Consumption - EJ\", header=2, index_col=0)\n",
    "solar_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Solar Consumption - EJ\", header=2, index_col=0)\n",
    "wind_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Wind Consumption - EJ\", header=2, index_col=0)\n",
    "geo_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Geo Biomass Other - EJ\", header=2, index_col=0)\n",
    "bio_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Biofuels consumption - PJ\", header=2, index_col=0, nrows=47)\n",
    "\n",
    "# Non-renewable energy consumption data\n",
    "oil_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Oil Consumption - EJ\", header=2, index_col=0)\n",
    "gas_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Gas Consumption - EJ\", header=2, index_col=0)\n",
    "coal_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Coal Consumption - EJ\", header=2, index_col=0)\n",
    "nuc_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Nuclear Consumption - EJ\", header=2, index_col=0)\n",
    "\n",
    "# Total energy consumption data\n",
    "tol_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Primary Energy Consumption\", header=2, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatic data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(df:pd.DataFrame, flag=False):\n",
    "    \"\"\"\n",
    "    Get an excel sheet ready for conversion to numpy arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): a dataframe containing an excel sheet\n",
    "    - flag (boolean): an indicator to convert PJ to EJ instead of kWh\n",
    "    \"\"\"\n",
    "    #------------------------------ \n",
    "    # Remove all irrelevant columns\n",
    "    #------------------------------\n",
    "\n",
    "    # Remove all data from before 1990\n",
    "    # Find the index of the \"1990\" column\n",
    "    drop_indx = list(df.columns).index(1990)\n",
    "    # Get the column labels of all columns left of \"1990\"\n",
    "    drop_cols = [df.columns[num] for num in np.arange(0, drop_indx)]\n",
    "    df = df.drop(columns=drop_cols)\n",
    "\n",
    "    # Remove data on growth-rate and share\n",
    "    # Get the column labels of the target columns\n",
    "    drop_cols = [df.columns[num] for num in [-3, -2, -1]]\n",
    "    df = df.drop(columns=drop_cols)\n",
    "\n",
    "    #---------------------------\n",
    "    # Remove all irrelevant rows\n",
    "    #---------------------------\n",
    "\n",
    "    # Remove all rows with any empty cells\n",
    "    # 0 doesn't make an empty cell\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Remove all \"Total\" and \"Other\" rows\n",
    "    # In addition, OECD, Non-OECD, the EU, and the USSR\n",
    "    # In addition, 8 other countries because they only appear in excel sheets for\n",
    "    # flaring emissions and nothing else. I can't make data samples for non-existent data\n",
    "    # Rationale for removing \"Other\" rows - some countries in some excel sheets appear\n",
    "    # individually, but are lumped into an \"Other\" row in other sheets.\n",
    "    # There's no possible way for me to know which portions of an\n",
    "    # \"Other\" row value belongs to which countries.\n",
    "    drop_rows = []\n",
    "    keywords = [\"Total\", \"Other\", \"OECD\", \"European Union\", \"USSR\", \"Bolivia\", \n",
    "                \"Bahrain\", \"Syria\", \"Yemen\", \"Libya\", \"Nigeria\", \"Brunei\", \"Myanmar\"]\n",
    "    for row in df.index:\n",
    "        # Mark a row for dropping if it contains any of the keywords\n",
    "        if any(keyword in row for keyword in keywords):\n",
    "            drop_rows.append(row)\n",
    "    df = df.drop(index=drop_rows)\n",
    "\n",
    "    # -----------------\n",
    "    # Convert the units\n",
    "    # -----------------\n",
    "\n",
    "    # This section is only performed on emissions data, \n",
    "    # renewable energy production data, and consumed\n",
    "    # biofuel energy data\n",
    "    # All other dataframes have \"Exajoules\" as their name\n",
    "    \n",
    "    # All CO2 data is currently represented as millions of tonnes\n",
    "    # Convert all produced renewable data to kilowatt-hour (kWh)\n",
    "    # 1 kWh = 3600 kJ\n",
    "    # 1 PJ = 1000000000000 kJ\n",
    "    # 1 TWh = 1000000000 kWh\n",
    "\n",
    "    if (df.index.name) == \"Million tonnes of carbon dioxide\":\n",
    "        # Convert to single tonnes\n",
    "        df = df * 1000000\n",
    "    elif df.index.name == \"Terawatt-hours\":\n",
    "        # Convert to kilowatt-hours\n",
    "        df = df * 1000000000\n",
    "    elif df.index.name == \"Petajoules\":\n",
    "        if flag:\n",
    "            # Convert to exajoules\n",
    "            df = df * 0.001\n",
    "        else:\n",
    "            # Convert to kilowatt-hours\n",
    "            df = df * (1000000000000/3600)\n",
    "\n",
    "    return df\n",
    "\n",
    "# tonnes = metric ton = 1000 kg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowIndices(df:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Return the row labels of a pd.DataFrame\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): a dataframe containing an excel sheet\n",
    "    \"\"\"\n",
    "\n",
    "    return [row for row in df.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dataframes\n",
    "\n",
    "# Unit: Tonnes\n",
    "nrg_emi_df = processData(nrg_emi_df)\n",
    "flar_emi_df = processData(flar_emi_df)\n",
    "equi_emi_df = processData(equi_emi_df)\n",
    "\n",
    "# Unit: Kilowatt-hours\n",
    "hydro_pro_df = processData(hydro_pro_df)\n",
    "solar_pro_df = processData(solar_pro_df)\n",
    "wind_pro_df = processData(wind_pro_df)\n",
    "geo_pro_df = processData(geo_pro_df)\n",
    "bio_pro_df = processData(bio_pro_df)\n",
    "\n",
    "# Unit: Exajoules\n",
    "hydro_con_df = processData(hydro_con_df)\n",
    "solar_con_df = processData(solar_con_df)\n",
    "wind_con_df = processData(wind_con_df)\n",
    "geo_con_df = processData(geo_con_df)\n",
    "bio_con_df = processData(bio_con_df, True)\n",
    "\n",
    "# Unit: Exajoules\n",
    "oil_con_df = processData(oil_con_df)\n",
    "gas_con_df = processData(gas_con_df)\n",
    "coal_con_df = processData(coal_con_df)\n",
    "nuc_con_df = processData(nuc_con_df)\n",
    "\n",
    "# Unit: Exajoules\n",
    "tol_con_df = processData(tol_con_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "\n",
    "nrg_emi = nrg_emi_df.to_numpy()\n",
    "flar_emi = flar_emi_df.to_numpy()\n",
    "equi_emi = equi_emi_df.to_numpy()\n",
    "hydro_p = hydro_pro_df.to_numpy()\n",
    "solar_p = solar_pro_df.to_numpy()\n",
    "wind_p = wind_pro_df.to_numpy()\n",
    "geo_p = geo_pro_df.to_numpy()\n",
    "bio_p = bio_pro_df.to_numpy()\n",
    "\n",
    "hydro_c = hydro_con_df.to_numpy()\n",
    "solar_c = solar_con_df.to_numpy()\n",
    "wind_c = wind_con_df.to_numpy()\n",
    "geo_c = geo_con_df.to_numpy()\n",
    "bio_c = bio_con_df.to_numpy()\n",
    "oil_c = oil_con_df.to_numpy()\n",
    "gas_c = gas_con_df.to_numpy()\n",
    "coal_c = coal_con_df.to_numpy()\n",
    "nuc_c = nuc_con_df.to_numpy()\n",
    "\n",
    "tol_c = tol_con_df.to_numpy()\n",
    "\n",
    "# print(len(rowIndices(hydro_con_df)), len(rowIndices(solar_con_df)), len(rowIndices(wind_con_df)), len(rowIndices(geo_con_df)), len(rowIndices(bio_con_df)))\n",
    "# print(len(rowIndices(oil_con_df)), len(rowIndices(gas_con_df)), len(rowIndices(coal_con_df)), len(rowIndices(nuc_con_df)))\n",
    "# print(len(rowIndices(tol_con_df)))\n",
    "\n",
    "# Get row indices of dataframes\n",
    "# There are three unique indices/list of countries\n",
    "\n",
    "# All of these dataframes (and their NDarray equivalents) have 83 indices.\n",
    "# Their row indices are shown in nrg_emi_indices\n",
    "#\n",
    "# nrg_emi_df\n",
    "# equi_emi_df\n",
    "# hydro_pro_df\n",
    "# solar_pro_df\n",
    "# wind_pro_df\n",
    "# geo_pro_df\n",
    "# hydro_con_df\n",
    "# solar_con_df\n",
    "# wind_con_df\n",
    "# geo_con_df\n",
    "# oil_con_df\n",
    "# gas_con_df\n",
    "# coal_con_df\n",
    "# nuc_con_df\n",
    "# tol_con_df\n",
    "\n",
    "# All of these dataframes (and their NDarray equivalents) have 41 indices\n",
    "# Their row indices are shown in flar_emi_indices\n",
    "# \n",
    "# flar_emi_df\n",
    "\n",
    "# All of these dataframes (and their NDarray equivalents) have 24 indices\n",
    "# Their row indices are shown in bio_indices\n",
    "#\n",
    "# bio_pro_df\n",
    "# bio_con_df\n",
    "\n",
    "# The rest of the dataframes share the same index list as nrg_emi_indices\n",
    "nrg_emi_indices = rowIndices(nrg_emi_df)\n",
    "flar_emi_indices = rowIndices(flar_emi_df)\n",
    "bio_indices = rowIndices(bio_pro_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>nrg_emi_indices:</b>\n",
    "\n",
    "['Canada', 'Mexico', 'US', 'Argentina', 'Brazil', 'Chile', 'Colombia', 'Ecuador', 'Peru', 'Trinidad & Tobago', 'Venezuela', 'Central America', 'Austria', 'Belgium', 'Bulgaria', 'Croatia', 'Cyprus', 'Czech Republic', 'Denmark', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Iceland', 'Ireland', 'Italy', 'Latvia', 'Lithuania', 'Luxembourg', 'Netherlands', 'North Macedonia', 'Norway', 'Poland', 'Portugal', 'Romania', 'Slovakia', 'Slovenia', 'Spain', 'Sweden', 'Switzerland', 'Turkey', 'Ukraine', 'United Kingdom', 'Azerbaijan', 'Belarus', 'Kazakhstan', 'Russian Federation', 'Turkmenistan', 'Uzbekistan', 'Iran', 'Iraq', 'Israel', 'Kuwait', 'Oman', 'Qatar', 'Saudi Arabia', 'United Arab Emirates', 'Algeria', 'Egypt', 'Morocco', 'South Africa', 'Eastern Africa', 'Middle Africa', 'Western Africa', 'Australia', 'Bangladesh', 'China', 'China Hong Kong SAR', 'India', 'Indonesia', 'Japan', 'Malaysia', 'New Zealand', 'Pakistan', 'Philippines', 'Singapore', 'South Korea', 'Sri Lanka', 'Taiwan', 'Thailand', 'Vietnam']\n",
    "\n",
    "<b>flar_emi_indices:</b>\n",
    "\n",
    "['Canada', 'Mexico', 'US', 'Argentina', 'Brazil', 'Colombia', 'Peru', 'Trinidad & Tobago', 'Venezuela', 'Denmark', 'Germany', 'Italy', 'Netherlands', 'Norway', 'Poland', 'Romania', 'Ukraine', 'United Kingdom', 'Azerbaijan', 'Kazakhstan', 'Russian Federation', 'Turkmenistan', 'Uzbekistan', 'Iran', 'Iraq', 'Kuwait', 'Oman', 'Qatar', 'Saudi Arabia', 'United Arab Emirates', 'Algeria', 'Egypt', 'Australia', 'Bangladesh', 'China', 'India', 'Indonesia', 'Malaysia', 'Pakistan', 'Thailand', 'Vietnam']\n",
    "\n",
    "<b>biofuel_indices:</b>\n",
    "\n",
    "['Canada', 'Mexico', 'US', 'Argentina', 'Brazil', 'Colombia', 'Austria', 'Belgium', 'Finland', 'France', 'Germany', 'Italy', 'Netherlands', 'Poland', 'Portugal', 'Spain', 'Sweden', 'United Kingdom', 'Australia', 'China', 'India', 'Indonesia', 'South Korea', 'Thailand']\n",
    "\n",
    "<b>Shape of nrg_emi:</b>\n",
    "\n",
    "(83, 33)\n",
    "\n",
    "<b>Columns of every dataframe:</b>\n",
    "\n",
    "Index([1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001,\n",
    "       2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013,\n",
    "       2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022],\n",
    "      dtype='object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Algeria', 'Argentina', 'Australia', 'Austria', 'Azerbaijan', 'Bangladesh', 'Belarus', 'Belgium', 'Brazil', 'Bulgaria', 'Canada', 'Central America', 'Chile', 'China', 'China Hong Kong SAR', 'Colombia', 'Croatia', 'Cyprus', 'Czech Republic', 'Denmark', 'Eastern Africa', 'Ecuador', 'Egypt', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Japan', 'Kazakhstan', 'Kuwait', 'Latvia', 'Lithuania', 'Luxembourg', 'Malaysia', 'Mexico', 'Middle Africa', 'Morocco', 'Netherlands', 'New Zealand', 'North Macedonia', 'Norway', 'Oman', 'Pakistan', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Qatar', 'Romania', 'Russian Federation', 'Saudi Arabia', 'Singapore', 'Slovakia', 'Slovenia', 'South Africa', 'South Korea', 'Spain', 'Sri Lanka', 'Sweden', 'Switzerland', 'Taiwan', 'Thailand', 'Trinidad & Tobago', 'Turkey', 'Turkmenistan', 'US', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'Uzbekistan', 'Venezuela', 'Vietnam', 'Western Africa']\n",
      "lsr shape: (33, 83, 8)\n"
     ]
    }
   ],
   "source": [
    "# Massive 3D numpy array for label making\n",
    "# 1st dimension - Years. 33 years from 1990-2022 (inclusive)\n",
    "# 2nd dimension - Countries/Regions. 83 unique countries/regions\n",
    "# 3rd dimension - Carbon Neutral features. 8 features (in this order): energy emissions, flaring emissions, CO2 equivalent emissions, \n",
    "# hydroelectric production, solar production, wind production, geothermal production, biofuel production\n",
    "# (33, 83, 8)\n",
    "\n",
    "# Find every unique country/region\n",
    "# This is a bit redundant because every country in flar_emi_indices and bio_indices \n",
    "# is already in nrg_emi_indices\n",
    "cotry_reg = list(set(nrg_emi_indices + flar_emi_indices + bio_indices))\n",
    "cotry_reg.sort()\n",
    "print(cotry_reg)\n",
    "\n",
    "dim_1 = []\n",
    "for year_indx in range(33):\n",
    "    dim_2 = []\n",
    "    for area in cotry_reg:\n",
    "        # There's no area check for the upcoming data\n",
    "        # because every area has this data\n",
    "        indx = nrg_emi_indices.index(area)\n",
    "        # Extract a float\n",
    "        a_nrg_emi = nrg_emi[indx][year_indx]\n",
    "        a_equi_emi = equi_emi[indx][year_indx]\n",
    "        a_hydro = hydro_p[indx][year_indx]\n",
    "        a_solar = solar_p[indx][year_indx]\n",
    "        a_wind = wind_p[indx][year_indx]\n",
    "        a_geo = geo_p[indx][year_indx]\n",
    "\n",
    "        if area in flar_emi_indices:\n",
    "            indx = flar_emi_indices.index(area)\n",
    "            # Extract a float\n",
    "            a_flar_emi = flar_emi[indx][year_indx]\n",
    "        else:\n",
    "            a_flar_emi = 0.\n",
    "\n",
    "        if area in bio_indices:\n",
    "            indx = bio_indices.index(area)\n",
    "            # Extract a float\n",
    "            a_biofuel = bio_p[indx][year_indx]\n",
    "        else:\n",
    "            a_biofuel = 0.\n",
    "\n",
    "        # Is also a set of features\n",
    "        dim_3 = [a_nrg_emi,\n",
    "                a_flar_emi,\n",
    "                a_equi_emi,\n",
    "                a_hydro,\n",
    "                a_solar,\n",
    "                a_wind,\n",
    "                a_geo,\n",
    "                a_biofuel]\n",
    "        dim_2.append(dim_3)\n",
    "    dim_1.append(dim_2)\n",
    "\n",
    "# Label Statistical Review \n",
    "# Full of floats\n",
    "lsr = np.array(dim_1)\n",
    "print(f\"lsr shape: {lsr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Algeria', 'Argentina', 'Australia', 'Austria', 'Azerbaijan', 'Bangladesh', 'Belarus', 'Belgium', 'Brazil', 'Bulgaria', 'Canada', 'Central America', 'Chile', 'China', 'China Hong Kong SAR', 'Colombia', 'Croatia', 'Cyprus', 'Czech Republic', 'Denmark', 'Eastern Africa', 'Ecuador', 'Egypt', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Japan', 'Kazakhstan', 'Kuwait', 'Latvia', 'Lithuania', 'Luxembourg', 'Malaysia', 'Mexico', 'Middle Africa', 'Morocco', 'Netherlands', 'New Zealand', 'North Macedonia', 'Norway', 'Oman', 'Pakistan', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Qatar', 'Romania', 'Russian Federation', 'Saudi Arabia', 'Singapore', 'Slovakia', 'Slovenia', 'South Africa', 'South Korea', 'Spain', 'Sri Lanka', 'Sweden', 'Switzerland', 'Taiwan', 'Thailand', 'Trinidad & Tobago', 'Turkey', 'Turkmenistan', 'US', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'Uzbekistan', 'Venezuela', 'Vietnam', 'Western Africa']\n",
      "csr shape: (33, 83, 9)\n",
      "ccsr shape: (2739, 9)\n"
     ]
    }
   ],
   "source": [
    "# Massive 3D numpy array for classification\n",
    "# 1st dimension - Years. 33 years from 1990-2022 (inclusive)\n",
    "# 2nd dimension - Countries/Regions. 83 unique countries/regions\n",
    "# 3rd dimension - Energy Consumption features. 9 features (in this order): oil, gas, coal, nuclear, \n",
    "# hydroelectric, solar, wind, geothermal, biofuel\n",
    "# (33, 83, 9)\n",
    "\n",
    "print(cotry_reg)\n",
    "\n",
    "dim_1 = []\n",
    "for year_indx in range(33):\n",
    "    dim_2 = []\n",
    "    for area in cotry_reg:\n",
    "        # There's no area check for the upcoming data\n",
    "        # because every area has this data\n",
    "        indx = nrg_emi_indices.index(area)\n",
    "        # Extract a float\n",
    "        # tol_consume = tol_c[indx][year_indx]\n",
    "        tol_consume = 1\n",
    "        a_hydro = hydro_c[indx][year_indx] / tol_consume\n",
    "        a_solar = solar_c[indx][year_indx] / tol_consume\n",
    "        a_wind = wind_c[indx][year_indx] / tol_consume\n",
    "        a_geo = geo_c[indx][year_indx] / tol_consume\n",
    "        a_oil = oil_c[indx][year_indx] / tol_consume\n",
    "        a_gas = gas_c[indx][year_indx] / tol_consume\n",
    "        a_coal = coal_c[indx][year_indx] / tol_consume\n",
    "        a_nuc = nuc_c[indx][year_indx] / tol_consume\n",
    "\n",
    "        if area in bio_indices:\n",
    "            indx = bio_indices.index(area)\n",
    "            # Extract a float\n",
    "            a_biofuel = bio_c[indx][year_indx] / tol_consume\n",
    "        else:\n",
    "            a_biofuel = 0.\n",
    "\n",
    "        # Is also a set of features\n",
    "        dim_3 = [a_oil, a_gas, a_coal, a_nuc,\n",
    "                 a_hydro, a_solar, a_wind, a_geo, a_biofuel]\n",
    "        dim_2.append(dim_3)\n",
    "    dim_1.append(dim_2)\n",
    "\n",
    "# Classification Statistical Review \n",
    "# Full of floats\n",
    "csr = np.array(dim_1)\n",
    "print(f\"csr shape: {csr.shape}\")\n",
    "\n",
    "# Concatenated version\n",
    "ccsr = np.concatenate(csr, axis=0)\n",
    "print(f\"ccsr shape: {ccsr.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeLabel(features):\n",
    "    \"\"\"\n",
    "    Make a label for a sample\n",
    "\n",
    "    - features (np.ndarray): set of 8 features\n",
    "    \"\"\"\n",
    "    # Unit: Tonnes of Carbon Dioxide\n",
    "    co2 = np.sum(features[:3])\n",
    "    # Unit: Kilowatt-hours\n",
    "    renewable = np.sum(features[3:])\n",
    "\n",
    "    # Electricity reductions emission factor\n",
    "    # 0.000709 tonnes CO2/kWh\n",
    "    # Unit: Tonnes of Carbon Dioxide\n",
    "    renewable *= 0.000709\n",
    "\n",
    "    # Remaining co2 after being offset by renewable energy production\n",
    "    rem_co2 = max(co2 - renewable, 0)\n",
    "\n",
    "    # if rem_co2 == co2:\n",
    "    #     return 6\n",
    "    if rem_co2 == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        percent = (rem_co2/co2) * 100\n",
    "        # Equivalent to np.floor(percent / 10)\n",
    "        # The label is the tens place of the percentage\n",
    "        # return int(np.floor(percent / 10))\n",
    "        if percent > 0.0 and percent <= 20.0:\n",
    "            label = 1\n",
    "        elif percent > 20.0 and percent <= 40.0:\n",
    "            label = 2\n",
    "        elif percent > 40.0 and percent <= 60.0:\n",
    "            label = 3\n",
    "        elif percent > 60.0 and percent <= 80.0:\n",
    "            label = 4\n",
    "        elif percent > 80.0 and percent <= 100.0:\n",
    "            label = 5\n",
    "        else:\n",
    "            label = 5\n",
    "        return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels shape: (33, 83)\n",
      "clabels shape: (2739,)\n"
     ]
    }
   ],
   "source": [
    "# Make labels for all of the data/samples/examples \n",
    "# An individual feature isn't a example, but a location in a particular year is\n",
    "# Thus, there are 33 * 83 = 2739 examples\n",
    "\n",
    "# There are 6 possible labels, 0-5\n",
    "# 0 means carbon neutral is achieved\n",
    "# 6 means the country is absolutely nowhere near carbon neutrality\n",
    "labels = np.array([[makeLabel(location) for location in year] for year in lsr])\n",
    "print(f\"labels shape: {labels.shape}\")\n",
    "\n",
    "# Concatenated version\n",
    "clabels = np.concatenate(labels, axis=0)\n",
    "print(f\"clabels shape: {clabels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled training subset, Years 1990-1991\n",
      "lab_set shape: (2, 83, 9)\n",
      "lab_set_label shape: (2, 83)\n",
      "clab_set shape: (166, 9)\n",
      "clab_set_label shape: (166,)\n",
      "\n",
      "Unlabeled training subset, Years 1992-2020\n",
      "unlab_set shape: (29, 83, 9)\n",
      "unlab_set_label shape: (29, 83)\n",
      "cunlab_set shape: (2407, 9)\n",
      "cunlab_set_label shape: (2407,)\n",
      "\n",
      "Test subset, Years 2021-2022\n",
      "test_set shape: (2, 83, 9)\n",
      "test_set_label shape: (2, 83)\n",
      "ctest_set shape: (166, 9)\n",
      "ctest_set_label shape: (166,)\n"
     ]
    }
   ],
   "source": [
    "# Split the big dataset into a three subsets: labeled training, unlabeled training, and test\n",
    "\n",
    "# Labeled training subset\n",
    "# Years 1990-1991\n",
    "lab_set = csr[0:2]\n",
    "lab_set_label = labels[0:2]\n",
    "\n",
    "# Unlabeled training subset\n",
    "# aka Pool\n",
    "# Years 1992-2020\n",
    "unlab_set = csr[2:31]\n",
    "unlab_set_label = labels[2:31]\n",
    "\n",
    "# Test subset\n",
    "# Years 2021-2022\n",
    "test_set = csr[31::]\n",
    "test_set_label = labels[31::]\n",
    "\n",
    "\n",
    "# Concatenated versions\n",
    "clab_set = np.concatenate(lab_set, axis=0)\n",
    "clab_set_label = np.concatenate(lab_set_label, axis=0)\n",
    "\n",
    "cunlab_set = np.concatenate(unlab_set, axis=0)\n",
    "cunlab_set_label = np.concatenate(unlab_set_label, axis=0)\n",
    "\n",
    "ctest_set = np.concatenate(test_set, axis=0)\n",
    "ctest_set_label = np.concatenate(test_set_label, axis=0)\n",
    "\n",
    "\n",
    "print(\"Labeled training subset, Years 1990-1991\")\n",
    "print(f\"lab_set shape: {lab_set.shape}\")\n",
    "print(f\"lab_set_label shape: {lab_set_label.shape}\")\n",
    "print(f\"clab_set shape: {clab_set.shape}\")\n",
    "print(f\"clab_set_label shape: {clab_set_label.shape}\\n\")\n",
    "\n",
    "print(\"Unlabeled training subset, Years 1992-2020\")\n",
    "print(f\"unlab_set shape: {unlab_set.shape}\")\n",
    "print(f\"unlab_set_label shape: {unlab_set_label.shape}\")\n",
    "print(f\"cunlab_set shape: {cunlab_set.shape}\")\n",
    "print(f\"cunlab_set_label shape: {cunlab_set_label.shape}\\n\")\n",
    "\n",
    "print(\"Test subset, Years 2021-2022\")\n",
    "print(f\"test_set shape: {test_set.shape}\")\n",
    "print(f\"test_set_label shape: {test_set_label.shape}\")\n",
    "print(f\"ctest_set shape: {ctest_set.shape}\")\n",
    "print(f\"ctest_set_label shape: {ctest_set_label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Plan\n",
    "- 1st Training phase: Train the classifier on the first three labeled years of the data\n",
    "- 2nd Training phase: Use the classifier and batch active learning on the rest of the unlabeled data until 2021. Examples that would provide the most information will be chosen to get their true label. The remaining examples will get pseudo-labeled\n",
    "- 1st Evaluate phase: Use the newly trained classifier to evaluate the data from 2021 and 2022\n",
    "- Predict phase (Predict plan): Use time series forecasting to predict a country's set of features until 2050\n",
    "- 2nd Evaluate phase: Use the classifier to predict levels of CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Active Learning\n",
    "def batch_active_learning(classifier: ActiveLearner, unlab_data, unlab_lab, all_data, all_labels, n_queries):\n",
    "    \"\"\"Train a classifier using batch active learning\n",
    "    \n",
    "    Parameters:\n",
    "    - classifier (ActiveLearner): a classifier from the scikit-learn (sklearn) module \n",
    "    - unlab_data (ndarray): the unlabeled dataset, shape=(29, 83, 9)\n",
    "    - unlab_lab (ndarray): the unlabled dataset's labels, shape=(29, 83)\n",
    "    - add_data (ndarray): all data in concatenated form, shape=(2739, 9)\n",
    "    - all_labels (ndarray): all data labels in concatenated form, shape=(2739,)\n",
    "    - n_queries (int): number of queries to make on each year of data\n",
    "    \"\"\"\n",
    "\n",
    "    count = 1\n",
    "    for year_data, year_label in zip(unlab_data, unlab_lab):\n",
    "        # year_data.shape = (83, 9)\n",
    "        # year_label.shape = (83,)\n",
    "        for _ in range(n_queries):\n",
    "            # Query based on uncertainty\n",
    "            query_index, _ = classifier.query(year_data)\n",
    "\n",
    "            # Retrieve the requested example and its label, and teach it to the classifier\n",
    "            example = year_data[query_index].reshape(1, -1)\n",
    "            example_label = year_label[query_index].reshape(1,)\n",
    "            # example.shape = (1, 9)\n",
    "            # example_label.shape = (9,)\n",
    "            classifier.teach(X=example, y=example_label)\n",
    "\n",
    "            # Remove the queried example and its label from the unlabeled datasets\n",
    "            year_data = np.delete(year_data, query_index, axis=0)\n",
    "            year_label = np.delete(year_label, query_index)\n",
    "\n",
    "            accuracy = classifier.score(all_data, all_labels)\n",
    "            print(f\"Accuracy after query {count}: {round(accuracy, 4)}\")\n",
    "\n",
    "            count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch active learning hyperparameters aka model parameters\n",
    "# These are different from real model parameters that are estimated by the model itself\n",
    "\n",
    "n_estimators = 1000\n",
    "criterion = \"log_loss\"\n",
    "max_depth = 50\n",
    "n_queries = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Training phase: Train a classifier on the first two labeled years of the data\n",
    "# 2nd Training phase: Use the classifier and batch active learning on the rest of the unlabeled data until 2021. Examples that would provide the most \n",
    "\n",
    "\n",
    "# Gaussian Naive Bayes isn't an option because the data distribution isn't gaussian/normal due to lacking a \"symmetric bell shape\". \n",
    "# Most of the data labels are on the high end of the scale. Thus, the data's bell shape isn't symmetric\n",
    "# Bernoulli Naive Bayes isn't an option because sample features must be binary-valued (Bernoulli, boolean)\n",
    "# Multinomial, Complement, and Categorical aren't considered  due to data being classified moreso out of probability rather than certainty.\n",
    "\n",
    "if TRAIN:\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth)\n",
    "\n",
    "    # classifier = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
    "\n",
    "    # Start the classifier off by training it on the labeled dataset\n",
    "    classifier = ActiveLearner(estimator=rf, X_training=clab_set, y_training=clab_set_label)\n",
    "\n",
    "    batch_active_learning(classifier, np.copy(unlab_set), np.copy(unlab_set_label), ccsr, clabels, n_queries)\n",
    "\n",
    "    joblib.dump(classifier, \"models/al_rand_forest4.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on test data: 0.8253012048192772\n"
     ]
    }
   ],
   "source": [
    "# 1st Evaluate phase: Use the newly trained classifier to evaluate the data from 2021 and 2022\n",
    "clf = joblib.load(\"models/al_rand_forest4.pkl\")\n",
    "accuracy = clf.score(np.concatenate(test_set, axis=0), np.concatenate(test_set_label, axis=0))\n",
    "print(f\"Model accuracy on test data: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict phase (Predict plan): Use time series forecasting to predict a country's set of features until 2050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Plan\n",
    "- Data processing phase: Separate data by country/region instead of by feature. For each country/region, split their data into 3/4 and 1/4, where 3/4 is the training set and 1/4 is the test set\n",
    "- Training forecast phase: For each country/region, apply a LSTM RNN model on the training data and test data\n",
    "- Evaluate phase: For each country/region, evaluate the validity of the model's forecasts\n",
    "- Real forecast phase: For each country/region, make forecasts of the features for 2023-2050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\t\"\"\"\n",
    "\tFrame a time series as a supervised learning dataset.\n",
    "\tArguments:\n",
    "\t\tdata: Sequence of observations as a list or NumPy array.\n",
    "\t\tn_in: Number of lag observations as input (X).\n",
    "\t\tn_out: Number of observations as output (y).\n",
    "\t\tdropnan: Boolean whether or not to drop rows with NaN values.\n",
    "\tReturns:\n",
    "\t\tPandas DataFrame of series framed for supervised learning.\n",
    "\t\"\"\"\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 50]\n",
      " [ 1 51]\n",
      " [ 2 52]\n",
      " [ 3 53]\n",
      " [ 4 54]\n",
      " [ 5 55]\n",
      " [ 6 56]\n",
      " [ 7 57]\n",
      " [ 8 58]\n",
      " [ 9 59]]\n",
      "   var1(t-1)  var2(t-1)  var1(t)  var2(t)\n",
      "1        0.0       50.0        1       51\n",
      "2        1.0       51.0        2       52\n",
      "3        2.0       52.0        3       53\n",
      "4        3.0       53.0        4       54\n",
      "5        4.0       54.0        5       55\n",
      "6        5.0       55.0        6       56\n",
      "7        6.0       56.0        7       57\n",
      "8        7.0       57.0        8       58\n",
      "9        8.0       58.0        9       59\n"
     ]
    }
   ],
   "source": [
    "raw = DataFrame()\n",
    "raw['ob1'] = [x for x in range(10)]\n",
    "raw['ob2'] = [x for x in range(50, 60)]\n",
    "values = raw.values\n",
    "print(values)\n",
    "data = series_to_supervised(values)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csr shape: (33, 83, 9)\n",
      "Time span: 1990-2022\n",
      "all_tsf shape: (83, 33, 9)\n"
     ]
    }
   ],
   "source": [
    "# Data processing phase: Separate data by country/region instead of by feature.\n",
    "# For each country/region, split their data into 3/4 and 1/4, where 3/4 is the training set and 1/4 is the test set\n",
    "\n",
    "# csr before: a list of snapshots of the world, with each snapshot showing countries' features during a particaulr year\n",
    "print(f\"csr shape: {csr.shape}\")\n",
    "# *_tsf after: a list of countries, with each country entry containing only their features throughout the years\n",
    "all_tsf = np.transpose(csr, [1, 0, 2])\n",
    "# train_tsf = np.transpose(csr[0:25], [1, 0, 2])\n",
    "# test_tsf = np.transpose(csr[25:], [1, 0, 2])\n",
    "\n",
    "print(\"Time span: 1990-2022\")\n",
    "print(f\"all_tsf shape: {all_tsf.shape}\")\n",
    "# print(\"Time span: 1990-2014\")\n",
    "# print(f\"train_tsf shape: {train_tsf.shape}\")\n",
    "# print(\"Time span: 2015-2022\")\n",
    "# print(f\"test_tsf shape: {test_tsf.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tsf shape = (83, 24, 18)\n",
      "test_tsf shape = (83, 7, 18)\n"
     ]
    }
   ],
   "source": [
    "# Data processing for time series forecasting\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_tsf = []\n",
    "test_tsf = []\n",
    "for country in all_tsf:\n",
    "    country_df = pd.DataFrame(country, columns=[\"Oil\", \"Gas\", \"Coal\", \"Nuclear\", \n",
    "                                                \"Hydro\", \"Solar\", \"Wind\", \"Geothermal\", \"Biofuel\"])\n",
    "    # country_df.shape = (33, 9), 33 rows and 9 columns\n",
    "\n",
    "    # Normalize features\n",
    "    scaled = scaler.fit_transform(country_df.values)\n",
    "    # scaled.shape = (33, 9)\n",
    "\n",
    "    train = scaled[:25]\n",
    "    test = scaled[25:]\n",
    "\n",
    "    reframed_train = series_to_supervised(train).values\n",
    "    reframed_test = series_to_supervised(test).values\n",
    "    # reframed_*.shape = (# of years, 18), # of years rows and 18 columns\n",
    "    # columns 0-8: t-1 for each feature\n",
    "    # columns 9-17: t for each feature\n",
    "\n",
    "    train_tsf.append(reframed_train)\n",
    "    test_tsf.append(reframed_test)\n",
    "    \n",
    "train_tsf = np.array(train_tsf)\n",
    "test_tsf = np.array(test_tsf)\n",
    "\n",
    "print(f\"train_tsf shape = {train_tsf.shape}\")\n",
    "print(f\"test_tsf shape = {test_tsf.shape}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4]])\n",
    "x[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Algeria:\n",
      "1/1 - 3s - loss: 0.1627 - val_loss: 0.3960 - 3s/epoch - 3s/step\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "0.4309027538819545\n",
      "(7, 9)\n",
      "(7, 9)\n",
      "[ 6.2863946e-01  1.3965228e-01  2.3366623e-03  3.6525091e-03\n",
      "  1.2700395e-01  2.8242270e-05  5.5920884e-07  1.0305763e-03\n",
      " -7.6888525e-03] \n",
      " [1.72674359e+00 7.22830247e-01 1.97638394e-03 0.00000000e+00\n",
      " 1.39107137e-01 4.30674795e-03 5.68979103e-03 1.05725811e-03\n",
      " 0.00000000e+00] \n",
      "-----------\n",
      "\n",
      "[ 6.2794697e-01  1.3969751e-01  2.3354371e-03  4.9804831e-03\n",
      "  1.2689571e-01  4.3205553e-05 -6.5613667e-06  1.0274473e-03\n",
      " -5.3576757e-03] \n",
      " [1.69259666e+00 7.47380152e-01 7.11853290e-03 0.00000000e+00\n",
      " 1.35903816e-01 8.80748078e-03 5.64086732e-03 1.05725811e-03\n",
      " 0.00000000e+00] \n",
      "-----------\n",
      "\n",
      "[ 6.3288778e-01  1.3985692e-01  2.3884226e-03  8.1944009e-03\n",
      "  1.2669252e-01  7.6037999e-05 -6.4616829e-06  1.0260927e-03\n",
      " -2.2105903e-03] \n",
      " [1.71175955e+00 8.59218610e-01 1.94132955e-02 0.00000000e+00\n",
      " 1.47666290e-01 1.04758985e-02 2.95151550e-03 1.05725811e-03\n",
      " 0.00000000e+00] \n",
      "-----------\n",
      "\n",
      "[6.39417231e-01 1.41958132e-01 2.38300930e-03 9.68610216e-03\n",
      " 1.26685172e-01 1.21849094e-04 1.84589044e-05 1.03544223e-03\n",
      " 1.82229327e-04] \n",
      " [1.78900316e+00 9.05863431e-01 1.60496340e-02 0.00000000e+00\n",
      " 1.54343242e-01 1.06449807e-02 2.94063322e-03 1.05725811e-03\n",
      " 0.00000000e+00] \n",
      "-----------\n",
      "\n",
      "[ 6.3780969e-01  1.4299963e-01  2.3940031e-03  9.9577447e-03\n",
      "  1.2665732e-01  1.2918362e-04  2.0843421e-05  1.0354081e-03\n",
      " -1.7517392e-04] \n",
      " [1.57809151e+00 8.57554672e-01 5.14673148e-03 0.00000000e+00\n",
      " 1.34528506e-01 1.18130494e-02 2.25596981e-03 1.05725811e-03\n",
      " 0.00000000e+00] \n",
      "-----------\n",
      "\n",
      "[6.3137639e-01 1.4322670e-01 2.3179573e-03 8.4396256e-03 1.2666279e-01\n",
      " 1.1833508e-04 6.9506582e-06 1.0336845e-03 1.0705236e-03] \n",
      " [1.65591002e+00 9.80877031e-01 1.04048688e-02 0.00000000e+00\n",
      " 1.26766219e-01 1.14089866e-02 3.06506303e-03 1.05725811e-03\n",
      " 0.00000000e+00] \n",
      "-----------\n",
      "\n",
      "[6.3531345e-01 1.4269418e-01 2.3253539e-03 8.2073733e-03 1.2668960e-01\n",
      " 1.0540069e-04 1.0126067e-05 1.0311725e-03 8.6203963e-04] \n",
      " [1.80304770e+00 8.83768516e-01 5.05846499e-03 0.00000000e+00\n",
      " 1.39651528e-01 1.16715963e-02 3.05388584e-03 1.05725811e-03\n",
      " 0.00000000e+00] \n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Time series forecasting\n",
    "\n",
    "for index in range(len(all_tsf)):\n",
    "    # Runs for 83 cycles, one cycle for a country\n",
    "    print(f\"For {cotry_reg[index]}:\")\n",
    "\n",
    "    # No putting the model creation code in a separate function\n",
    "    # I'll get warnings about potential damage\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences = True, input_shape=(1, 9)))\n",
    "    model.add(LSTM(units=50, return_sequences = False))\n",
    "    model.add(Dense(25))\n",
    "    model.add(Dense(9))\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_absolute_error\")\n",
    "\n",
    "    train: np.ndarray = train_tsf[index]\n",
    "    # train_tsf.shape = (24, 18)\n",
    "    train_X, train_y = train[:, :9], train[:, 9:]\n",
    "    # train_*.shape = (24, 9)\n",
    "\n",
    "    test: np.ndarray = test_tsf[index]\n",
    "    # test_tsf.shape = (7, 18)\n",
    "    test_X, test_y = test[:, :9], test[:, 9:]\n",
    "    # test_*.shape = (7, 9)\n",
    "\n",
    "    train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
    "    # train_X.shape = (24, 1, 9)\n",
    "    # test_X.shape = (7, 1, 9)\n",
    "\n",
    "    # Fit the network on the training data\n",
    "    model.fit(train_X, train_y, epochs=1, batch_size=40, \n",
    "              validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    yhat = model.predict(test_X)\n",
    "    # type(yhat) = np.ndarray\n",
    "    # np.shape(yhat) = (7, 9)\n",
    "\n",
    "    # Invert the forecast's scaling\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "    # test_X.shape = (7, 9)\n",
    "    # inv_yhat = np.concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(yhat)\n",
    "    # inv_yhat = inv_yhat[:, 0]\n",
    "\n",
    "    # Invert the real data's scaling\n",
    "    # inv_y = np.concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "    inv_y = scaler.inverse_transform(test_y)\n",
    "    # inv_y = inv_y[:, 0]\n",
    "\n",
    "    # Note: The objective is to MINIMIZE rmse\n",
    "    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "\n",
    "    print(rmse)\n",
    "    print(inv_yhat.shape)\n",
    "    print(inv_y.shape)\n",
    "    for p_sample, r_sample in zip(inv_yhat, inv_y):\n",
    "        print(p_sample, \"\\n\", r_sample, \"\\n-----------\\n\")\n",
    "\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
