{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Module imports</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Data import</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into DataFrames\n",
    "\n",
    "# These two groups of data will be used for label making\n",
    "# Emissions data\n",
    "nrg_emi_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"CO2 Emissions from Energy\", header=2, index_col=0)\n",
    "# The sheet called \"Natural Gas Flaring\" is already a part of the calculations for the sheet called \"CO2 from Flaring\"\n",
    "flar_emi_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"CO2 from Flaring\", header=2, index_col=0)\n",
    "equi_emi_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"CO2e Methane, Process emissions\", header=2, index_col=0)\n",
    "\n",
    "# Renewable energy production data\n",
    "hydro_pro_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Hydro Generation - TWh\", header=2, index_col=0)\n",
    "solar_pro_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Solar Generation - TWh\", header=2, index_col=0)\n",
    "wind_pro_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Wind Generation - TWh\", header=2, index_col=0)\n",
    "geo_pro_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Geo Biomass Other - TWh\", header=2, index_col=0)\n",
    "bio_pro_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Biofuels production - PJ\", header=2, index_col=0, nrows=47)\n",
    "\n",
    "# These three groups of data will be used for the feature sets\n",
    "# Renewable energy consumption data\n",
    "hydro_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Hydro Consumption - EJ\", header=2, index_col=0)\n",
    "solar_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Solar Consumption - EJ\", header=2, index_col=0)\n",
    "wind_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Wind Consumption - EJ\", header=2, index_col=0)\n",
    "geo_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Geo Biomass Other - EJ\", header=2, index_col=0)\n",
    "bio_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Biofuels consumption - PJ\", header=2, index_col=0, nrows=47)\n",
    "\n",
    "# Non-renewable energy consumption data\n",
    "oil_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Oil Consumption - EJ\", header=2, index_col=0)\n",
    "gas_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Gas Consumption - EJ\", header=2, index_col=0)\n",
    "coal_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Coal Consumption - EJ\", header=2, index_col=0)\n",
    "nuc_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Nuclear Consumption - EJ\", header=2, index_col=0)\n",
    "\n",
    "# Total energy consumption data\n",
    "tol_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Primary Energy Consumption\", header=2, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Programmatic data processing</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(df:pd.DataFrame, flag=False):\n",
    "    \"\"\"\n",
    "    Get an excel sheet ready for conversion to numpy arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): a dataframe containing an excel sheet\n",
    "    - flag (boolean): an indicator to convert PJ to EJ instead of kWh\n",
    "    \"\"\"\n",
    "    #------------------------------ \n",
    "    # Remove all irrelevant columns\n",
    "    #------------------------------\n",
    "\n",
    "    # Remove all data from before 1990\n",
    "    # Find the index of the \"1990\" column\n",
    "    drop_indx = list(df.columns).index(1990)\n",
    "    # Get the column labels of all columns left of \"1990\"\n",
    "    drop_cols = [df.columns[num] for num in np.arange(0, drop_indx)]\n",
    "    df = df.drop(columns=drop_cols)\n",
    "\n",
    "    # Remove data on growth-rate and share\n",
    "    # Get the column labels of the target columns\n",
    "    drop_cols = [df.columns[num] for num in [-3, -2, -1]]\n",
    "    df = df.drop(columns=drop_cols)\n",
    "\n",
    "    #---------------------------\n",
    "    # Remove all irrelevant rows\n",
    "    #---------------------------\n",
    "\n",
    "    # Remove all rows with any empty cells\n",
    "    # 0 doesn't make an empty cell\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Remove all \"Total\" and \"Other\" rows\n",
    "    # In addition, OECD, Non-OECD, the EU, and the USSR\n",
    "    # In addition, 8 other countries because they only appear in excel sheets for\n",
    "    # flaring emissions and nothing else. I can't make data samples for non-existent data\n",
    "    # Rationale for removing \"Other\" rows - some countries in some excel sheets appear\n",
    "    # individually, but are lumped into an \"Other\" row in other sheets.\n",
    "    # There's no possible way for me to know which portions of an\n",
    "    # \"Other\" row value belongs to which countries.\n",
    "    drop_rows = []\n",
    "    keywords = [\"Total\", \"Other\", \"OECD\", \"European Union\", \"USSR\", \"Bolivia\", \n",
    "                \"Bahrain\", \"Syria\", \"Yemen\", \"Libya\", \"Nigeria\", \"Brunei\", \"Myanmar\"]\n",
    "    for row in df.index:\n",
    "        # Mark a row for dropping if it contains any of the keywords\n",
    "        if any(keyword in row for keyword in keywords):\n",
    "            drop_rows.append(row)\n",
    "    df = df.drop(index=drop_rows)\n",
    "\n",
    "    # -----------------\n",
    "    # Convert the units\n",
    "    # -----------------\n",
    "\n",
    "    # This section is only performed on emissions data, \n",
    "    # renewable energy production data, and consumed\n",
    "    # biofuel energy data\n",
    "    # All other dataframes have \"Exajoules\" as their name\n",
    "    \n",
    "    # All CO2 data is currently represented as millions of tonnes\n",
    "    # Convert all produced renewable data to kilowatt-hour (kWh)\n",
    "    # 1 kWh = 3600 kJ\n",
    "    # 1 PJ = 1000000000000 kJ\n",
    "    # 1 TWh = 1000000000 kWh\n",
    "\n",
    "    if (df.index.name) == \"Million tonnes of carbon dioxide\":\n",
    "        # Convert to single tonnes\n",
    "        df = df * 1000000\n",
    "    elif df.index.name == \"Terawatt-hours\":\n",
    "        # Convert to kilowatt-hours\n",
    "        df = df * 1000000000\n",
    "    elif df.index.name == \"Petajoules\":\n",
    "        if flag:\n",
    "            # Convert to exajoules\n",
    "            f = df * 0.001\n",
    "        else:\n",
    "            # Convert to kilowatt-hours\n",
    "            df = df * (1000000000000/3600)\n",
    "\n",
    "    return df\n",
    "\n",
    "# tonnes = metric ton = 1000 kg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowIndices(df:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Return the row labels of a pd.DataFrame\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): a dataframe containing an excel sheet\n",
    "    \"\"\"\n",
    "\n",
    "    return [row for row in df.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dataframes\n",
    "\n",
    "# Unit: Tonnes\n",
    "nrg_emi_df = processData(nrg_emi_df)\n",
    "flar_emi_df = processData(flar_emi_df)\n",
    "equi_emi_df = processData(equi_emi_df)\n",
    "\n",
    "# Unit: Kilowatt-hours\n",
    "hydro_pro_df = processData(hydro_pro_df)\n",
    "solar_pro_df = processData(solar_pro_df)\n",
    "wind_pro_df = processData(wind_pro_df)\n",
    "geo_pro_df = processData(geo_pro_df)\n",
    "bio_pro_df = processData(bio_pro_df)\n",
    "\n",
    "# Unit: Exajoules\n",
    "hydro_con_df = processData(hydro_con_df)\n",
    "solar_con_df = processData(solar_con_df)\n",
    "wind_con_df = processData(wind_con_df)\n",
    "geo_con_df = processData(geo_con_df)\n",
    "bio_con_df = processData(bio_con_df, True)\n",
    "\n",
    "# Unit: Exajoules\n",
    "oil_con_df = processData(oil_con_df)\n",
    "gas_con_df = processData(gas_con_df)\n",
    "coal_con_df = processData(coal_con_df)\n",
    "nuc_con_df = processData(nuc_con_df)\n",
    "\n",
    "# Unit: Exajoules\n",
    "tol_con_df = processData(tol_con_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "\n",
    "nrg_emi = nrg_emi_df.to_numpy()\n",
    "flar_emi = flar_emi_df.to_numpy()\n",
    "equi_emi = equi_emi_df.to_numpy()\n",
    "hydro_p = hydro_pro_df.to_numpy()\n",
    "solar_p = solar_pro_df.to_numpy()\n",
    "wind_p = wind_pro_df.to_numpy()\n",
    "geo_p = geo_pro_df.to_numpy()\n",
    "bio_p = bio_pro_df.to_numpy()\n",
    "\n",
    "hydro_c = hydro_con_df.to_numpy()\n",
    "solar_c = solar_con_df.to_numpy()\n",
    "wind_c = wind_con_df.to_numpy()\n",
    "geo_c = geo_con_df.to_numpy()\n",
    "bio_c = bio_con_df.to_numpy()\n",
    "oil_c = oil_con_df.to_numpy()\n",
    "gas_c = gas_con_df.to_numpy()\n",
    "coal_c = coal_con_df.to_numpy()\n",
    "nuc_c = nuc_con_df.to_numpy()\n",
    "\n",
    "tol_c = tol_con_df.to_numpy()\n",
    "\n",
    "# print(len(rowIndices(hydro_con_df)), len(rowIndices(solar_con_df)), len(rowIndices(wind_con_df)), len(rowIndices(geo_con_df)), len(rowIndices(bio_con_df)))\n",
    "# print(len(rowIndices(oil_con_df)), len(rowIndices(gas_con_df)), len(rowIndices(coal_con_df)), len(rowIndices(nuc_con_df)))\n",
    "# print(len(rowIndices(tol_con_df)))\n",
    "\n",
    "# Get row indices of dataframes\n",
    "# There are three unique indices/list of countries\n",
    "\n",
    "# All of these dataframes (and their NDarray equivalents) have 83 indices.\n",
    "# Their row indices are shown in nrg_emi_indices\n",
    "#\n",
    "# nrg_emi_df\n",
    "# equi_emi_df\n",
    "# hydro_pro_df\n",
    "# solar_pro_df\n",
    "# wind_pro_df\n",
    "# geo_pro_df\n",
    "# hydro_con_df\n",
    "# solar_con_df\n",
    "# wind_con_df\n",
    "# geo_con_df\n",
    "# oil_con_df\n",
    "# gas_con_df\n",
    "# coal_con_df\n",
    "# nuc_con_df\n",
    "# tol_con_df\n",
    "\n",
    "# All of these dataframes (and their NDarray equivalents) have 41 indices\n",
    "# Their row indices are shown in flar_emi_indices\n",
    "# \n",
    "# flar_emi_df\n",
    "\n",
    "# All of these dataframes (and their NDarray equivalents) have 24 indices\n",
    "# Their row indices are shown in bio_indices\n",
    "#\n",
    "# bio_pro_df\n",
    "# bio_con_df\n",
    "\n",
    "# The rest of the dataframes share the same index list as nrg_emi_indices\n",
    "nrg_emi_indices = rowIndices(nrg_emi_df)\n",
    "flar_emi_indices = rowIndices(flar_emi_df)\n",
    "bio_indices = rowIndices(bio_pro_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>nrg_emi_indices:</b>\n",
    "\n",
    "['Canada', 'Mexico', 'US', 'Argentina', 'Brazil', 'Chile', 'Colombia', 'Ecuador', 'Peru', 'Trinidad & Tobago', 'Venezuela', 'Central America', 'Austria', 'Belgium', 'Bulgaria', 'Croatia', 'Cyprus', 'Czech Republic', 'Denmark', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Iceland', 'Ireland', 'Italy', 'Latvia', 'Lithuania', 'Luxembourg', 'Netherlands', 'North Macedonia', 'Norway', 'Poland', 'Portugal', 'Romania', 'Slovakia', 'Slovenia', 'Spain', 'Sweden', 'Switzerland', 'Turkey', 'Ukraine', 'United Kingdom', 'Azerbaijan', 'Belarus', 'Kazakhstan', 'Russian Federation', 'Turkmenistan', 'Uzbekistan', 'Iran', 'Iraq', 'Israel', 'Kuwait', 'Oman', 'Qatar', 'Saudi Arabia', 'United Arab Emirates', 'Algeria', 'Egypt', 'Morocco', 'South Africa', 'Eastern Africa', 'Middle Africa', 'Western Africa', 'Australia', 'Bangladesh', 'China', 'China Hong Kong SAR', 'India', 'Indonesia', 'Japan', 'Malaysia', 'New Zealand', 'Pakistan', 'Philippines', 'Singapore', 'South Korea', 'Sri Lanka', 'Taiwan', 'Thailand', 'Vietnam']\n",
    "\n",
    "<b>flar_emi_indices:</b>\n",
    "\n",
    "['Canada', 'Mexico', 'US', 'Argentina', 'Brazil', 'Colombia', 'Peru', 'Trinidad & Tobago', 'Venezuela', 'Denmark', 'Germany', 'Italy', 'Netherlands', 'Norway', 'Poland', 'Romania', 'Ukraine', 'United Kingdom', 'Azerbaijan', 'Kazakhstan', 'Russian Federation', 'Turkmenistan', 'Uzbekistan', 'Iran', 'Iraq', 'Kuwait', 'Oman', 'Qatar', 'Saudi Arabia', 'United Arab Emirates', 'Algeria', 'Egypt', 'Australia', 'Bangladesh', 'China', 'India', 'Indonesia', 'Malaysia', 'Pakistan', 'Thailand', 'Vietnam']\n",
    "\n",
    "<b>biofuel_indices:</b>\n",
    "\n",
    "['Canada', 'Mexico', 'US', 'Argentina', 'Brazil', 'Colombia', 'Austria', 'Belgium', 'Finland', 'France', 'Germany', 'Italy', 'Netherlands', 'Poland', 'Portugal', 'Spain', 'Sweden', 'United Kingdom', 'Australia', 'China', 'India', 'Indonesia', 'South Korea', 'Thailand']\n",
    "\n",
    "<b>Shape of nrg_emi:</b>\n",
    "\n",
    "(83, 33)\n",
    "\n",
    "<b>Columns of every dataframe:</b>\n",
    "\n",
    "Index([1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001,\n",
    "       2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013,\n",
    "       2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022],\n",
    "      dtype='object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Algeria', 'Argentina', 'Australia', 'Austria', 'Azerbaijan', 'Bangladesh', 'Belarus', 'Belgium', 'Brazil', 'Bulgaria', 'Canada', 'Central America', 'Chile', 'China', 'China Hong Kong SAR', 'Colombia', 'Croatia', 'Cyprus', 'Czech Republic', 'Denmark', 'Eastern Africa', 'Ecuador', 'Egypt', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Japan', 'Kazakhstan', 'Kuwait', 'Latvia', 'Lithuania', 'Luxembourg', 'Malaysia', 'Mexico', 'Middle Africa', 'Morocco', 'Netherlands', 'New Zealand', 'North Macedonia', 'Norway', 'Oman', 'Pakistan', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Qatar', 'Romania', 'Russian Federation', 'Saudi Arabia', 'Singapore', 'Slovakia', 'Slovenia', 'South Africa', 'South Korea', 'Spain', 'Sri Lanka', 'Sweden', 'Switzerland', 'Taiwan', 'Thailand', 'Trinidad & Tobago', 'Turkey', 'Turkmenistan', 'US', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'Uzbekistan', 'Venezuela', 'Vietnam', 'Western Africa']\n",
      "lsr shape: (33, 83, 8)\n"
     ]
    }
   ],
   "source": [
    "# Massive 3D numpy array for label making\n",
    "# 1st dimension - Years. 33 years from 1990-2022 (inclusive)\n",
    "# 2nd dimension - Countries/Regions. 83 unique countries/regions\n",
    "# 3rd dimension - Carbon Neutral features. 8 features (in this order): energy emissions, flaring emissions, CO2 equivalent emissions, \n",
    "# hydroelectric production, solar production, wind production, geothermal production, biofuel production\n",
    "# (33, 83, 8)\n",
    "\n",
    "# Find every unique country/region\n",
    "# This is a bit redundant because every country in flar_emi_indices and bio_indices \n",
    "# is already in nrg_emi_indices\n",
    "cotry_reg = list(set(nrg_emi_indices + flar_emi_indices + bio_indices))\n",
    "cotry_reg.sort()\n",
    "print(cotry_reg)\n",
    "\n",
    "dim_1 = []\n",
    "for year_indx in range(33):\n",
    "    dim_2 = []\n",
    "    for area in cotry_reg:\n",
    "        # There's no area check for the upcoming data\n",
    "        # because every area has this data\n",
    "        indx = nrg_emi_indices.index(area)\n",
    "        # Extract a float\n",
    "        a_nrg_emi = nrg_emi[indx][year_indx]\n",
    "        a_equi_emi = equi_emi[indx][year_indx]\n",
    "        a_hydro = hydro_p[indx][year_indx]\n",
    "        a_solar = solar_p[indx][year_indx]\n",
    "        a_wind = wind_p[indx][year_indx]\n",
    "        a_geo = geo_p[indx][year_indx]\n",
    "\n",
    "        if area in flar_emi_indices:\n",
    "            indx = flar_emi_indices.index(area)\n",
    "            # Extract a float\n",
    "            a_flar_emi = flar_emi[indx][year_indx]\n",
    "        else:\n",
    "            a_flar_emi = 0.\n",
    "\n",
    "        if area in bio_indices:\n",
    "            indx = bio_indices.index(area)\n",
    "            # Extract a float\n",
    "            a_biofuel = bio_p[indx][year_indx]\n",
    "        else:\n",
    "            a_biofuel = 0.\n",
    "\n",
    "        # Is also a set of features\n",
    "        dim_3 = [a_nrg_emi,\n",
    "                a_flar_emi,\n",
    "                a_equi_emi,\n",
    "                a_hydro,\n",
    "                a_solar,\n",
    "                a_wind,\n",
    "                a_geo,\n",
    "                a_biofuel]\n",
    "        dim_2.append(dim_3)\n",
    "    dim_1.append(dim_2)\n",
    "\n",
    "# Label Statistical Review \n",
    "# Full of floats\n",
    "lsr = np.array(dim_1)\n",
    "print(f\"lsr shape: {lsr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Algeria', 'Argentina', 'Australia', 'Austria', 'Azerbaijan', 'Bangladesh', 'Belarus', 'Belgium', 'Brazil', 'Bulgaria', 'Canada', 'Central America', 'Chile', 'China', 'China Hong Kong SAR', 'Colombia', 'Croatia', 'Cyprus', 'Czech Republic', 'Denmark', 'Eastern Africa', 'Ecuador', 'Egypt', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Japan', 'Kazakhstan', 'Kuwait', 'Latvia', 'Lithuania', 'Luxembourg', 'Malaysia', 'Mexico', 'Middle Africa', 'Morocco', 'Netherlands', 'New Zealand', 'North Macedonia', 'Norway', 'Oman', 'Pakistan', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Qatar', 'Romania', 'Russian Federation', 'Saudi Arabia', 'Singapore', 'Slovakia', 'Slovenia', 'South Africa', 'South Korea', 'Spain', 'Sri Lanka', 'Sweden', 'Switzerland', 'Taiwan', 'Thailand', 'Trinidad & Tobago', 'Turkey', 'Turkmenistan', 'US', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'Uzbekistan', 'Venezuela', 'Vietnam', 'Western Africa']\n",
      "csr shape: (33, 83, 9)\n"
     ]
    }
   ],
   "source": [
    "# Massive 3D numpy array for classification\n",
    "# 1st dimension - Years. 33 years from 1990-2022 (inclusive)\n",
    "# 2nd dimension - Countries/Regions. 83 unique countries/regions\n",
    "# 3rd dimension - Energy Consumption features. 9 features (in this order): oil, gas, coal, nuclear, \n",
    "# hydroelectric, solar, wind, geothermal, biofuel\n",
    "# (33, 83, 9)\n",
    "\n",
    "print(cotry_reg)\n",
    "\n",
    "dim_1 = []\n",
    "for year_indx in range(33):\n",
    "    dim_2 = []\n",
    "    for area in cotry_reg:\n",
    "        # There's no area check for the upcoming data\n",
    "        # because every area has this data\n",
    "        indx = nrg_emi_indices.index(area)\n",
    "        # Extract a float\n",
    "        tol_consume = tol_c[indx][year_indx]\n",
    "        a_hydro = hydro_c[indx][year_indx] / tol_consume\n",
    "        a_solar = solar_c[indx][year_indx] / tol_consume\n",
    "        a_wind = wind_c[indx][year_indx] / tol_consume\n",
    "        a_geo = geo_c[indx][year_indx] / tol_consume\n",
    "        a_oil = oil_c[indx][year_indx] / tol_consume\n",
    "        a_gas = gas_c[indx][year_indx] / tol_consume\n",
    "        a_coal = coal_c[indx][year_indx] / tol_consume\n",
    "        a_nuc = nuc_c[indx][year_indx] / tol_consume\n",
    "\n",
    "        if area in bio_indices:\n",
    "            indx = bio_indices.index(area)\n",
    "            # Extract a float\n",
    "            a_biofuel = bio_c[indx][year_indx] / tol_consume\n",
    "        else:\n",
    "            a_biofuel = 0.\n",
    "\n",
    "        # Is also a set of features\n",
    "        dim_3 = [a_oil, a_gas, a_coal, a_nuc,\n",
    "                 a_hydro, a_solar, a_wind, a_geo, a_biofuel]\n",
    "        dim_2.append(dim_3)\n",
    "    dim_1.append(dim_2)\n",
    "\n",
    "# Classification Statistical Review \n",
    "# Full of floats\n",
    "csr = np.array(dim_1)\n",
    "print(f\"csr shape: {csr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeLabel(features):\n",
    "    \"\"\"\n",
    "    Make a label for a sample\n",
    "\n",
    "    - features (np.ndarray): set of 8 features\n",
    "    \"\"\"\n",
    "    # Unit: Tonnes of Carbon Dioxide\n",
    "    co2 = np.sum(features[:3])\n",
    "    # Unit: Kilowatt-hours\n",
    "    renewable = np.sum(features[3:])\n",
    "\n",
    "    # Electricity reductions emission factor\n",
    "    # 0.000709 tonnes CO2/kWh\n",
    "    # Unit: Tonnes of Carbon Dioxide\n",
    "    renewable *= 0.000709\n",
    "\n",
    "    # Remaining co2 after being offset by renewable energy production\n",
    "    rem_co2 = max(co2 - renewable, 0)\n",
    "\n",
    "    # if rem_co2 == co2:\n",
    "    #     return 6\n",
    "    if rem_co2 == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        percent = (rem_co2/co2) * 100\n",
    "        # Equivalent to np.floor(percent / 10)\n",
    "        # The label is the tens place of the percentage\n",
    "        # return int(np.floor(percent / 10))\n",
    "        if percent > 0.0 and percent <= 20.0:\n",
    "            label = 1\n",
    "        elif percent > 20.0 and percent <= 40.0:\n",
    "            label = 2\n",
    "        elif percent > 40.0 and percent <= 60.0:\n",
    "            label = 3\n",
    "        elif percent > 60.0 and percent <= 80.0:\n",
    "            label = 4\n",
    "        elif percent > 80.0 and percent <= 100.0:\n",
    "            label = 5\n",
    "        else:\n",
    "            label = 5\n",
    "        return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels shape: (33, 83)'}\n"
     ]
    }
   ],
   "source": [
    "# Make labels for all of the data/samples/examples \n",
    "# An individual feature isn't a example, but a location in a particular year is\n",
    "# Thus, there are 33 * 83 = 2739 examples\n",
    "\n",
    "# There are 6 possible labels, 0-5\n",
    "# 0 means carbon neutral is achieved\n",
    "# 6 means the country is absolutely nowhere near carbon neutrality\n",
    "labels = np.array([[makeLabel(location) for location in year] for year in lsr])\n",
    "print({f\"labels shape: {labels.shape}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled training subset, Years 1990-1991\n",
      "lab_set shape: (3, 83, 9)\n",
      "lab_set_label shape: (3, 83)\n",
      "\n",
      "Unlabeled training subset, Years 1992-2020\n",
      "unlab_set shape: (28, 83, 9)\n",
      "unlab_set_label shape: (28, 83)\n",
      "\n",
      "Test subset, Years 2021-2022\n",
      "test_set shape: (2, 83, 9)\n",
      "test_set_label shape: (2, 83)\n"
     ]
    }
   ],
   "source": [
    "# Split the big dataset into a three subsets: labeled training, unlabeled training, and test\n",
    "\n",
    "# Labeled training subset\n",
    "# Years 1990-1992\n",
    "lab_set = csr[0:3]\n",
    "lab_set_label = labels[0:3]\n",
    "\n",
    "# Unlabeled training subset\n",
    "# Years 1993-2020\n",
    "unlab_set = csr[3:31]\n",
    "unlab_set_label = labels[3:31]\n",
    "\n",
    "# Test subset\n",
    "# Years 2021-2022\n",
    "test_set = csr[31::]\n",
    "test_set_label = labels[31::]\n",
    "\n",
    "print(\"Labeled training subset, Years 1990-1991\")\n",
    "print(f\"lab_set shape: {lab_set.shape}\")\n",
    "print(f\"lab_set_label shape: {lab_set_label.shape}\\n\")\n",
    "print(\"Unlabeled training subset, Years 1992-2020\")\n",
    "print(f\"unlab_set shape: {unlab_set.shape}\")\n",
    "print(f\"unlab_set_label shape: {unlab_set_label.shape}\\n\")\n",
    "print(\"Test subset, Years 2021-2022\")\n",
    "print(f\"test_set shape: {test_set.shape}\")\n",
    "print(f\"test_set_label shape: {test_set_label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Classification Plan</u>\n",
    "- 1st Training phase: Train a classifier on the first two labeled years of the data\n",
    "- 2nd Training phase: Use the classifier and batch active learning on the rest of the unlabeled data until 2021. Examples that would provide the most information will be chosen to get their true label. The remaining examples will get pseudo-labeled\n",
    "- 1st Evaluate phase: Use the newly trained classifier to evaluate the data from 2021 and 2022\n",
    "- Predict phase: Use time series forecasting (RNN) to predict a country's set of features until 2050\n",
    "- 2nd Evaluate phase: Use the classifier to predict levels of CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_fn()\n",
    "def accuracy_fn(pred_labels, true_labels):\n",
    "    \"\"\"Returns the accuracy of a set of predicted labels\n",
    "    \n",
    "    Parameters:\n",
    "    - pred_labels (ndarray): array of predicted labels, shape=(91,)\n",
    "    - labels (ndarray): array of true labels, shape=(91,)\n",
    "\n",
    "    Returns:\n",
    "    - accuracy (float): accuracy of predictions\n",
    "    \"\"\"\n",
    "\n",
    "    correct = np.sum([1 for label1, label2 in zip(pred_labels, true_labels) if label1 == label2])\n",
    "\n",
    "    accuracy = correct / len(true_labels)\n",
    "\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249, 8)"
      ]
     },
     "execution_count": 1080,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.random((3, 83, 8))\n",
    "y = np.concatenate(x, axis=0)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Active Learning\n",
    "def train_labeled(classifier, lab_data, lab_label, epoch):\n",
    "    \"\"\"Train on the current labeled dataset\n",
    "\n",
    "    Parameters:\n",
    "    - classifier (classifier type): classifier-in-training\n",
    "    - lab_data (ndarray): current labeled data, shape=(# of years, 83, 9)\n",
    "    - lab_label (ndarray): labels for current labeled data, shape=(# of years, 83)\n",
    "    - epoch (int): cycles to run the training for\n",
    "\n",
    "    Returns:\n",
    "    - classifier (classifier type): trained classifier\n",
    "    \"\"\"\n",
    "    for _ in range(epoch):\n",
    "        # Labeled examples have to be concatenated because not every\n",
    "        # year's worth of data contains examples of every class\n",
    "        # ex. If the classifier trains on data belonging to\n",
    "        # only 8/11 classes, predict_proba() will only return\n",
    "        # probabilities for these 8/11 classes and will ignore\n",
    "        # the possibility of the 3 others\n",
    "        labeled_examples = np.concatenate(lab_data, axis=0)\n",
    "        labels = np.concatenate(lab_label, axis=0)\n",
    "        # labeled_examples.shape = (# of years * 83, 9)\n",
    "        # labels.shape = (# of years * 83,)\n",
    "        classifier.fit(labeled_examples, labels)\n",
    "\n",
    "    return classifier\n",
    "\n",
    "def predict_unlabeled(classifier, batch_data):\n",
    "    \"\"\"Predict on the unlabeled data of a year\n",
    "    \n",
    "    Parameters:\n",
    "    - classifier (classifier type): a trained classifier\n",
    "    - batch_data (ndarray): a year's worth of unlabeled data, shape=(83, 9)\n",
    "\n",
    "    Returns:\n",
    "    - pred_class (ndarray): array of predicted classes, shape=(83,)\n",
    "    - pred_proba (ndarray): array of array of class probabilities, shape=(83, 6)\n",
    "    \"\"\"\n",
    "    pred_class = np.array(classifier.predict(batch_data))\n",
    "    pred_proba = np.array(classifier.predict_proba(batch_data))\n",
    "\n",
    "    return pred_class, pred_proba\n",
    "    \n",
    "def batch_active_learning(classifier, lab_data, lab_label, unlab_data, unlab_label, confident_threshold, epoch):\n",
    "    \"\"\"Train a classifier using batch active learning\n",
    "    \n",
    "    Parameters:\n",
    "    - classifier: a classifier from the scikit-learn (sklearn) module \n",
    "    - lab_data (ndarray): the labeled dataset, inital shape=(3, 83, 9)\n",
    "    - lab_label (ndarray): the labled dataset's labels, inital shape=(3, 83)\n",
    "    - unlab_data (ndarray): the unlabeled dataset, inital shape=(28, 83, 9)\n",
    "    - unlab_label (ndarray): the unlabeled dataset's labels, inital shape=(28, 83)\n",
    "    - confident_threshold (float): threshold for the algorithm to request labels\n",
    "    - epoch (int): number of epoches training will last for\n",
    "\n",
    "    Returns:\n",
    "    - classifier (classifier type): trained classifier\n",
    "    \"\"\"\n",
    "\n",
    "    index = 0\n",
    "    episode = 1\n",
    "    # classifier = train_labeled(classifier, lab_data, lab_label, epoch)\n",
    "    while index < 28:\n",
    "        print(f\"Episode {episode}: \")\n",
    "\n",
    "        classifier = train_labeled(classifier, lab_data, lab_label, epoch)\n",
    "\n",
    "        # Predict on the next batch of unlabeled data\n",
    "        # 1 year is a batch\n",
    "        # 4 batches per episode\n",
    "        batch_data = []\n",
    "        batch_label = []\n",
    "        for modifier in range(4):\n",
    "            batch_data.append(unlab_data[index + modifier])\n",
    "            batch_label.append(unlab_label[index + modifier])\n",
    "\n",
    "        # np.shape(batch_data) = (4, 83, 9)\n",
    "        # np.shape(batch_label) = (4, 83)\n",
    "\n",
    "        pred_class = []\n",
    "        pred_proba = []\n",
    "        for batch in batch_data:\n",
    "            prediction_class, pred_probability = predict_unlabeled(classifier, batch)\n",
    "            pred_class.append(prediction_class)\n",
    "            pred_proba.append(pred_probability)\n",
    "        print(f\"score: {classifier.score(np.concatenate(batch_data, axis=0), np.concatenate(batch_label, axis=0))}\")\n",
    "\n",
    "        # np.shape(pred_class) = (4, 83)\n",
    "        # np.shape(pred_proba) = (4, 83, 6)\n",
    "\n",
    "        # Choose which examples to request a true label for\n",
    "        # For these examples, replace their predicted label with their true label\n",
    "        # Remember that the order of examples in pred_class, pred_proba, batch_data, and batch_label are the same\n",
    "        # Ex. The label information of the example at index 0 of batch_data is found at index 0 of the other arrays\n",
    "        uncertain = 0\n",
    "        # 4 cycles\n",
    "        for i, batch_proba in enumerate(pred_proba):\n",
    "            # 83 cycles\n",
    "            for j, probas in enumerate(batch_proba):\n",
    "                pred = np.max(probas)\n",
    "                if pred < confident_threshold:\n",
    "                    uncertain += 1\n",
    "                    pred_class[i][j] = batch_label[i][j]\n",
    "\n",
    "        print(f\"{uncertain} label request(s) made\")\n",
    "\n",
    "        # Reshape batch_data and pred_class for np.append()\n",
    "        # rbatch_data = np.reshape(batch_data, (1, 83, 9))\n",
    "        # rpred_class = np.reshape(pred_class, (1, 83))\n",
    "\n",
    "        # classifier = train_labeled(classifier, batch_data, pred_class, epoch)\n",
    "\n",
    "        # Add the newly pseudo-labeled, and any true-labeled, examples to the labeled data set\n",
    "        lab_data = np.append(lab_data, batch_data, axis=0)\n",
    "        lab_label = np.append(lab_label, pred_class, axis=0)\n",
    "\n",
    "        index += 4\n",
    "        episode += 1\n",
    "\n",
    "    # Train one last time with all the passed examples, labeled and pseudo-labeled\n",
    "    classifier = train_labeled(classifier, lab_data, lab_label, epoch)\n",
    "        \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch active learning hyperparameters aka model parameters\n",
    "# These are different from real model parameters that are estimated by the model itself\n",
    "\n",
    "n_estimators = 1000\n",
    "max_iter = 1000\n",
    "learning_rate = 0.01\n",
    "max_depth = 50\n",
    "confident_threshold = 0.70\n",
    "epoch = 1\n",
    "n_classes = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9457831325301205\n",
      "7 label request(s) made\n",
      "Episode 2: \n",
      "score: 0.9096385542168675\n",
      "14 label request(s) made\n",
      "Episode 3: \n",
      "score: 0.9006024096385542\n",
      "17 label request(s) made\n",
      "Episode 4: \n",
      "score: 0.9096385542168675\n",
      "9 label request(s) made\n",
      "Episode 5: \n",
      "score: 0.8614457831325302\n",
      "15 label request(s) made\n",
      "Episode 6: \n",
      "score: 0.8162650602409639\n",
      "4 label request(s) made\n",
      "Episode 7: \n",
      "score: 0.7349397590361446\n",
      "12 label request(s) made\n"
     ]
    }
   ],
   "source": [
    "# 1st Training phase: Train a classifier on the first two labeled years of the data\n",
    "# 2nd Training phase: Use the classifier and batch active learning on the rest of the unlabeled data until 2021. Examples that would provide the most \n",
    "\n",
    "\n",
    "# Gaussian Naive Bayes isn't an option because the data distribution isn't gaussian/normal due to lacking a \"symmetric bell shape\". \n",
    "# Most of the data labels are on the high end of the scale. Thus, the data's bell shape isn't symmetric\n",
    "# Bernoulli Naive Bayes isn't an option because sample features must be binary-valued (Bernoulli, boolean)\n",
    "# Multinomial, Complement, and Categorical aren't considered  due to data being classified moreso out of probability rather than certainty.\n",
    "\n",
    "# classifier = RandomForestClassifier(n_estimators=n_estimators, criterion=\"log_loss\", max_depth=max_depth)\n",
    "\n",
    "\n",
    "classifier = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
    "\n",
    "classifier = batch_active_learning(classifier, \n",
    "                                   np.copy(lab_set), np.copy(lab_set_label), \n",
    "                                   np.copy(unlab_set), np.copy(unlab_set_label), \n",
    "                                   confident_threshold, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/grad_boost1.pkl']"
      ]
     },
     "execution_count": 1084,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(classifier, \"models/grad_boost1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate()\n",
    "\n",
    "def evaluate(classifier, test_data, test_label):\n",
    "    \"\"\"Have a classifier evaluate test data and return the accuracy\n",
    "    \n",
    "    Parameters:\n",
    "    - classifier: a classifier trained on labeled and pseudo-labeled data\n",
    "    - test_data (ndarray): the test dataset, inital shape=(2, 83, 8)\n",
    "    - test_label (ndarray): the test dataset's labels, inital shape=(2, 83)\n",
    "\n",
    "    Returns:\n",
    "    - accuracy (float): the accuracy of the classifier's predictions on the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    accuracies = []\n",
    "    for test_examples, labels in zip(test_data, test_label):\n",
    "        # test_examples.shape = (91, 8)\n",
    "        # labels.shape = (91,)\n",
    "        preds = np.array(classifier.predict(test_examples))\n",
    "        accuracies.append(accuracy_fn(preds, labels))\n",
    "    print(accuracies)\n",
    "    accuracy = round(np.mean(accuracies) * 100, 2)\n",
    "\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6746987951807228, 0.7108433734939759]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69.28"
      ]
     },
     "execution_count": 1086,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st Evaluate phase: Use the newly trained classifier to evaluate the data from 2021 and 2022\n",
    "accuracy = evaluate(joblib.load(\"models/grad_boost1.pkl\"), test_set, test_set_label)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 5, 6, 7, 8, 9, 10}\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n",
      "{0, 1, 4, 5, 6, 7, 8, 9, 10}\n",
      "{0, 1, 2, 3, 5, 6, 7, 8, 9, 10}\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n",
      "{0, 1, 4, 5, 6, 7, 8, 9, 10}\n"
     ]
    }
   ],
   "source": [
    "print(set(labels[0]))\n",
    "print(set(labels[1]))\n",
    "print(set(labels[2]))\n",
    "\n",
    "print(set(lab_set_label[0]))\n",
    "print(set(lab_set_label[1]))\n",
    "print(set(lab_set_label[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN classifier class\n",
    "class RecurrentNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecurrentNeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=8, out_features=16)\n",
    "        # nn.ReLU() doesn't need parameters in this case\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(in_features=16, out_features=16)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(in_features=16, out_features=16)\n",
    "        self.activation3 = nn.ReLU()\n",
    "        # self.batchNorm = nn.BatchNorm1d()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        # self.dropout1 = nn.Dropout()\n",
    "        self.dense1 = nn.Linear(in_features=16, out_features=1)\n",
    "        # self.dropout2 = nn.Dropout()\n",
    "        # self.dense2 = nn.Linear()\n",
    "        # self.dropout3 = nn.Dropout()\n",
    "        # self.dense3 = nn.Linear()\n",
    "        # self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.activation3(x)\n",
    "        # x = self.batchNorm(x)\n",
    "        # x = self.flatten(x)\n",
    "        # x = self.dropout1(x)\n",
    "        x = self.dense1(x)\n",
    "        # x = self.dropout2(x)\n",
    "        # x = self.dense2(x)\n",
    "        # x = self.dropout3(x)\n",
    "        # x = self.dense3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RNN = RecurrentNeuralNetwork().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(RNN.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series forecasting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
