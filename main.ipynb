{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Module imports</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from modAL.models import ActiveLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Data import</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into DataFrames\n",
    "\n",
    "# These two groups of data will be used for label making\n",
    "# Emissions data\n",
    "nrg_emi_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"CO2 Emissions from Energy\", header=2, index_col=0)\n",
    "# The sheet called \"Natural Gas Flaring\" is already a part of the calculations for the sheet called \"CO2 from Flaring\"\n",
    "flar_emi_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"CO2 from Flaring\", header=2, index_col=0)\n",
    "equi_emi_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"CO2e Methane, Process emissions\", header=2, index_col=0)\n",
    "\n",
    "# Renewable energy production data\n",
    "hydro_pro_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Hydro Generation - TWh\", header=2, index_col=0)\n",
    "solar_pro_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Solar Generation - TWh\", header=2, index_col=0)\n",
    "wind_pro_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Wind Generation - TWh\", header=2, index_col=0)\n",
    "geo_pro_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Geo Biomass Other - TWh\", header=2, index_col=0)\n",
    "bio_pro_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Biofuels production - PJ\", header=2, index_col=0, nrows=47)\n",
    "\n",
    "# These three groups of data will be used for the feature sets\n",
    "# Renewable energy consumption data\n",
    "hydro_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Hydro Consumption - EJ\", header=2, index_col=0)\n",
    "solar_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Solar Consumption - EJ\", header=2, index_col=0)\n",
    "wind_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Wind Consumption - EJ\", header=2, index_col=0)\n",
    "geo_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Geo Biomass Other - EJ\", header=2, index_col=0)\n",
    "bio_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Biofuels consumption - PJ\", header=2, index_col=0, nrows=47)\n",
    "\n",
    "# Non-renewable energy consumption data\n",
    "oil_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Oil Consumption - EJ\", header=2, index_col=0)\n",
    "gas_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Gas Consumption - EJ\", header=2, index_col=0)\n",
    "coal_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Coal Consumption - EJ\", header=2, index_col=0)\n",
    "nuc_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Nuclear Consumption - EJ\", header=2, index_col=0)\n",
    "\n",
    "# Total energy consumption data\n",
    "tol_con_df = pd.read_excel(io=\"data/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Primary Energy Consumption\", header=2, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Programmatic data processing</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(df:pd.DataFrame, flag=False):\n",
    "    \"\"\"\n",
    "    Get an excel sheet ready for conversion to numpy arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): a dataframe containing an excel sheet\n",
    "    - flag (boolean): an indicator to convert PJ to EJ instead of kWh\n",
    "    \"\"\"\n",
    "    #------------------------------ \n",
    "    # Remove all irrelevant columns\n",
    "    #------------------------------\n",
    "\n",
    "    # Remove all data from before 1990\n",
    "    # Find the index of the \"1990\" column\n",
    "    drop_indx = list(df.columns).index(1990)\n",
    "    # Get the column labels of all columns left of \"1990\"\n",
    "    drop_cols = [df.columns[num] for num in np.arange(0, drop_indx)]\n",
    "    df = df.drop(columns=drop_cols)\n",
    "\n",
    "    # Remove data on growth-rate and share\n",
    "    # Get the column labels of the target columns\n",
    "    drop_cols = [df.columns[num] for num in [-3, -2, -1]]\n",
    "    df = df.drop(columns=drop_cols)\n",
    "\n",
    "    #---------------------------\n",
    "    # Remove all irrelevant rows\n",
    "    #---------------------------\n",
    "\n",
    "    # Remove all rows with any empty cells\n",
    "    # 0 doesn't make an empty cell\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Remove all \"Total\" and \"Other\" rows\n",
    "    # In addition, OECD, Non-OECD, the EU, and the USSR\n",
    "    # In addition, 8 other countries because they only appear in excel sheets for\n",
    "    # flaring emissions and nothing else. I can't make data samples for non-existent data\n",
    "    # Rationale for removing \"Other\" rows - some countries in some excel sheets appear\n",
    "    # individually, but are lumped into an \"Other\" row in other sheets.\n",
    "    # There's no possible way for me to know which portions of an\n",
    "    # \"Other\" row value belongs to which countries.\n",
    "    drop_rows = []\n",
    "    keywords = [\"Total\", \"Other\", \"OECD\", \"European Union\", \"USSR\", \"Bolivia\", \n",
    "                \"Bahrain\", \"Syria\", \"Yemen\", \"Libya\", \"Nigeria\", \"Brunei\", \"Myanmar\"]\n",
    "    for row in df.index:\n",
    "        # Mark a row for dropping if it contains any of the keywords\n",
    "        if any(keyword in row for keyword in keywords):\n",
    "            drop_rows.append(row)\n",
    "    df = df.drop(index=drop_rows)\n",
    "\n",
    "    # -----------------\n",
    "    # Convert the units\n",
    "    # -----------------\n",
    "\n",
    "    # This section is only performed on emissions data, \n",
    "    # renewable energy production data, and consumed\n",
    "    # biofuel energy data\n",
    "    # All other dataframes have \"Exajoules\" as their name\n",
    "    \n",
    "    # All CO2 data is currently represented as millions of tonnes\n",
    "    # Convert all produced renewable data to kilowatt-hour (kWh)\n",
    "    # 1 kWh = 3600 kJ\n",
    "    # 1 PJ = 1000000000000 kJ\n",
    "    # 1 TWh = 1000000000 kWh\n",
    "\n",
    "    if (df.index.name) == \"Million tonnes of carbon dioxide\":\n",
    "        # Convert to single tonnes\n",
    "        df = df * 1000000\n",
    "    elif df.index.name == \"Terawatt-hours\":\n",
    "        # Convert to kilowatt-hours\n",
    "        df = df * 1000000000\n",
    "    elif df.index.name == \"Petajoules\":\n",
    "        if flag:\n",
    "            # Convert to exajoules\n",
    "            f = df * 0.001\n",
    "        else:\n",
    "            # Convert to kilowatt-hours\n",
    "            df = df * (1000000000000/3600)\n",
    "\n",
    "    return df\n",
    "\n",
    "# tonnes = metric ton = 1000 kg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowIndices(df:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Return the row labels of a pd.DataFrame\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): a dataframe containing an excel sheet\n",
    "    \"\"\"\n",
    "\n",
    "    return [row for row in df.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dataframes\n",
    "\n",
    "# Unit: Tonnes\n",
    "nrg_emi_df = processData(nrg_emi_df)\n",
    "flar_emi_df = processData(flar_emi_df)\n",
    "equi_emi_df = processData(equi_emi_df)\n",
    "\n",
    "# Unit: Kilowatt-hours\n",
    "hydro_pro_df = processData(hydro_pro_df)\n",
    "solar_pro_df = processData(solar_pro_df)\n",
    "wind_pro_df = processData(wind_pro_df)\n",
    "geo_pro_df = processData(geo_pro_df)\n",
    "bio_pro_df = processData(bio_pro_df)\n",
    "\n",
    "# Unit: Exajoules\n",
    "hydro_con_df = processData(hydro_con_df)\n",
    "solar_con_df = processData(solar_con_df)\n",
    "wind_con_df = processData(wind_con_df)\n",
    "geo_con_df = processData(geo_con_df)\n",
    "bio_con_df = processData(bio_con_df, True)\n",
    "\n",
    "# Unit: Exajoules\n",
    "oil_con_df = processData(oil_con_df)\n",
    "gas_con_df = processData(gas_con_df)\n",
    "coal_con_df = processData(coal_con_df)\n",
    "nuc_con_df = processData(nuc_con_df)\n",
    "\n",
    "# Unit: Exajoules\n",
    "tol_con_df = processData(tol_con_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "\n",
    "nrg_emi = nrg_emi_df.to_numpy()\n",
    "flar_emi = flar_emi_df.to_numpy()\n",
    "equi_emi = equi_emi_df.to_numpy()\n",
    "hydro_p = hydro_pro_df.to_numpy()\n",
    "solar_p = solar_pro_df.to_numpy()\n",
    "wind_p = wind_pro_df.to_numpy()\n",
    "geo_p = geo_pro_df.to_numpy()\n",
    "bio_p = bio_pro_df.to_numpy()\n",
    "\n",
    "hydro_c = hydro_con_df.to_numpy()\n",
    "solar_c = solar_con_df.to_numpy()\n",
    "wind_c = wind_con_df.to_numpy()\n",
    "geo_c = geo_con_df.to_numpy()\n",
    "bio_c = bio_con_df.to_numpy()\n",
    "oil_c = oil_con_df.to_numpy()\n",
    "gas_c = gas_con_df.to_numpy()\n",
    "coal_c = coal_con_df.to_numpy()\n",
    "nuc_c = nuc_con_df.to_numpy()\n",
    "\n",
    "tol_c = tol_con_df.to_numpy()\n",
    "\n",
    "# print(len(rowIndices(hydro_con_df)), len(rowIndices(solar_con_df)), len(rowIndices(wind_con_df)), len(rowIndices(geo_con_df)), len(rowIndices(bio_con_df)))\n",
    "# print(len(rowIndices(oil_con_df)), len(rowIndices(gas_con_df)), len(rowIndices(coal_con_df)), len(rowIndices(nuc_con_df)))\n",
    "# print(len(rowIndices(tol_con_df)))\n",
    "\n",
    "# Get row indices of dataframes\n",
    "# There are three unique indices/list of countries\n",
    "\n",
    "# All of these dataframes (and their NDarray equivalents) have 83 indices.\n",
    "# Their row indices are shown in nrg_emi_indices\n",
    "#\n",
    "# nrg_emi_df\n",
    "# equi_emi_df\n",
    "# hydro_pro_df\n",
    "# solar_pro_df\n",
    "# wind_pro_df\n",
    "# geo_pro_df\n",
    "# hydro_con_df\n",
    "# solar_con_df\n",
    "# wind_con_df\n",
    "# geo_con_df\n",
    "# oil_con_df\n",
    "# gas_con_df\n",
    "# coal_con_df\n",
    "# nuc_con_df\n",
    "# tol_con_df\n",
    "\n",
    "# All of these dataframes (and their NDarray equivalents) have 41 indices\n",
    "# Their row indices are shown in flar_emi_indices\n",
    "# \n",
    "# flar_emi_df\n",
    "\n",
    "# All of these dataframes (and their NDarray equivalents) have 24 indices\n",
    "# Their row indices are shown in bio_indices\n",
    "#\n",
    "# bio_pro_df\n",
    "# bio_con_df\n",
    "\n",
    "# The rest of the dataframes share the same index list as nrg_emi_indices\n",
    "nrg_emi_indices = rowIndices(nrg_emi_df)\n",
    "flar_emi_indices = rowIndices(flar_emi_df)\n",
    "bio_indices = rowIndices(bio_pro_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>nrg_emi_indices:</b>\n",
    "\n",
    "['Canada', 'Mexico', 'US', 'Argentina', 'Brazil', 'Chile', 'Colombia', 'Ecuador', 'Peru', 'Trinidad & Tobago', 'Venezuela', 'Central America', 'Austria', 'Belgium', 'Bulgaria', 'Croatia', 'Cyprus', 'Czech Republic', 'Denmark', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Iceland', 'Ireland', 'Italy', 'Latvia', 'Lithuania', 'Luxembourg', 'Netherlands', 'North Macedonia', 'Norway', 'Poland', 'Portugal', 'Romania', 'Slovakia', 'Slovenia', 'Spain', 'Sweden', 'Switzerland', 'Turkey', 'Ukraine', 'United Kingdom', 'Azerbaijan', 'Belarus', 'Kazakhstan', 'Russian Federation', 'Turkmenistan', 'Uzbekistan', 'Iran', 'Iraq', 'Israel', 'Kuwait', 'Oman', 'Qatar', 'Saudi Arabia', 'United Arab Emirates', 'Algeria', 'Egypt', 'Morocco', 'South Africa', 'Eastern Africa', 'Middle Africa', 'Western Africa', 'Australia', 'Bangladesh', 'China', 'China Hong Kong SAR', 'India', 'Indonesia', 'Japan', 'Malaysia', 'New Zealand', 'Pakistan', 'Philippines', 'Singapore', 'South Korea', 'Sri Lanka', 'Taiwan', 'Thailand', 'Vietnam']\n",
    "\n",
    "<b>flar_emi_indices:</b>\n",
    "\n",
    "['Canada', 'Mexico', 'US', 'Argentina', 'Brazil', 'Colombia', 'Peru', 'Trinidad & Tobago', 'Venezuela', 'Denmark', 'Germany', 'Italy', 'Netherlands', 'Norway', 'Poland', 'Romania', 'Ukraine', 'United Kingdom', 'Azerbaijan', 'Kazakhstan', 'Russian Federation', 'Turkmenistan', 'Uzbekistan', 'Iran', 'Iraq', 'Kuwait', 'Oman', 'Qatar', 'Saudi Arabia', 'United Arab Emirates', 'Algeria', 'Egypt', 'Australia', 'Bangladesh', 'China', 'India', 'Indonesia', 'Malaysia', 'Pakistan', 'Thailand', 'Vietnam']\n",
    "\n",
    "<b>biofuel_indices:</b>\n",
    "\n",
    "['Canada', 'Mexico', 'US', 'Argentina', 'Brazil', 'Colombia', 'Austria', 'Belgium', 'Finland', 'France', 'Germany', 'Italy', 'Netherlands', 'Poland', 'Portugal', 'Spain', 'Sweden', 'United Kingdom', 'Australia', 'China', 'India', 'Indonesia', 'South Korea', 'Thailand']\n",
    "\n",
    "<b>Shape of nrg_emi:</b>\n",
    "\n",
    "(83, 33)\n",
    "\n",
    "<b>Columns of every dataframe:</b>\n",
    "\n",
    "Index([1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001,\n",
    "       2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013,\n",
    "       2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022],\n",
    "      dtype='object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Algeria', 'Argentina', 'Australia', 'Austria', 'Azerbaijan', 'Bangladesh', 'Belarus', 'Belgium', 'Brazil', 'Bulgaria', 'Canada', 'Central America', 'Chile', 'China', 'China Hong Kong SAR', 'Colombia', 'Croatia', 'Cyprus', 'Czech Republic', 'Denmark', 'Eastern Africa', 'Ecuador', 'Egypt', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Japan', 'Kazakhstan', 'Kuwait', 'Latvia', 'Lithuania', 'Luxembourg', 'Malaysia', 'Mexico', 'Middle Africa', 'Morocco', 'Netherlands', 'New Zealand', 'North Macedonia', 'Norway', 'Oman', 'Pakistan', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Qatar', 'Romania', 'Russian Federation', 'Saudi Arabia', 'Singapore', 'Slovakia', 'Slovenia', 'South Africa', 'South Korea', 'Spain', 'Sri Lanka', 'Sweden', 'Switzerland', 'Taiwan', 'Thailand', 'Trinidad & Tobago', 'Turkey', 'Turkmenistan', 'US', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'Uzbekistan', 'Venezuela', 'Vietnam', 'Western Africa']\n",
      "lsr shape: (33, 83, 8)\n"
     ]
    }
   ],
   "source": [
    "# Massive 3D numpy array for label making\n",
    "# 1st dimension - Years. 33 years from 1990-2022 (inclusive)\n",
    "# 2nd dimension - Countries/Regions. 83 unique countries/regions\n",
    "# 3rd dimension - Carbon Neutral features. 8 features (in this order): energy emissions, flaring emissions, CO2 equivalent emissions, \n",
    "# hydroelectric production, solar production, wind production, geothermal production, biofuel production\n",
    "# (33, 83, 8)\n",
    "\n",
    "# Find every unique country/region\n",
    "# This is a bit redundant because every country in flar_emi_indices and bio_indices \n",
    "# is already in nrg_emi_indices\n",
    "cotry_reg = list(set(nrg_emi_indices + flar_emi_indices + bio_indices))\n",
    "cotry_reg.sort()\n",
    "print(cotry_reg)\n",
    "\n",
    "dim_1 = []\n",
    "for year_indx in range(33):\n",
    "    dim_2 = []\n",
    "    for area in cotry_reg:\n",
    "        # There's no area check for the upcoming data\n",
    "        # because every area has this data\n",
    "        indx = nrg_emi_indices.index(area)\n",
    "        # Extract a float\n",
    "        a_nrg_emi = nrg_emi[indx][year_indx]\n",
    "        a_equi_emi = equi_emi[indx][year_indx]\n",
    "        a_hydro = hydro_p[indx][year_indx]\n",
    "        a_solar = solar_p[indx][year_indx]\n",
    "        a_wind = wind_p[indx][year_indx]\n",
    "        a_geo = geo_p[indx][year_indx]\n",
    "\n",
    "        if area in flar_emi_indices:\n",
    "            indx = flar_emi_indices.index(area)\n",
    "            # Extract a float\n",
    "            a_flar_emi = flar_emi[indx][year_indx]\n",
    "        else:\n",
    "            a_flar_emi = 0.\n",
    "\n",
    "        if area in bio_indices:\n",
    "            indx = bio_indices.index(area)\n",
    "            # Extract a float\n",
    "            a_biofuel = bio_p[indx][year_indx]\n",
    "        else:\n",
    "            a_biofuel = 0.\n",
    "\n",
    "        # Is also a set of features\n",
    "        dim_3 = [a_nrg_emi,\n",
    "                a_flar_emi,\n",
    "                a_equi_emi,\n",
    "                a_hydro,\n",
    "                a_solar,\n",
    "                a_wind,\n",
    "                a_geo,\n",
    "                a_biofuel]\n",
    "        dim_2.append(dim_3)\n",
    "    dim_1.append(dim_2)\n",
    "\n",
    "# Label Statistical Review \n",
    "# Full of floats\n",
    "lsr = np.array(dim_1)\n",
    "print(f\"lsr shape: {lsr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Algeria', 'Argentina', 'Australia', 'Austria', 'Azerbaijan', 'Bangladesh', 'Belarus', 'Belgium', 'Brazil', 'Bulgaria', 'Canada', 'Central America', 'Chile', 'China', 'China Hong Kong SAR', 'Colombia', 'Croatia', 'Cyprus', 'Czech Republic', 'Denmark', 'Eastern Africa', 'Ecuador', 'Egypt', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Japan', 'Kazakhstan', 'Kuwait', 'Latvia', 'Lithuania', 'Luxembourg', 'Malaysia', 'Mexico', 'Middle Africa', 'Morocco', 'Netherlands', 'New Zealand', 'North Macedonia', 'Norway', 'Oman', 'Pakistan', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Qatar', 'Romania', 'Russian Federation', 'Saudi Arabia', 'Singapore', 'Slovakia', 'Slovenia', 'South Africa', 'South Korea', 'Spain', 'Sri Lanka', 'Sweden', 'Switzerland', 'Taiwan', 'Thailand', 'Trinidad & Tobago', 'Turkey', 'Turkmenistan', 'US', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'Uzbekistan', 'Venezuela', 'Vietnam', 'Western Africa']\n",
      "csr shape: (33, 83, 9)\n",
      "clabels shape: (2739, 9)\n"
     ]
    }
   ],
   "source": [
    "# Massive 3D numpy array for classification\n",
    "# 1st dimension - Years. 33 years from 1990-2022 (inclusive)\n",
    "# 2nd dimension - Countries/Regions. 83 unique countries/regions\n",
    "# 3rd dimension - Energy Consumption features. 9 features (in this order): oil, gas, coal, nuclear, \n",
    "# hydroelectric, solar, wind, geothermal, biofuel\n",
    "# (33, 83, 9)\n",
    "\n",
    "print(cotry_reg)\n",
    "\n",
    "dim_1 = []\n",
    "for year_indx in range(33):\n",
    "    dim_2 = []\n",
    "    for area in cotry_reg:\n",
    "        # There's no area check for the upcoming data\n",
    "        # because every area has this data\n",
    "        indx = nrg_emi_indices.index(area)\n",
    "        # Extract a float\n",
    "        tol_consume = tol_c[indx][year_indx]\n",
    "        a_hydro = hydro_c[indx][year_indx] / tol_consume\n",
    "        a_solar = solar_c[indx][year_indx] / tol_consume\n",
    "        a_wind = wind_c[indx][year_indx] / tol_consume\n",
    "        a_geo = geo_c[indx][year_indx] / tol_consume\n",
    "        a_oil = oil_c[indx][year_indx] / tol_consume\n",
    "        a_gas = gas_c[indx][year_indx] / tol_consume\n",
    "        a_coal = coal_c[indx][year_indx] / tol_consume\n",
    "        a_nuc = nuc_c[indx][year_indx] / tol_consume\n",
    "\n",
    "        if area in bio_indices:\n",
    "            indx = bio_indices.index(area)\n",
    "            # Extract a float\n",
    "            a_biofuel = bio_c[indx][year_indx] / tol_consume\n",
    "        else:\n",
    "            a_biofuel = 0.\n",
    "\n",
    "        # Is also a set of features\n",
    "        dim_3 = [a_oil, a_gas, a_coal, a_nuc,\n",
    "                 a_hydro, a_solar, a_wind, a_geo, a_biofuel]\n",
    "        dim_2.append(dim_3)\n",
    "    dim_1.append(dim_2)\n",
    "\n",
    "# Classification Statistical Review \n",
    "# Full of floats\n",
    "csr = np.array(dim_1)\n",
    "print(f\"csr shape: {csr.shape}\")\n",
    "\n",
    "# Concatenated version\n",
    "ccsr = np.concatenate(csr, axis=0)\n",
    "print(f\"clabels shape: {ccsr.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeLabel(features):\n",
    "    \"\"\"\n",
    "    Make a label for a sample\n",
    "\n",
    "    - features (np.ndarray): set of 8 features\n",
    "    \"\"\"\n",
    "    # Unit: Tonnes of Carbon Dioxide\n",
    "    co2 = np.sum(features[:3])\n",
    "    # Unit: Kilowatt-hours\n",
    "    renewable = np.sum(features[3:])\n",
    "\n",
    "    # Electricity reductions emission factor\n",
    "    # 0.000709 tonnes CO2/kWh\n",
    "    # Unit: Tonnes of Carbon Dioxide\n",
    "    renewable *= 0.000709\n",
    "\n",
    "    # Remaining co2 after being offset by renewable energy production\n",
    "    rem_co2 = max(co2 - renewable, 0)\n",
    "\n",
    "    # if rem_co2 == co2:\n",
    "    #     return 6\n",
    "    if rem_co2 == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        percent = (rem_co2/co2) * 100\n",
    "        # Equivalent to np.floor(percent / 10)\n",
    "        # The label is the tens place of the percentage\n",
    "        # return int(np.floor(percent / 10))\n",
    "        if percent > 0.0 and percent <= 20.0:\n",
    "            label = 1\n",
    "        elif percent > 20.0 and percent <= 40.0:\n",
    "            label = 2\n",
    "        elif percent > 40.0 and percent <= 60.0:\n",
    "            label = 3\n",
    "        elif percent > 60.0 and percent <= 80.0:\n",
    "            label = 4\n",
    "        elif percent > 80.0 and percent <= 100.0:\n",
    "            label = 5\n",
    "        else:\n",
    "            label = 5\n",
    "        return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels shape: (33, 83)\n",
      "clabels shape: (2739,)\n"
     ]
    }
   ],
   "source": [
    "# Make labels for all of the data/samples/examples \n",
    "# An individual feature isn't a example, but a location in a particular year is\n",
    "# Thus, there are 33 * 83 = 2739 examples\n",
    "\n",
    "# There are 6 possible labels, 0-5\n",
    "# 0 means carbon neutral is achieved\n",
    "# 6 means the country is absolutely nowhere near carbon neutrality\n",
    "labels = np.array([[makeLabel(location) for location in year] for year in lsr])\n",
    "print(f\"labels shape: {labels.shape}\")\n",
    "\n",
    "# Concatenated version\n",
    "clabels = np.concatenate(labels, axis=0)\n",
    "print(f\"clabels shape: {clabels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled training subset, Years 1990-1991\n",
      "lab_set shape: (2, 83, 9)\n",
      "lab_set_label shape: (2, 83)\n",
      "clab_set shape: (166, 9)\n",
      "clab_set_label shape: (166,)\n",
      "\n",
      "Unlabeled training subset, Years 1992-2020\n",
      "unlab_set shape: (29, 83, 9)\n",
      "unlab_set_label shape: (29, 83)\n",
      "cunlab_set shape: (2407, 9)\n",
      "cunlab_set_label shape: (2407,)\n",
      "\n",
      "Test subset, Years 2021-2022\n",
      "test_set shape: (2, 83, 9)\n",
      "test_set_label shape: (2, 83)\n",
      "ctest_set shape: (166, 9)\n",
      "ctest_set_label shape: (166,)\n"
     ]
    }
   ],
   "source": [
    "# Split the big dataset into a three subsets: labeled training, unlabeled training, and test\n",
    "\n",
    "# Labeled training subset\n",
    "# Years 1990-1991\n",
    "lab_set = csr[0:2]\n",
    "lab_set_label = labels[0:2]\n",
    "\n",
    "# Unlabeled training subset\n",
    "# aka Pool\n",
    "# Years 1992-2020\n",
    "unlab_set = csr[2:31]\n",
    "unlab_set_label = labels[2:31]\n",
    "\n",
    "# Test subset\n",
    "# Years 2021-2022\n",
    "test_set = csr[31::]\n",
    "test_set_label = labels[31::]\n",
    "\n",
    "\n",
    "# Concatenated versions\n",
    "clab_set = np.concatenate(lab_set, axis=0)\n",
    "clab_set_label = np.concatenate(lab_set_label, axis=0)\n",
    "\n",
    "cunlab_set = np.concatenate(unlab_set, axis=0)\n",
    "cunlab_set_label = np.concatenate(unlab_set_label, axis=0)\n",
    "\n",
    "ctest_set = np.concatenate(test_set, axis=0)\n",
    "ctest_set_label = np.concatenate(test_set_label, axis=0)\n",
    "\n",
    "\n",
    "print(\"Labeled training subset, Years 1990-1991\")\n",
    "print(f\"lab_set shape: {lab_set.shape}\")\n",
    "print(f\"lab_set_label shape: {lab_set_label.shape}\")\n",
    "print(f\"clab_set shape: {clab_set.shape}\")\n",
    "print(f\"clab_set_label shape: {clab_set_label.shape}\\n\")\n",
    "\n",
    "print(\"Unlabeled training subset, Years 1992-2020\")\n",
    "print(f\"unlab_set shape: {unlab_set.shape}\")\n",
    "print(f\"unlab_set_label shape: {unlab_set_label.shape}\")\n",
    "print(f\"cunlab_set shape: {cunlab_set.shape}\")\n",
    "print(f\"cunlab_set_label shape: {cunlab_set_label.shape}\\n\")\n",
    "\n",
    "print(\"Test subset, Years 2021-2022\")\n",
    "print(f\"test_set shape: {test_set.shape}\")\n",
    "print(f\"test_set_label shape: {test_set_label.shape}\")\n",
    "print(f\"ctest_set shape: {ctest_set.shape}\")\n",
    "print(f\"ctest_set_label shape: {ctest_set_label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Classification Plan</u>\n",
    "- 1st Training phase: Train the classifier on the first three labeled years of the data\n",
    "- 2nd Training phase: Use the classifier and batch active learning on the rest of the unlabeled data until 2021. Examples that would provide the most information will be chosen to get their true label. The remaining examples will get pseudo-labeled\n",
    "- 1st Evaluate phase: Use the newly trained classifier to evaluate the data from 2021 and 2022\n",
    "- Predict phase: Use time series forecasting (RNN) to predict a country's set of features until 2050\n",
    "- 2nd Evaluate phase: Use the classifier to predict levels of CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Active Learning\n",
    "def batch_active_learning(classifier: ActiveLearner, unlab_data, unlab_lab, all_data, all_labels, n_queries):\n",
    "    \"\"\"Train a classifier using batch active learning\n",
    "    \n",
    "    Parameters:\n",
    "    - classifier (ActiveLearner): a classifier from the scikit-learn (sklearn) module \n",
    "    - unlab_data (ndarray): the unlabeled dataset, shape=(29, 83, 9)\n",
    "    - unlab_lab (ndarray): the unlabled dataset's labels, shape=(29, 83)\n",
    "    - add_data (ndarray): all data in concatenated form, shape=(2739, 9)\n",
    "    - all_labels (ndarray): all data labels in concatenated form, shape=(2739,)\n",
    "    - n_queries (int): number of queries to make on each year of data\n",
    "    \"\"\"\n",
    "\n",
    "    count = 1\n",
    "    for year_data, year_label in zip(unlab_data, unlab_lab):\n",
    "        # year_data.shape = (83, 9)\n",
    "        # year_label.shape = (83,)\n",
    "        for _ in range(n_queries):\n",
    "            # Query based on uncertainty\n",
    "            query_index, _ = classifier.query(year_data)\n",
    "\n",
    "            # Retrieve the requested example and its label, and teach it to the classifier\n",
    "            example = year_data[query_index].reshape(1, -1)\n",
    "            example_label = year_label[query_index].reshape(1,)\n",
    "            # example.shape = (1, 9)\n",
    "            # example_label.shape = (9,)\n",
    "            classifier.teach(X=example, y=example_label)\n",
    "\n",
    "            # Remove the queried example and its label from the unlabeled datasets\n",
    "            year_data = np.delete(year_data, query_index, axis=0)\n",
    "            year_label = np.delete(year_label, query_index)\n",
    "\n",
    "            accuracy = classifier.score(all_data, all_labels)\n",
    "            print(f\"Accuracy after query {count}: {round(accuracy, 4)}\")\n",
    "\n",
    "            count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch active learning hyperparameters aka model parameters\n",
    "# These are different from real model parameters that are estimated by the model itself\n",
    "\n",
    "n_estimators = 1000\n",
    "criterion = \"log_loss\"\n",
    "max_depth = 50\n",
    "n_queries = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after query 1: 0.8488\n",
      "Accuracy after query 2: 0.8543\n",
      "Accuracy after query 3: 0.8576\n",
      "Accuracy after query 4: 0.8583\n",
      "Accuracy after query 5: 0.8594\n",
      "Accuracy after query 6: 0.8602\n",
      "Accuracy after query 7: 0.8616\n",
      "Accuracy after query 8: 0.8616\n",
      "Accuracy after query 9: 0.862\n",
      "Accuracy after query 10: 0.8631\n",
      "Accuracy after query 11: 0.8602\n",
      "Accuracy after query 12: 0.862\n",
      "Accuracy after query 13: 0.8613\n",
      "Accuracy after query 14: 0.8664\n",
      "Accuracy after query 15: 0.8671\n",
      "Accuracy after query 16: 0.8656\n",
      "Accuracy after query 17: 0.8656\n",
      "Accuracy after query 18: 0.8675\n",
      "Accuracy after query 19: 0.8671\n",
      "Accuracy after query 20: 0.8671\n",
      "Accuracy after query 21: 0.8715\n",
      "Accuracy after query 22: 0.8719\n",
      "Accuracy after query 23: 0.8751\n",
      "Accuracy after query 24: 0.8817\n",
      "Accuracy after query 25: 0.8828\n",
      "Accuracy after query 26: 0.8788\n",
      "Accuracy after query 27: 0.877\n",
      "Accuracy after query 28: 0.8781\n",
      "Accuracy after query 29: 0.881\n",
      "Accuracy after query 30: 0.8799\n",
      "Accuracy after query 31: 0.8817\n",
      "Accuracy after query 32: 0.8806\n",
      "Accuracy after query 33: 0.8813\n",
      "Accuracy after query 34: 0.881\n",
      "Accuracy after query 35: 0.8784\n",
      "Accuracy after query 36: 0.8821\n",
      "Accuracy after query 37: 0.8766\n",
      "Accuracy after query 38: 0.8799\n",
      "Accuracy after query 39: 0.8784\n",
      "Accuracy after query 40: 0.8781\n",
      "Accuracy after query 41: 0.8773\n",
      "Accuracy after query 42: 0.8773\n",
      "Accuracy after query 43: 0.8861\n",
      "Accuracy after query 44: 0.8781\n",
      "Accuracy after query 45: 0.8781\n",
      "Accuracy after query 46: 0.8777\n",
      "Accuracy after query 47: 0.8784\n",
      "Accuracy after query 48: 0.8802\n",
      "Accuracy after query 49: 0.8784\n",
      "Accuracy after query 50: 0.8773\n",
      "Accuracy after query 51: 0.8759\n",
      "Accuracy after query 52: 0.8799\n",
      "Accuracy after query 53: 0.8799\n",
      "Accuracy after query 54: 0.8792\n",
      "Accuracy after query 55: 0.8795\n",
      "Accuracy after query 56: 0.877\n",
      "Accuracy after query 57: 0.8781\n",
      "Accuracy after query 58: 0.8799\n",
      "Accuracy after query 59: 0.8806\n",
      "Accuracy after query 60: 0.8821\n",
      "Accuracy after query 61: 0.8799\n",
      "Accuracy after query 62: 0.8828\n",
      "Accuracy after query 63: 0.8802\n",
      "Accuracy after query 64: 0.8824\n",
      "Accuracy after query 65: 0.8839\n",
      "Accuracy after query 66: 0.8854\n",
      "Accuracy after query 67: 0.8832\n",
      "Accuracy after query 68: 0.8828\n",
      "Accuracy after query 69: 0.8872\n",
      "Accuracy after query 70: 0.8835\n",
      "Accuracy after query 71: 0.8824\n",
      "Accuracy after query 72: 0.8817\n",
      "Accuracy after query 73: 0.8832\n",
      "Accuracy after query 74: 0.8843\n",
      "Accuracy after query 75: 0.8843\n",
      "Accuracy after query 76: 0.8901\n",
      "Accuracy after query 77: 0.8894\n",
      "Accuracy after query 78: 0.8894\n",
      "Accuracy after query 79: 0.8919\n",
      "Accuracy after query 80: 0.8919\n",
      "Accuracy after query 81: 0.8908\n",
      "Accuracy after query 82: 0.8952\n",
      "Accuracy after query 83: 0.8963\n",
      "Accuracy after query 84: 0.8945\n",
      "Accuracy after query 85: 0.8938\n",
      "Accuracy after query 86: 0.8949\n",
      "Accuracy after query 87: 0.8952\n",
      "Accuracy after query 88: 0.8916\n",
      "Accuracy after query 89: 0.8919\n",
      "Accuracy after query 90: 0.8912\n",
      "Accuracy after query 91: 0.8919\n",
      "Accuracy after query 92: 0.8897\n",
      "Accuracy after query 93: 0.8923\n",
      "Accuracy after query 94: 0.8927\n",
      "Accuracy after query 95: 0.8945\n",
      "Accuracy after query 96: 0.8952\n",
      "Accuracy after query 97: 0.8963\n",
      "Accuracy after query 98: 0.8945\n",
      "Accuracy after query 99: 0.8952\n",
      "Accuracy after query 100: 0.8981\n",
      "Accuracy after query 101: 0.8989\n",
      "Accuracy after query 102: 0.8996\n",
      "Accuracy after query 103: 0.9018\n",
      "Accuracy after query 104: 0.9051\n",
      "Accuracy after query 105: 0.9051\n",
      "Accuracy after query 106: 0.9043\n",
      "Accuracy after query 107: 0.9025\n",
      "Accuracy after query 108: 0.9051\n",
      "Accuracy after query 109: 0.904\n",
      "Accuracy after query 110: 0.9062\n",
      "Accuracy after query 111: 0.9054\n",
      "Accuracy after query 112: 0.9073\n",
      "Accuracy after query 113: 0.9043\n",
      "Accuracy after query 114: 0.9062\n",
      "Accuracy after query 115: 0.9047\n",
      "Accuracy after query 116: 0.9065\n",
      "Accuracy after query 117: 0.9073\n",
      "Accuracy after query 118: 0.9102\n",
      "Accuracy after query 119: 0.9076\n",
      "Accuracy after query 120: 0.9076\n",
      "Accuracy after query 121: 0.908\n",
      "Accuracy after query 122: 0.9069\n",
      "Accuracy after query 123: 0.9116\n",
      "Accuracy after query 124: 0.9102\n",
      "Accuracy after query 125: 0.9106\n",
      "Accuracy after query 126: 0.912\n",
      "Accuracy after query 127: 0.9116\n",
      "Accuracy after query 128: 0.9106\n",
      "Accuracy after query 129: 0.912\n",
      "Accuracy after query 130: 0.9113\n",
      "Accuracy after query 131: 0.9102\n",
      "Accuracy after query 132: 0.9113\n",
      "Accuracy after query 133: 0.9109\n",
      "Accuracy after query 134: 0.912\n",
      "Accuracy after query 135: 0.9142\n",
      "Accuracy after query 136: 0.916\n",
      "Accuracy after query 137: 0.9142\n",
      "Accuracy after query 138: 0.916\n",
      "Accuracy after query 139: 0.9146\n",
      "Accuracy after query 140: 0.9153\n",
      "Accuracy after query 141: 0.9142\n",
      "Accuracy after query 142: 0.9164\n",
      "Accuracy after query 143: 0.9157\n",
      "Accuracy after query 144: 0.9153\n",
      "Accuracy after query 145: 0.9157\n",
      "Accuracy after query 146: 0.9131\n",
      "Accuracy after query 147: 0.9142\n",
      "Accuracy after query 148: 0.9142\n",
      "Accuracy after query 149: 0.912\n",
      "Accuracy after query 150: 0.9138\n",
      "Accuracy after query 151: 0.9135\n",
      "Accuracy after query 152: 0.9142\n",
      "Accuracy after query 153: 0.9135\n",
      "Accuracy after query 154: 0.9164\n",
      "Accuracy after query 155: 0.9153\n",
      "Accuracy after query 156: 0.9135\n",
      "Accuracy after query 157: 0.9131\n",
      "Accuracy after query 158: 0.9131\n",
      "Accuracy after query 159: 0.9131\n",
      "Accuracy after query 160: 0.916\n",
      "Accuracy after query 161: 0.9168\n",
      "Accuracy after query 162: 0.9179\n",
      "Accuracy after query 163: 0.9211\n",
      "Accuracy after query 164: 0.9197\n",
      "Accuracy after query 165: 0.9219\n",
      "Accuracy after query 166: 0.9189\n",
      "Accuracy after query 167: 0.9171\n",
      "Accuracy after query 168: 0.9197\n",
      "Accuracy after query 169: 0.9197\n",
      "Accuracy after query 170: 0.9164\n",
      "Accuracy after query 171: 0.92\n",
      "Accuracy after query 172: 0.916\n",
      "Accuracy after query 173: 0.9193\n",
      "Accuracy after query 174: 0.9153\n",
      "Accuracy after query 175: 0.9168\n",
      "Accuracy after query 176: 0.9157\n",
      "Accuracy after query 177: 0.9211\n",
      "Accuracy after query 178: 0.9186\n",
      "Accuracy after query 179: 0.9179\n",
      "Accuracy after query 180: 0.9182\n",
      "Accuracy after query 181: 0.9193\n",
      "Accuracy after query 182: 0.9244\n",
      "Accuracy after query 183: 0.9292\n",
      "Accuracy after query 184: 0.9273\n",
      "Accuracy after query 185: 0.9325\n",
      "Accuracy after query 186: 0.9303\n",
      "Accuracy after query 187: 0.9288\n",
      "Accuracy after query 188: 0.9332\n",
      "Accuracy after query 189: 0.9336\n",
      "Accuracy after query 190: 0.9325\n",
      "Accuracy after query 191: 0.9303\n",
      "Accuracy after query 192: 0.9306\n",
      "Accuracy after query 193: 0.9317\n",
      "Accuracy after query 194: 0.935\n",
      "Accuracy after query 195: 0.9343\n",
      "Accuracy after query 196: 0.935\n",
      "Accuracy after query 197: 0.9343\n",
      "Accuracy after query 198: 0.9336\n",
      "Accuracy after query 199: 0.9354\n",
      "Accuracy after query 200: 0.9361\n",
      "Accuracy after query 201: 0.9354\n",
      "Accuracy after query 202: 0.9365\n",
      "Accuracy after query 203: 0.9376\n",
      "Accuracy after query 204: 0.9401\n",
      "Accuracy after query 205: 0.9401\n",
      "Accuracy after query 206: 0.9401\n",
      "Accuracy after query 207: 0.9434\n",
      "Accuracy after query 208: 0.9423\n",
      "Accuracy after query 209: 0.9412\n",
      "Accuracy after query 210: 0.9398\n",
      "Accuracy after query 211: 0.9423\n",
      "Accuracy after query 212: 0.9405\n",
      "Accuracy after query 213: 0.9409\n",
      "Accuracy after query 214: 0.9416\n",
      "Accuracy after query 215: 0.9419\n",
      "Accuracy after query 216: 0.9412\n",
      "Accuracy after query 217: 0.9405\n",
      "Accuracy after query 218: 0.9438\n",
      "Accuracy after query 219: 0.946\n",
      "Accuracy after query 220: 0.9478\n",
      "Accuracy after query 221: 0.946\n",
      "Accuracy after query 222: 0.9489\n",
      "Accuracy after query 223: 0.946\n",
      "Accuracy after query 224: 0.9471\n",
      "Accuracy after query 225: 0.9493\n",
      "Accuracy after query 226: 0.9485\n",
      "Accuracy after query 227: 0.9496\n",
      "Accuracy after query 228: 0.9493\n",
      "Accuracy after query 229: 0.9485\n",
      "Accuracy after query 230: 0.9493\n",
      "Accuracy after query 231: 0.9503\n",
      "Accuracy after query 232: 0.95\n",
      "Accuracy after query 233: 0.9485\n",
      "Accuracy after query 234: 0.9503\n",
      "Accuracy after query 235: 0.9496\n",
      "Accuracy after query 236: 0.95\n",
      "Accuracy after query 237: 0.9485\n",
      "Accuracy after query 238: 0.9489\n",
      "Accuracy after query 239: 0.9503\n",
      "Accuracy after query 240: 0.9514\n",
      "Accuracy after query 241: 0.9503\n",
      "Accuracy after query 242: 0.9518\n",
      "Accuracy after query 243: 0.9485\n",
      "Accuracy after query 244: 0.9503\n",
      "Accuracy after query 245: 0.9511\n",
      "Accuracy after query 246: 0.95\n",
      "Accuracy after query 247: 0.9525\n",
      "Accuracy after query 248: 0.9525\n",
      "Accuracy after query 249: 0.9533\n",
      "Accuracy after query 250: 0.9522\n",
      "Accuracy after query 251: 0.9544\n",
      "Accuracy after query 252: 0.9569\n",
      "Accuracy after query 253: 0.9555\n",
      "Accuracy after query 254: 0.9555\n",
      "Accuracy after query 255: 0.9555\n",
      "Accuracy after query 256: 0.9562\n",
      "Accuracy after query 257: 0.958\n",
      "Accuracy after query 258: 0.9587\n",
      "Accuracy after query 259: 0.9587\n",
      "Accuracy after query 260: 0.9587\n",
      "Accuracy after query 261: 0.958\n",
      "Accuracy after query 262: 0.9584\n",
      "Accuracy after query 263: 0.9598\n",
      "Accuracy after query 264: 0.9595\n",
      "Accuracy after query 265: 0.9595\n",
      "Accuracy after query 266: 0.9598\n",
      "Accuracy after query 267: 0.9591\n",
      "Accuracy after query 268: 0.9587\n",
      "Accuracy after query 269: 0.9591\n",
      "Accuracy after query 270: 0.9595\n",
      "Accuracy after query 271: 0.9598\n",
      "Accuracy after query 272: 0.962\n",
      "Accuracy after query 273: 0.962\n",
      "Accuracy after query 274: 0.9624\n",
      "Accuracy after query 275: 0.965\n",
      "Accuracy after query 276: 0.9631\n",
      "Accuracy after query 277: 0.9639\n",
      "Accuracy after query 278: 0.9635\n",
      "Accuracy after query 279: 0.9664\n",
      "Accuracy after query 280: 0.9653\n",
      "Accuracy after query 281: 0.9657\n",
      "Accuracy after query 282: 0.9668\n",
      "Accuracy after query 283: 0.965\n",
      "Accuracy after query 284: 0.965\n",
      "Accuracy after query 285: 0.965\n",
      "Accuracy after query 286: 0.9679\n",
      "Accuracy after query 287: 0.9675\n",
      "Accuracy after query 288: 0.9682\n",
      "Accuracy after query 289: 0.9693\n",
      "Accuracy after query 290: 0.9686\n"
     ]
    }
   ],
   "source": [
    "# 1st Training phase: Train a classifier on the first two labeled years of the data\n",
    "# 2nd Training phase: Use the classifier and batch active learning on the rest of the unlabeled data until 2021. Examples that would provide the most \n",
    "\n",
    "\n",
    "# Gaussian Naive Bayes isn't an option because the data distribution isn't gaussian/normal due to lacking a \"symmetric bell shape\". \n",
    "# Most of the data labels are on the high end of the scale. Thus, the data's bell shape isn't symmetric\n",
    "# Bernoulli Naive Bayes isn't an option because sample features must be binary-valued (Bernoulli, boolean)\n",
    "# Multinomial, Complement, and Categorical aren't considered  due to data being classified moreso out of probability rather than certainty.\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth)\n",
    "\n",
    "# classifier = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
    "\n",
    "# Start the classifier off by training it on the labeled dataset\n",
    "classifier = ActiveLearner(estimator=rf, X_training=clab_set, y_training=clab_set_label)\n",
    "\n",
    "batch_active_learning(classifier, np.copy(unlab_set), np.copy(unlab_set_label), ccsr, clabels, n_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/al_rand_forest2.pkl']"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(classifier, \"models/al_rand_forest2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8493975903614458"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st Evaluate phase: Use the newly trained classifier to evaluate the data from 2021 and 2022\n",
    "clf = joblib.load(\"models/al_rand_forest2.pkl\")\n",
    "accuracy = clf.score(np.concatenate(test_set, axis=0), np.concatenate(test_set_label, axis=0))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 5, 6, 7, 8, 9, 10}\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n",
      "{0, 1, 4, 5, 6, 7, 8, 9, 10}\n",
      "{0, 1, 2, 3, 5, 6, 7, 8, 9, 10}\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n",
      "{0, 1, 4, 5, 6, 7, 8, 9, 10}\n"
     ]
    }
   ],
   "source": [
    "print(set(labels[0]))\n",
    "print(set(labels[1]))\n",
    "print(set(labels[2]))\n",
    "\n",
    "print(set(lab_set_label[0]))\n",
    "print(set(lab_set_label[1]))\n",
    "print(set(lab_set_label[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN classifier class\n",
    "class RecurrentNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecurrentNeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=8, out_features=16)\n",
    "        # nn.ReLU() doesn't need parameters in this case\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(in_features=16, out_features=16)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(in_features=16, out_features=16)\n",
    "        self.activation3 = nn.ReLU()\n",
    "        # self.batchNorm = nn.BatchNorm1d()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        # self.dropout1 = nn.Dropout()\n",
    "        self.dense1 = nn.Linear(in_features=16, out_features=1)\n",
    "        # self.dropout2 = nn.Dropout()\n",
    "        # self.dense2 = nn.Linear()\n",
    "        # self.dropout3 = nn.Dropout()\n",
    "        # self.dense3 = nn.Linear()\n",
    "        # self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.activation3(x)\n",
    "        # x = self.batchNorm(x)\n",
    "        # x = self.flatten(x)\n",
    "        # x = self.dropout1(x)\n",
    "        x = self.dense1(x)\n",
    "        # x = self.dropout2(x)\n",
    "        # x = self.dense2(x)\n",
    "        # x = self.dropout3(x)\n",
    "        # x = self.dense3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RNN = RecurrentNeuralNetwork().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(RNN.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series forecasting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
