{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Andre\\Downloads\\BDS-Final-Project\\env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.saving import save_model\n",
    "from keras.models import load_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from modAL.models import ActiveLearner\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into DataFrames\n",
    "\n",
    "# These two groups of data will be used for label making\n",
    "# Emissions data\n",
    "nrg_emi_df = pd.read_excel(io=\"data/dataset/Statistical Review of World Energy Data.xlsx\", sheet_name=\"CO2 Emissions from Energy\", header=2, index_col=0)\n",
    "# The sheet called \"Natural Gas Flaring\" is already a part of the calculations for the sheet called \"CO2 from Flaring\"\n",
    "flar_emi_df = pd.read_excel(io=\"data/dataset/Statistical Review of World Energy Data.xlsx\", sheet_name=\"CO2 from Flaring\", header=2, index_col=0)\n",
    "equi_emi_df = pd.read_excel(io=\"data/dataset/Statistical Review of World Energy Data.xlsx\", sheet_name=\"CO2e Methane, Process emissions\", header=2, index_col=0)\n",
    "\n",
    "# Renewable energy production data\n",
    "hydro_pro_df = pd.read_excel(io=\"data/dataset/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Hydro Generation - TWh\", header=2, index_col=0)\n",
    "solar_pro_df = pd.read_excel(io=\"data/dataset/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Solar Generation - TWh\", header=2, index_col=0)\n",
    "wind_pro_df = pd.read_excel(io=\"data/dataset/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Wind Generation - TWh\", header=2, index_col=0)\n",
    "geo_pro_df = pd.read_excel(io=\"data/dataset/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Geo Biomass Other - TWh\", header=2, index_col=0)\n",
    "bio_pro_df = pd.read_excel(io=\"data/dataset/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Biofuels production - PJ\", header=2, index_col=0, nrows=47)\n",
    "\n",
    "# These three groups of data will be used for the feature sets\n",
    "# Renewable energy consumption data\n",
    "hydro_con_df = pd.read_excel(io=\"data/dataset/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Hydro Consumption - EJ\", header=2, index_col=0)\n",
    "solar_con_df = pd.read_excel(io=\"data/dataset/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Solar Consumption - EJ\", header=2, index_col=0)\n",
    "wind_con_df = pd.read_excel(io=\"data/dataset/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Wind Consumption - EJ\", header=2, index_col=0)\n",
    "geo_con_df = pd.read_excel(io=\"data/dataset/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Geo Biomass Other - EJ\", header=2, index_col=0)\n",
    "bio_con_df = pd.read_excel(io=\"data/dataset/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Biofuels consumption - PJ\", header=2, index_col=0, nrows=47)\n",
    "\n",
    "# Non-renewable energy consumption data\n",
    "oil_con_df = pd.read_excel(io=\"data/dataset/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Oil Consumption - EJ\", header=2, index_col=0)\n",
    "gas_con_df = pd.read_excel(io=\"data/dataset/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Gas Consumption - EJ\", header=2, index_col=0)\n",
    "coal_con_df = pd.read_excel(io=\"data/dataset/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Coal Consumption - EJ\", header=2, index_col=0)\n",
    "nuc_con_df = pd.read_excel(io=\"data/dataset/Statistical Review of World Energy Data.xlsx\", sheet_name=\"Nuclear Consumption - EJ\", header=2, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatic data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(df:pd.DataFrame, flag=False):\n",
    "    \"\"\"\n",
    "    Get an excel sheet ready for conversion to numpy arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): a dataframe containing an excel sheet\n",
    "    - flag (boolean): an indicator to convert PJ to EJ instead of kWh\n",
    "    \"\"\"\n",
    "    #------------------------------ \n",
    "    # Remove all irrelevant columns\n",
    "    #------------------------------\n",
    "\n",
    "    # Remove all data from before 1990\n",
    "    # Find the index of the \"1990\" column\n",
    "    drop_indx = list(df.columns).index(1990)\n",
    "    # Get the column labels of all columns left of \"1990\"\n",
    "    drop_cols = [df.columns[num] for num in np.arange(0, drop_indx)]\n",
    "    df = df.drop(columns=drop_cols)\n",
    "\n",
    "    # Remove data on growth-rate and share\n",
    "    # Get the column labels of the target columns\n",
    "    drop_cols = [df.columns[num] for num in [-3, -2, -1]]\n",
    "    df = df.drop(columns=drop_cols)\n",
    "\n",
    "    #---------------------------\n",
    "    # Remove all irrelevant rows\n",
    "    #---------------------------\n",
    "\n",
    "    # Remove all rows with any empty cells\n",
    "    # 0 doesn't make an empty cell\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Remove all \"Total\" and \"Other\" rows\n",
    "    # In addition, OECD, Non-OECD, the EU, and the USSR\n",
    "    # In addition, 8 other countries because they only appear in excel sheets for\n",
    "    # flaring emissions and nothing else. I can't make data samples for non-existent data\n",
    "    # Rationale for removing \"Other\" rows - some countries in some excel sheets appear\n",
    "    # individually, but are lumped into an \"Other\" row in other sheets.\n",
    "    # There's no possible way for me to know which portions of an\n",
    "    # \"Other\" row value belongs to which countries.\n",
    "    drop_rows = []\n",
    "    keywords = [\"Total\", \"Other\", \"OECD\", \"European Union\", \"USSR\", \"Bolivia\", \n",
    "                \"Bahrain\", \"Syria\", \"Yemen\", \"Libya\", \"Nigeria\", \"Brunei\", \"Myanmar\"]\n",
    "    for row in df.index:\n",
    "        # Mark a row for dropping if it contains any of the keywords\n",
    "        if any(keyword in row for keyword in keywords):\n",
    "            drop_rows.append(row)\n",
    "    df = df.drop(index=drop_rows)\n",
    "\n",
    "    # -----------------\n",
    "    # Convert the units\n",
    "    # -----------------\n",
    "\n",
    "    # This section is only performed on emissions data, \n",
    "    # renewable energy production data, and consumed\n",
    "    # biofuel energy data\n",
    "    # All other dataframes have \"Exajoules\" as their name\n",
    "    \n",
    "    # All CO2 data is currently represented as millions of tonnes\n",
    "    # Convert all produced renewable data to kilowatt-hour (kWh)\n",
    "    # 1 kWh = 3600 kJ\n",
    "    # 1 PJ = 1000000000000 kJ\n",
    "    # 1 TWh = 1000000000 kWh\n",
    "\n",
    "    if (df.index.name) == \"Million tonnes of carbon dioxide\":\n",
    "        # Convert to single tonnes\n",
    "        df = df * 1000000\n",
    "    elif df.index.name == \"Terawatt-hours\":\n",
    "        # Convert to kilowatt-hours\n",
    "        df = df * 1000000000\n",
    "    elif df.index.name == \"Petajoules\":\n",
    "        if flag:\n",
    "            # Convert to exajoules\n",
    "            df = df * 0.001\n",
    "        else:\n",
    "            # Convert to kilowatt-hours\n",
    "            df = df * (1000000000000/3600)\n",
    "\n",
    "    return df\n",
    "\n",
    "# tonnes = metric ton = 1000 kg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowIndices(df:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Return the row labels of a pd.DataFrame\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): a dataframe containing an excel sheet\n",
    "    \"\"\"\n",
    "\n",
    "    return [row for row in df.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dataframes\n",
    "\n",
    "# Unit: Tonnes\n",
    "nrg_emi_df = processData(nrg_emi_df)\n",
    "flar_emi_df = processData(flar_emi_df)\n",
    "equi_emi_df = processData(equi_emi_df)\n",
    "\n",
    "# Unit: Kilowatt-hours\n",
    "hydro_pro_df = processData(hydro_pro_df)\n",
    "solar_pro_df = processData(solar_pro_df)\n",
    "wind_pro_df = processData(wind_pro_df)\n",
    "geo_pro_df = processData(geo_pro_df)\n",
    "bio_pro_df = processData(bio_pro_df)\n",
    "\n",
    "# Unit: Exajoules\n",
    "hydro_con_df = processData(hydro_con_df)\n",
    "solar_con_df = processData(solar_con_df)\n",
    "wind_con_df = processData(wind_con_df)\n",
    "geo_con_df = processData(geo_con_df)\n",
    "bio_con_df = processData(bio_con_df, True)\n",
    "\n",
    "# Unit: Exajoules\n",
    "oil_con_df = processData(oil_con_df)\n",
    "gas_con_df = processData(gas_con_df)\n",
    "coal_con_df = processData(coal_con_df)\n",
    "nuc_con_df = processData(nuc_con_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "\n",
    "nrg_emi = nrg_emi_df.to_numpy()\n",
    "flar_emi = flar_emi_df.to_numpy()\n",
    "equi_emi = equi_emi_df.to_numpy()\n",
    "hydro_p = hydro_pro_df.to_numpy()\n",
    "solar_p = solar_pro_df.to_numpy()\n",
    "wind_p = wind_pro_df.to_numpy()\n",
    "geo_p = geo_pro_df.to_numpy()\n",
    "bio_p = bio_pro_df.to_numpy()\n",
    "\n",
    "hydro_c = hydro_con_df.to_numpy()\n",
    "solar_c = solar_con_df.to_numpy()\n",
    "wind_c = wind_con_df.to_numpy()\n",
    "geo_c = geo_con_df.to_numpy()\n",
    "bio_c = bio_con_df.to_numpy()\n",
    "oil_c = oil_con_df.to_numpy()\n",
    "gas_c = gas_con_df.to_numpy()\n",
    "coal_c = coal_con_df.to_numpy()\n",
    "nuc_c = nuc_con_df.to_numpy()\n",
    "\n",
    "# print(len(rowIndices(hydro_con_df)), len(rowIndices(solar_con_df)), len(rowIndices(wind_con_df)), len(rowIndices(geo_con_df)), len(rowIndices(bio_con_df)))\n",
    "# print(len(rowIndices(oil_con_df)), len(rowIndices(gas_con_df)), len(rowIndices(coal_con_df)), len(rowIndices(nuc_con_df)))\n",
    "# print(len(rowIndices(tol_con_df)))\n",
    "\n",
    "# Get row indices of dataframes\n",
    "# There are three unique indices/list of countries\n",
    "\n",
    "# All of these dataframes (and their NDarray equivalents) have 83 indices.\n",
    "# Their row indices are shown in nrg_emi_indices\n",
    "#\n",
    "# nrg_emi_df\n",
    "# equi_emi_df\n",
    "# hydro_pro_df\n",
    "# solar_pro_df\n",
    "# wind_pro_df\n",
    "# geo_pro_df\n",
    "# hydro_con_df\n",
    "# solar_con_df\n",
    "# wind_con_df\n",
    "# geo_con_df\n",
    "# oil_con_df\n",
    "# gas_con_df\n",
    "# coal_con_df\n",
    "# nuc_con_df\n",
    "\n",
    "# All of these dataframes (and their NDarray equivalents) have 41 indices\n",
    "# Their row indices are shown in flar_emi_indices\n",
    "# \n",
    "# flar_emi_df\n",
    "\n",
    "# All of these dataframes (and their NDarray equivalents) have 24 indices\n",
    "# Their row indices are shown in bio_indices\n",
    "#\n",
    "# bio_pro_df\n",
    "# bio_con_df\n",
    "\n",
    "# The rest of the dataframes share the same index list as nrg_emi_indices\n",
    "nrg_emi_indices = rowIndices(nrg_emi_df)\n",
    "flar_emi_indices = rowIndices(flar_emi_df)\n",
    "bio_indices = rowIndices(bio_pro_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>nrg_emi_indices:</b>\n",
    "\n",
    "['Canada', 'Mexico', 'US', 'Argentina', 'Brazil', 'Chile', 'Colombia', 'Ecuador', 'Peru', 'Trinidad & Tobago', 'Venezuela', 'Central America', 'Austria', 'Belgium', 'Bulgaria', 'Croatia', 'Cyprus', 'Czech Republic', 'Denmark', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Iceland', 'Ireland', 'Italy', 'Latvia', 'Lithuania', 'Luxembourg', 'Netherlands', 'North Macedonia', 'Norway', 'Poland', 'Portugal', 'Romania', 'Slovakia', 'Slovenia', 'Spain', 'Sweden', 'Switzerland', 'Turkey', 'Ukraine', 'United Kingdom', 'Azerbaijan', 'Belarus', 'Kazakhstan', 'Russian Federation', 'Turkmenistan', 'Uzbekistan', 'Iran', 'Iraq', 'Israel', 'Kuwait', 'Oman', 'Qatar', 'Saudi Arabia', 'United Arab Emirates', 'Algeria', 'Egypt', 'Morocco', 'South Africa', 'Eastern Africa', 'Middle Africa', 'Western Africa', 'Australia', 'Bangladesh', 'China', 'China Hong Kong SAR', 'India', 'Indonesia', 'Japan', 'Malaysia', 'New Zealand', 'Pakistan', 'Philippines', 'Singapore', 'South Korea', 'Sri Lanka', 'Taiwan', 'Thailand', 'Vietnam']\n",
    "\n",
    "<b>flar_emi_indices:</b>\n",
    "\n",
    "['Canada', 'Mexico', 'US', 'Argentina', 'Brazil', 'Colombia', 'Peru', 'Trinidad & Tobago', 'Venezuela', 'Denmark', 'Germany', 'Italy', 'Netherlands', 'Norway', 'Poland', 'Romania', 'Ukraine', 'United Kingdom', 'Azerbaijan', 'Kazakhstan', 'Russian Federation', 'Turkmenistan', 'Uzbekistan', 'Iran', 'Iraq', 'Kuwait', 'Oman', 'Qatar', 'Saudi Arabia', 'United Arab Emirates', 'Algeria', 'Egypt', 'Australia', 'Bangladesh', 'China', 'India', 'Indonesia', 'Malaysia', 'Pakistan', 'Thailand', 'Vietnam']\n",
    "\n",
    "<b>biofuel_indices:</b>\n",
    "\n",
    "['Canada', 'Mexico', 'US', 'Argentina', 'Brazil', 'Colombia', 'Austria', 'Belgium', 'Finland', 'France', 'Germany', 'Italy', 'Netherlands', 'Poland', 'Portugal', 'Spain', 'Sweden', 'United Kingdom', 'Australia', 'China', 'India', 'Indonesia', 'South Korea', 'Thailand']\n",
    "\n",
    "<b>Shape of nrg_emi:</b>\n",
    "\n",
    "(83, 33)\n",
    "\n",
    "<b>Columns of every dataframe:</b>\n",
    "\n",
    "Index([1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001,\n",
    "       2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013,\n",
    "       2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022],\n",
    "      dtype='object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Algeria', 'Argentina', 'Australia', 'Austria', 'Azerbaijan', 'Bangladesh', 'Belarus', 'Belgium', 'Brazil', 'Bulgaria', 'Canada', 'Central America', 'Chile', 'China', 'China Hong Kong SAR', 'Colombia', 'Croatia', 'Cyprus', 'Czech Republic', 'Denmark', 'Eastern Africa', 'Ecuador', 'Egypt', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Japan', 'Kazakhstan', 'Kuwait', 'Latvia', 'Lithuania', 'Luxembourg', 'Malaysia', 'Mexico', 'Middle Africa', 'Morocco', 'Netherlands', 'New Zealand', 'North Macedonia', 'Norway', 'Oman', 'Pakistan', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Qatar', 'Romania', 'Russian Federation', 'Saudi Arabia', 'Singapore', 'Slovakia', 'Slovenia', 'South Africa', 'South Korea', 'Spain', 'Sri Lanka', 'Sweden', 'Switzerland', 'Taiwan', 'Thailand', 'Trinidad & Tobago', 'Turkey', 'Turkmenistan', 'US', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'Uzbekistan', 'Venezuela', 'Vietnam', 'Western Africa']\n",
      "lsr shape: (33, 83, 8)\n"
     ]
    }
   ],
   "source": [
    "# Massive 3D numpy array for label making\n",
    "# 1st dimension - Years. 33 years from 1990-2022 (inclusive)\n",
    "# 2nd dimension - Countries/Regions. 83 unique countries/regions\n",
    "# 3rd dimension - Carbon Neutral features. 8 features (in this order): energy emissions, flaring emissions, CO2 equivalent emissions, \n",
    "# hydroelectric production, solar production, wind production, geothermal production, biofuel production\n",
    "# (33, 83, 8)\n",
    "\n",
    "# Find every unique country/region\n",
    "# This is a bit redundant because every country in flar_emi_indices and bio_indices \n",
    "# is already in nrg_emi_indices\n",
    "cotry_reg = list(set(nrg_emi_indices + flar_emi_indices + bio_indices))\n",
    "cotry_reg.sort()\n",
    "print(cotry_reg)\n",
    "\n",
    "dim_1 = []\n",
    "for year_indx in range(33):\n",
    "    dim_2 = []\n",
    "    for area in cotry_reg:\n",
    "        # There's no area check for the upcoming data\n",
    "        # because every area has this data\n",
    "        indx = nrg_emi_indices.index(area)\n",
    "        # Extract a float\n",
    "        a_nrg_emi = nrg_emi[indx][year_indx]\n",
    "        a_equi_emi = equi_emi[indx][year_indx]\n",
    "        a_hydro = hydro_p[indx][year_indx]\n",
    "        a_solar = solar_p[indx][year_indx]\n",
    "        a_wind = wind_p[indx][year_indx]\n",
    "        a_geo = geo_p[indx][year_indx]\n",
    "\n",
    "        if area in flar_emi_indices:\n",
    "            indx = flar_emi_indices.index(area)\n",
    "            # Extract a float\n",
    "            a_flar_emi = flar_emi[indx][year_indx]\n",
    "        else:\n",
    "            a_flar_emi = 0.\n",
    "\n",
    "        if area in bio_indices:\n",
    "            indx = bio_indices.index(area)\n",
    "            # Extract a float\n",
    "            a_biofuel = bio_p[indx][year_indx]\n",
    "        else:\n",
    "            a_biofuel = 0.\n",
    "\n",
    "        # Is also a set of features\n",
    "        dim_3 = [a_nrg_emi,\n",
    "                a_flar_emi,\n",
    "                a_equi_emi,\n",
    "                a_hydro,\n",
    "                a_solar,\n",
    "                a_wind,\n",
    "                a_geo,\n",
    "                a_biofuel]\n",
    "        dim_2.append(dim_3)\n",
    "    dim_1.append(dim_2)\n",
    "\n",
    "# Label Statistical Review \n",
    "# Full of floats\n",
    "lsr = np.array(dim_1)\n",
    "print(f\"lsr shape: {lsr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Algeria', 'Argentina', 'Australia', 'Austria', 'Azerbaijan', 'Bangladesh', 'Belarus', 'Belgium', 'Brazil', 'Bulgaria', 'Canada', 'Central America', 'Chile', 'China', 'China Hong Kong SAR', 'Colombia', 'Croatia', 'Cyprus', 'Czech Republic', 'Denmark', 'Eastern Africa', 'Ecuador', 'Egypt', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Japan', 'Kazakhstan', 'Kuwait', 'Latvia', 'Lithuania', 'Luxembourg', 'Malaysia', 'Mexico', 'Middle Africa', 'Morocco', 'Netherlands', 'New Zealand', 'North Macedonia', 'Norway', 'Oman', 'Pakistan', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Qatar', 'Romania', 'Russian Federation', 'Saudi Arabia', 'Singapore', 'Slovakia', 'Slovenia', 'South Africa', 'South Korea', 'Spain', 'Sri Lanka', 'Sweden', 'Switzerland', 'Taiwan', 'Thailand', 'Trinidad & Tobago', 'Turkey', 'Turkmenistan', 'US', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'Uzbekistan', 'Venezuela', 'Vietnam', 'Western Africa']\n",
      "csr shape: (33, 83, 9)\n",
      "ccsr shape: (2739, 9)\n"
     ]
    }
   ],
   "source": [
    "# Massive 3D numpy array for classification\n",
    "# 1st dimension - Years. 33 years from 1990-2022 (inclusive)\n",
    "# 2nd dimension - Countries/Regions. 83 unique countries/regions\n",
    "# 3rd dimension - Energy Consumption features. 9 features (in this order): oil, gas, coal, nuclear, \n",
    "# hydroelectric, solar, wind, geothermal, biofuel\n",
    "# (33, 83, 9)\n",
    "\n",
    "print(cotry_reg)\n",
    "\n",
    "dim_1 = []\n",
    "for year_indx in range(33):\n",
    "    dim_2 = []\n",
    "    for area in cotry_reg:\n",
    "        # There's no area check for the upcoming data\n",
    "        # because every area has this data\n",
    "        indx = nrg_emi_indices.index(area)\n",
    "        # Extract a float\n",
    "        a_hydro = hydro_c[indx][year_indx]\n",
    "        a_solar = solar_c[indx][year_indx]\n",
    "        a_wind = wind_c[indx][year_indx]\n",
    "        a_geo = geo_c[indx][year_indx]\n",
    "        a_oil = oil_c[indx][year_indx]\n",
    "        a_gas = gas_c[indx][year_indx]\n",
    "        a_coal = coal_c[indx][year_indx]\n",
    "        a_nuc = nuc_c[indx][year_indx]\n",
    "\n",
    "        if area in bio_indices:\n",
    "            indx = bio_indices.index(area)\n",
    "            # Extract a float\n",
    "            a_biofuel = bio_c[indx][year_indx]\n",
    "        else:\n",
    "            a_biofuel = 0.\n",
    "\n",
    "        # Is also a set of features\n",
    "        dim_3 = [a_oil, a_gas, a_coal, a_nuc,\n",
    "                 a_hydro, a_solar, a_wind, a_geo, a_biofuel]\n",
    "        dim_2.append(dim_3)\n",
    "    dim_1.append(dim_2)\n",
    "\n",
    "# Classification Statistical Review \n",
    "# Full of floats\n",
    "csr = np.array(dim_1)\n",
    "print(f\"csr shape: {csr.shape}\")\n",
    "\n",
    "# Concatenated version\n",
    "ccsr = np.concatenate(csr, axis=0)\n",
    "print(f\"ccsr shape: {ccsr.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeLabel(features):\n",
    "    \"\"\"\n",
    "    Make a label for a sample\n",
    "\n",
    "    - features (np.ndarray): set of 8 features\n",
    "    \"\"\"\n",
    "    # Unit: Tonnes of Carbon Dioxide\n",
    "    co2 = np.sum(features[:3])\n",
    "    # Unit: Kilowatt-hours\n",
    "    renewable = np.sum(features[3:])\n",
    "\n",
    "    # Electricity reductions emission factor\n",
    "    # 0.000709 tonnes CO2/kWh\n",
    "    # Unit: Tonnes of Carbon Dioxide\n",
    "    renewable *= 0.000709\n",
    "\n",
    "    # Remaining co2 after being offset by renewable energy production\n",
    "    rem_co2 = max(co2 - renewable, 0)\n",
    "\n",
    "    if rem_co2 == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        percent = (rem_co2/co2) * 100\n",
    "        # Equivalent to np.floor(percent / 10)\n",
    "        # The label is the tens place of the percentage\n",
    "        # return int(np.floor(percent / 10))\n",
    "        if percent > 0.0 and percent <= 20.0:\n",
    "            label = 1\n",
    "        elif percent > 20.0 and percent <= 40.0:\n",
    "            label = 2\n",
    "        elif percent > 40.0 and percent <= 60.0:\n",
    "            label = 3\n",
    "        elif percent > 60.0 and percent <= 80.0:\n",
    "            label = 4\n",
    "        elif percent > 80.0 and percent <= 100.0:\n",
    "            label = 5\n",
    "        else:\n",
    "            label = 5\n",
    "        return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels shape: (33, 83)\n",
      "clabels shape: (2739,)\n"
     ]
    }
   ],
   "source": [
    "# Make labels for all of the data/samples/examples \n",
    "# An individual feature isn't a example, but a location in a particular year is\n",
    "# Thus, there are 33 * 83 = 2739 examples\n",
    "\n",
    "# There are 6 possible labels, 0-5\n",
    "# 0 means carbon neutral is achieved\n",
    "# 5 means the country is absolutely nowhere near carbon neutrality\n",
    "labels = np.array([[makeLabel(location) for location in year] for year in lsr])\n",
    "print(f\"labels shape: {labels.shape}\")\n",
    "\n",
    "# Concatenated version\n",
    "clabels = np.concatenate(labels, axis=0)\n",
    "print(f\"clabels shape: {clabels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled training subset, Years 1990-1991\n",
      "lab_set shape: (2, 83, 9)\n",
      "lab_set_label shape: (2, 83)\n",
      "clab_set shape: (166, 9)\n",
      "clab_set_label shape: (166,)\n",
      "\n",
      "Unlabeled training subset, Years 1992-2020\n",
      "unlab_set shape: (29, 83, 9)\n",
      "unlab_set_label shape: (29, 83)\n",
      "cunlab_set shape: (2407, 9)\n",
      "cunlab_set_label shape: (2407,)\n",
      "\n",
      "Test subset, Years 2021-2022\n",
      "test_set shape: (2, 83, 9)\n",
      "test_set_label shape: (2, 83)\n",
      "ctest_set shape: (166, 9)\n",
      "ctest_set_label shape: (166,)\n"
     ]
    }
   ],
   "source": [
    "# Split the big dataset into a three subsets: labeled training, unlabeled training, and test\n",
    "\n",
    "# Labeled training subset\n",
    "# Years 1990-1991\n",
    "lab_set = csr[0:2]\n",
    "lab_set_label = labels[0:2]\n",
    "\n",
    "# Unlabeled training subset\n",
    "# aka Pool\n",
    "# Years 1992-2020\n",
    "unlab_set = csr[2:31]\n",
    "unlab_set_label = labels[2:31]\n",
    "\n",
    "# Test subset\n",
    "# Years 2021-2022\n",
    "test_set = csr[31::]\n",
    "test_set_label = labels[31::]\n",
    "\n",
    "\n",
    "# Concatenated versions\n",
    "clab_set = np.concatenate(lab_set, axis=0)\n",
    "clab_set_label = np.concatenate(lab_set_label, axis=0)\n",
    "\n",
    "cunlab_set = np.concatenate(unlab_set, axis=0)\n",
    "cunlab_set_label = np.concatenate(unlab_set_label, axis=0)\n",
    "\n",
    "ctest_set = np.concatenate(test_set, axis=0)\n",
    "ctest_set_label = np.concatenate(test_set_label, axis=0)\n",
    "\n",
    "\n",
    "print(\"Labeled training subset, Years 1990-1991\")\n",
    "print(f\"lab_set shape: {lab_set.shape}\")\n",
    "print(f\"lab_set_label shape: {lab_set_label.shape}\")\n",
    "print(f\"clab_set shape: {clab_set.shape}\")\n",
    "print(f\"clab_set_label shape: {clab_set_label.shape}\\n\")\n",
    "\n",
    "print(\"Unlabeled training subset, Years 1992-2020\")\n",
    "print(f\"unlab_set shape: {unlab_set.shape}\")\n",
    "print(f\"unlab_set_label shape: {unlab_set_label.shape}\")\n",
    "print(f\"cunlab_set shape: {cunlab_set.shape}\")\n",
    "print(f\"cunlab_set_label shape: {cunlab_set_label.shape}\\n\")\n",
    "\n",
    "print(\"Test subset, Years 2021-2022\")\n",
    "print(f\"test_set shape: {test_set.shape}\")\n",
    "print(f\"test_set_label shape: {test_set_label.shape}\")\n",
    "print(f\"ctest_set shape: {ctest_set.shape}\")\n",
    "print(f\"ctest_set_label shape: {ctest_set_label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Plan\n",
    "- Baseline train phase: Train the classifier on all the labeled data, it's supervised learning\n",
    "- 1st Training phase: Train a new classifier on the first two labeled years of the data\n",
    "- 2nd Training phase: Use the classifier and batch active learning on the rest of the unlabeled data until 2021. Examples that would provide the most information will be chosen to get their true label and taught to the classifier\n",
    "- 1st Evaluate phase: Use the newly trained classifier to evaluate the data from 2021 and 2022\n",
    "- Predict phase (Predict plan): Use time series forecasting to predict a country's set of features until 2050\n",
    "- 2nd Evaluate phase: Use the classifier to predict levels of CN\n",
    "- Baseline Evaluate phase: Use the baseline classifier to predict levels of CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier parameters\n",
    "# These are different from real model parameters that are estimated by the model itself\n",
    "\n",
    "n_estimators = 500\n",
    "criterion = \"log_loss\"\n",
    "max_depth = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model accuracy on test data: 0.8493975903614458\n"
     ]
    }
   ],
   "source": [
    "# Baseline phase: Train the classifier on all the labeled data, it's supervised learning\n",
    "\n",
    "path = \"models/bal/learners/baseline.pkl\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    baseline = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth)\n",
    "\n",
    "    # Combine all labeled and unlabeled data\n",
    "    baseline_data = np.concatenate((clab_set, cunlab_set), axis=0)\n",
    "    baseline_labels = np.concatenate((clab_set_label, cunlab_set_label), axis=0)\n",
    "    # baseline_data.shape = (2573, 9)\n",
    "    # baseline_labels.shape = (2573,)\n",
    "\n",
    "    baseline.fit(baseline_data, baseline_labels)\n",
    "    print(f\"Baseline model training accuracy: {baseline.score(ccsr, clabels)}\")\n",
    "\n",
    "    joblib.dump(baseline, path)\n",
    "else:\n",
    "    baseline = joblib.load(path)\n",
    "\n",
    "print(f\"Baseline model accuracy on test data: {baseline.score(ctest_set, ctest_set_label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Active Learning\n",
    "def batch_active_learning(classifier: ActiveLearner, unlab_data, unlab_lab, all_data, all_labels, n_queries):\n",
    "    \"\"\"Train a classifier using batch active learning\n",
    "    \n",
    "    Parameters:\n",
    "    - classifier (ActiveLearner): a classifier from the scikit-learn (sklearn) module \n",
    "    - unlab_data (ndarray): the unlabeled dataset, shape=(29, 83, 9)\n",
    "    - unlab_lab (ndarray): the unlabled dataset's labels, shape=(29, 83)\n",
    "    - add_data (ndarray): all data in concatenated form, shape=(2739, 9)\n",
    "    - all_labels (ndarray): all data labels in concatenated form, shape=(2739,)\n",
    "    - n_queries (int): number of queries to make on each year of data\n",
    "    \"\"\"\n",
    "\n",
    "    count = 1\n",
    "    for year_data, year_label in zip(unlab_data, unlab_lab):\n",
    "        # year_data.shape = (83, 9)\n",
    "        # year_label.shape = (83,)\n",
    "        for _ in range(n_queries):\n",
    "            # Query based on uncertainty\n",
    "            query_index, _ = classifier.query(year_data)\n",
    "\n",
    "            # Retrieve the requested example and its label, and teach it to the classifier\n",
    "            example = year_data[query_index].reshape(1, -1)\n",
    "            example_label = year_label[query_index].reshape(1,)\n",
    "            # example.shape = (1, 9)\n",
    "            # example_label.shape = (9,)\n",
    "            classifier.teach(X=example, y=example_label)\n",
    "\n",
    "            # Remove the queried example and its label from the unlabeled datasets\n",
    "            year_data = np.delete(year_data, query_index, axis=0)\n",
    "            year_label = np.delete(year_label, query_index)\n",
    "\n",
    "            accuracy = classifier.score(all_data, all_labels)\n",
    "            print(f\"Accuracy after query {count}: {round(accuracy, 4)}\")\n",
    "\n",
    "            count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch active learning hyperparameters aka model parameters\n",
    "# These are different from real model parameters that are estimated by the model itself\n",
    "\n",
    "n_queries = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Training phase: Train a classifier on the first two labeled years of the data\n",
    "# 2nd Training phase: Use the classifier and batch active learning on the rest of the unlabeled data until 2021. Examples that would provide the most \n",
    "\n",
    "path = \"models/bal/learners/al_rand_forest7.pkl\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth)\n",
    "\n",
    "    # Start the classifier off by training it on the labeled dataset\n",
    "    classifier = ActiveLearner(estimator=rf, X_training=clab_set, y_training=clab_set_label)\n",
    "\n",
    "    batch_active_learning(classifier, np.copy(unlab_set), np.copy(unlab_set_label), ccsr, clabels, n_queries)\n",
    "\n",
    "    joblib.dump(classifier, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on test data: 0.8554216867469879\n"
     ]
    }
   ],
   "source": [
    "# 1st Evaluate phase: Use the newly trained classifier to evaluate the data from 2021 and 2022\n",
    "clf = joblib.load(\"models/bal/learners/al_rand_forest7.pkl\")\n",
    "accuracy = clf.score(ctest_set, ctest_set_label)\n",
    "print(f\"Model accuracy on test data: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict phase (Predict plan): Use time series forecasting to predict a country's set of features until 2050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Plan\n",
    "- 1st data processing phase: Separate data by country/region instead of by feature. For each country/region, split their data into 3/4 and 1/4, where 3/4 is the training set and 1/4 is the test set\n",
    "- Training forecast phase: For each country/region, apply a LSTM RNN model on the training data and test data\n",
    "- Evaluate phase: For each country/region, evaluate the validity of the model's forecasts\n",
    "- 2nd data processing phase: Keep the data intact as a whole, but make sure it's normalized\n",
    "- Final fitting phase: Make new models that are fitted on all of their country's data\n",
    "- Real forecast phase: For each country/region, make forecasts of the features for 2023-2050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to create new framings of time series problems given a desired length of inout and output sequences\n",
    "# This was taken from https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\t\"\"\"\n",
    "\tFrame a time series as a supervised learning dataset.\n",
    "\tArguments:\n",
    "\t\tdata: Sequence of observations as a list or NumPy array.\n",
    "\t\tn_in: Number of lag observations as input (X).\n",
    "\t\tn_out: Number of observations as output (y).\n",
    "\t\tdropnan: Boolean whether or not to drop rows with NaN values.\n",
    "\tReturns:\n",
    "\t\tPandas DataFrame of series framed for supervised learning.\n",
    "\t\"\"\"\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csr shape: (33, 83, 9)\n",
      "Time span: 1990-2022\n",
      "all_tsf shape: (83, 33, 9)\n"
     ]
    }
   ],
   "source": [
    "# 1st data processing phase: Separate data by country/region instead of by feature.\n",
    "\n",
    "# csr before: a list of snapshots of the world, with each snapshot showing countries' features during a particaulr year\n",
    "print(f\"csr shape: {csr.shape}\")\n",
    "# *_tsf after: a list of countries, with each country entry containing only their features throughout the years\n",
    "all_tsf = np.transpose(csr, [1, 0, 2])\n",
    "# train_tsf = np.transpose(csr[0:25], [1, 0, 2])\n",
    "# test_tsf = np.transpose(csr[25:], [1, 0, 2])\n",
    "\n",
    "print(\"Time span: 1990-2022\")\n",
    "print(f\"all_tsf shape: {all_tsf.shape}\")\n",
    "# print(\"Time span: 1990-2014\")\n",
    "# print(f\"train_tsf shape: {train_tsf.shape}\")\n",
    "# print(\"Time span: 2015-2022\")\n",
    "# print(f\"test_tsf shape: {test_tsf.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tsf shape = (83, 24, 18)\n",
      "test_tsf shape = (83, 7, 18)\n"
     ]
    }
   ],
   "source": [
    "# Data processing for time series forecasting\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_tsf = []\n",
    "test_tsf = []\n",
    "for country in all_tsf:\n",
    "    country_df = DataFrame(country, columns=[\"Oil\", \"Gas\", \"Coal\", \"Nuclear\", \n",
    "                                             \"Hydro\", \"Solar\", \"Wind\", \"Geothermal\", \"Biofuel\"])\n",
    "    # country_df.shape = (33, 9), 33 rows and 9 columns\n",
    "\n",
    "    # Normalize features for LSTM\n",
    "    scaled = scaler.fit_transform(country_df.values)\n",
    "    # scaled.shape = (33, 9)\n",
    "\n",
    "    # For each country/region, split their data into 3/4 and 1/4, \n",
    "    # where 3/4 is the training set and 1/4 is the test set\n",
    "    train = scaled[:25]\n",
    "    test = scaled[25:]\n",
    "\n",
    "    # Recall that the # of years dimension decreases by 1\n",
    "    # in series_to_supervised()\n",
    "    reframed_train = series_to_supervised(train).values\n",
    "    reframed_test = series_to_supervised(test).values\n",
    "    # reframed_*.shape = (# of years, 18), # of years rows and 18 columns\n",
    "    # columns 0-8: t-1 for each feature\n",
    "    # columns 9-17: t for each feature\n",
    "\n",
    "    train_tsf.append(reframed_train)\n",
    "    test_tsf.append(reframed_test)\n",
    "    \n",
    "train_tsf = np.array(train_tsf)\n",
    "test_tsf = np.array(test_tsf)\n",
    "\n",
    "print(f\"train_tsf shape = {train_tsf.shape}\")\n",
    "print(f\"test_tsf shape = {test_tsf.shape}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training forecast phase: For each country/region, apply a LSTM RNN model on the training data and test data\n",
    "# Time series forecast training\n",
    "\n",
    "for index in range(len(cotry_reg)):\n",
    "    # Runs for 83 cycles, one cycle for a country\n",
    "\n",
    "    path = f\"models/lstm/train_test/{cotry_reg[index]}_model.keras\"\n",
    "    if os.path.exists(path):\n",
    "        continue\n",
    "\n",
    "    print(f\"{cotry_reg[index]} model\")\n",
    "\n",
    "    # No putting the model creation code in a separate function\n",
    "    # I'll get warnings about potential damage\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences = True, input_shape=(1, 9)))\n",
    "    model.add(LSTM(units=50, return_sequences = False))\n",
    "    model.add(Dense(25))\n",
    "    model.add(Dense(9))\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_absolute_error\")\n",
    "\n",
    "    train: np.ndarray = train_tsf[index]\n",
    "    # train_tsf.shape = (24, 18)\n",
    "    train_X, train_y = train[:, :9], train[:, 9:]\n",
    "    # train_*.shape = (24, 9)\n",
    "\n",
    "    test: np.ndarray = test_tsf[index]\n",
    "    # test_tsf.shape = (7, 18)\n",
    "    test_X, test_y = test[:, :9], test[:, 9:]\n",
    "    # test_*.shape = (7, 9)\n",
    "\n",
    "    train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
    "    # train_X.shape = (24, 1, 9)\n",
    "    # test_X.shape = (7, 1, 9)\n",
    "\n",
    "    # Fit the network on the training data\n",
    "    model.fit(train_X, train_y, epochs=50, batch_size=10, \n",
    "              validation_data=(test_X, test_y), verbose=0, shuffle=False)\n",
    "\n",
    "    save_model(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate phase: For each country/region, evaluate the validity of the model's forecasts\n",
    "# Time series forecast evaluating\n",
    "\n",
    "rmse = []\n",
    "if not os.path.exists(\"data/generated_data/rmse.txt\"):\n",
    "    for index in range(len(cotry_reg)):\n",
    "        # Runs for 83 cycles, one cycle for a country\n",
    "        print(f\"For {cotry_reg[index]}:\")\n",
    "\n",
    "        path = f\"models/lstm/train_test/{cotry_reg[index]}_model.keras\"\n",
    "        model = load_model(path)\n",
    "\n",
    "        test: np.ndarray = test_tsf[index]\n",
    "        # test_tsf.shape = (7, 18)\n",
    "        test_X, test_y = test[:, :9], test[:, 9:]\n",
    "        # test_*.shape = (7, 9)\n",
    "\n",
    "        test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
    "        # test_X.shape = (7, 1, 9)\n",
    "        \n",
    "        # Predict on the test data\n",
    "        yhat = model.predict(test_X, verbose=0)\n",
    "        # type(yhat) = np.ndarray\n",
    "        # np.shape(yhat) = (7, 9)\n",
    "\n",
    "        # Invert the forecast's scaling\n",
    "        inv_yhat = scaler.inverse_transform(yhat)\n",
    "\n",
    "        # Invert the real data's scaling\n",
    "        inv_y = scaler.inverse_transform(test_y)\n",
    "\n",
    "        # Note: The objective is to MINIMIZE rmse\n",
    "        rmse.append(sqrt(mean_squared_error(inv_y, inv_yhat)))\n",
    "\n",
    "    for index in range(len(cotry_reg)):\n",
    "        print(f\"{cotry_reg[index]} rmse: {rmse[index]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_tsf shape = (83, 32, 18)\n"
     ]
    }
   ],
   "source": [
    "# 2nd data processing phase: Keep the data intact as a whole, but make sure it's normalized\n",
    "\n",
    "full_tsf = []\n",
    "for country in all_tsf:\n",
    "    country_df = DataFrame(country, columns=[\"Oil\", \"Gas\", \"Coal\", \"Nuclear\", \n",
    "                                             \"Hydro\", \"Solar\", \"Wind\", \"Geothermal\", \"Biofuel\"])\n",
    "    # country_df.shape = (33, 9), 33 rows and 9 columns\n",
    "\n",
    "    # Normalize features for LSTM\n",
    "    scaled = scaler.fit_transform(country_df.values)\n",
    "    # scaled.shape = (33, 9)\n",
    "\n",
    "    # Recall that the # of years dimension decreases by 1\n",
    "    # in series_to_supervised()\n",
    "    reframed = series_to_supervised(scaled).values\n",
    "    # reframed.shape = (# of years, 18), # of years rows and 18 columns\n",
    "    # columns 0-8: t-1 for each feature\n",
    "    # columns 9-17: t for each feature\n",
    "\n",
    "    full_tsf.append(reframed)\n",
    "\n",
    "full_tsf = np.array(full_tsf)\n",
    "\n",
    "print(f\"full_tsf shape = {full_tsf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final fitting phase: Make new models that are fitted on all of their country's data\n",
    "\n",
    "for index in range(len(cotry_reg)):\n",
    "    # Runs for 83 cycles, one cycle for a country\n",
    "\n",
    "    path = f\"models/lstm/full/full_{cotry_reg[index]}_model.keras\"\n",
    "    if os.path.exists(path):\n",
    "        continue\n",
    "\n",
    "    print(f\"Full {cotry_reg[index]} model\")\n",
    "\n",
    "    # No putting the model creation code in a separate function\n",
    "    # I'll get warnings about potential damage\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences = True, input_shape=(1, 9)))\n",
    "    model.add(LSTM(units=50, return_sequences = False))\n",
    "    model.add(Dense(25))\n",
    "    model.add(Dense(9))\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_absolute_error\")\n",
    "\n",
    "    full: np.ndarray = full_tsf[index]\n",
    "    # full.shape = (32, 18)\n",
    "    full_X, full_y = full[:, :9], full[:, 9:]\n",
    "    # full_*.shape = (32, 9)\n",
    "\n",
    "    full_X = np.reshape(full_X, (full_X.shape[0], 1, full_X.shape[1]))\n",
    "    # full_X.shape = (24, 1, 9)\n",
    "\n",
    "    # Fit the network on all the data\n",
    "    model.fit(full_X, full_y, epochs=50, batch_size=10, verbose=0, shuffle=False)\n",
    "\n",
    "    save_model(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forecasts shape: (83, 29, 9)\n"
     ]
    }
   ],
   "source": [
    "# Real forecast phase: For each country/region, make forecasts of the features for 2023-2050\n",
    "# 2023-2050 is 28 years, this does count predictions of 2050. \n",
    "# The LSTM models are one-step, so each model should run 28 times\n",
    "# If the models were multi-step, they could run fewer than 28 times, but they're not\n",
    "# These multiple one-steps align with traditional models of time series forecasting since\n",
    "# the latter uses a recursive approach for predictions. Meanwhile, machine learning models\n",
    "# of time series forecasting use a direct approach.\n",
    "\n",
    "def predict(model: Sequential, data, num_prediction, look_back):\n",
    "    \"\"\"Make one-step predictions for an amount of years\"\"\"\n",
    "    prediction_list = data[-look_back:]\n",
    "    # 28 cycles\n",
    "    for _ in range(num_prediction):\n",
    "        x = prediction_list[-look_back:]\n",
    "        # x.shape = (15, 1, 9)\n",
    "        out = model.predict(x, verbose=0)\n",
    "        # out.shape = (15, 9)\n",
    "        out = out[-1]\n",
    "        # out.shape = (9,)\n",
    "        # Turn negatives into zero\n",
    "        out = out.clip(min=0)\n",
    "        out = np.expand_dims(out, axis=[0,1])\n",
    "        # out.shape = (1, 1, 9)\n",
    "        prediction_list = np.append(prediction_list, out, axis=0)\n",
    "\n",
    "    # Remove dimension required for LSTM\n",
    "    prediction_list = np.reshape(prediction_list, \n",
    "                                 (prediction_list.shape[0], prediction_list.shape[2]))\n",
    "    prediction_list = prediction_list[look_back-1:]\n",
    "    # Undo normalization\n",
    "    prediction_list = scaler.inverse_transform(prediction_list)\n",
    "        \n",
    "    # predictions_list.shape = (43, 9)  \n",
    "    return prediction_list\n",
    "\n",
    "# Years into the future\n",
    "num_prediction = 28\n",
    "# aka # of lag observations to use\n",
    "look_back = 15\n",
    "\n",
    "forecasts = []\n",
    "if not os.path.exists(\"data/generated_data/forecasts.npy\"):\n",
    "    # Runs for 83 cycles\n",
    "    for index in range(len(cotry_reg)):\n",
    "        name = cotry_reg[index]\n",
    "        data = all_tsf[index]\n",
    "        # data.shape = (33, 9)\n",
    "        # Normalize data\n",
    "        data = scaler.fit_transform(data)\n",
    "        # Reshape for the LSTM model\n",
    "        data = np.reshape(data, (data.shape[0], 1, data.shape[1]))\n",
    "        # data.shape = (33, 1, 9)\n",
    "\n",
    "        path = f\"models/lstm/full/full_{name}_model.keras\"\n",
    "        model = load_model(path)\n",
    "\n",
    "        print(f\"Predicting for {name}...\")\n",
    "        forecasts.append(predict(model, data, num_prediction, look_back))\n",
    "\n",
    "    forecasts = np.array(forecasts)\n",
    "    print(f\"forecasts shape: {forecasts.shape}\")\n",
    "    np.save(\"data/generated_data/forecasts\", forecasts)\n",
    "else:\n",
    "    forecasts = np.load(\"data/generated_data/forecasts.npy\")\n",
    "    print(f\"forecasts shape: {forecasts.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample of the actual and predicted data\n",
    "\n",
    "columns = [\"Oil\", \"Gas\", \"Coal\", \"Nuclear\", \"Hydro\", \"Solar\", \"Wind\", \"Geothermal\", \"Biofuel\"]\n",
    "\n",
    "# Only show the forecasts of one country\n",
    "# Showing the forecast graphs of all countries would mean 747 graphs get printed\n",
    "# Index 69 is Switzerland\n",
    "country_index = 69\n",
    "\n",
    "name = cotry_reg[country_index]\n",
    "\n",
    "real = DataFrame(all_tsf[country_index], columns=columns)\n",
    "\n",
    "pred = DataFrame(forecasts[country_index], columns=columns)\n",
    "\n",
    "# print(real)\n",
    "# print()\n",
    "# print(pred)\n",
    "\n",
    "curt_range = np.array(real.index) + 1990\n",
    "futr_range = np.array(pred.index) + 2022\n",
    "\n",
    "for index, feature in enumerate(columns):\n",
    "    title = f\"Figure {index+31}. {name} - {feature} Consumption Forecast\"\n",
    "    plt.title(title)\n",
    "    plt.plot(curt_range, real[feature], label=\"Real\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.plot(futr_range, pred[feature], label=\"Forecast\")\n",
    "    plt.ylabel(\"Energy Consumption (EJ)\")\n",
    "    plt.grid(True, \"both\", \"both\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig(f\"data/graphs/{name}_{feature}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: Algeria\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Argentina\n",
      "[5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Australia\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Austria\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "Country: Azerbaijan\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Bangladesh\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Belarus\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Belgium\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Brazil\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Country: Bulgaria\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Canada\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "Country: Central America\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Chile\n",
      "[4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: China\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: China Hong Kong SAR\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Colombia\n",
      "[3 3 3 4 4 4 3 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "Country: Croatia\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Cyprus\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Czech Republic\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Denmark\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Eastern Africa\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "Country: Ecuador\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "Country: Egypt\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Estonia\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Finland\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: France\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Germany\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Greece\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Hungary\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Iceland\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Country: India\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Indonesia\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Iran\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Iraq\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Ireland\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Israel\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Italy\n",
      "[4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Japan\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Kazakhstan\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Kuwait\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Latvia\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Lithuania\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Luxembourg\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Malaysia\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Mexico\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Middle Africa\n",
      "[4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Morocco\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Netherlands\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: New Zealand\n",
      "[2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: North Macedonia\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Norway\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Country: Oman\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Pakistan\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Peru\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Philippines\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Poland\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Portugal\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Qatar\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Romania\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Russian Federation\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Saudi Arabia\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Singapore\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Slovakia\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Slovenia\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: South Africa\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: South Korea\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Spain\n",
      "[4 4 4 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Sri Lanka\n",
      "[4 4 4 4 4 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Sweden\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Country: Switzerland\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Country: Taiwan\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Thailand\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Trinidad & Tobago\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Turkey\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Turkmenistan\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: US\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Ukraine\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: United Arab Emirates\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: United Kingdom\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Uzbekistan\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Venezuela\n",
      "[4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Vietnam\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Western Africa\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "# 2nd Evaluate phase: Use the classifier to predict levels of CN\n",
    "\n",
    "for index, country in enumerate(cotry_reg):\n",
    "    print(f\"Country: {country}\")\n",
    "    \n",
    "    clf: RandomForestClassifier = joblib.load(\"models/bal/learners/al_rand_forest7.pkl\")\n",
    "    pred = clf.predict(forecasts[index])\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: Algeria\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Argentina\n",
      "[5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Australia\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Austria\n",
      "[2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "Country: Azerbaijan\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Bangladesh\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Belarus\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Belgium\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Brazil\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Country: Bulgaria\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Canada\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "Country: Central America\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Chile\n",
      "[4 4 4 4 4 4 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: China\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: China Hong Kong SAR\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Colombia\n",
      "[4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Croatia\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Cyprus\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Czech Republic\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Denmark\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "Country: Eastern Africa\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Ecuador\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 4]\n",
      "Country: Egypt\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Estonia\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Finland\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Country: France\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Germany\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Greece\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Hungary\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Iceland\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Country: India\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Indonesia\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Iran\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Iraq\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Ireland\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Israel\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Italy\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Japan\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Kazakhstan\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Kuwait\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Latvia\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Lithuania\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Luxembourg\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Malaysia\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Mexico\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Middle Africa\n",
      "[4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Morocco\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Netherlands\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: New Zealand\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Country: North Macedonia\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Norway\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Country: Oman\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Pakistan\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Peru\n",
      "[4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Philippines\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Poland\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Portugal\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "Country: Qatar\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Romania\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Russian Federation\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Saudi Arabia\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Singapore\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Slovakia\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Slovenia\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: South Africa\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: South Korea\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Spain\n",
      "[4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "Country: Sri Lanka\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Sweden\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Country: Switzerland\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2]\n",
      "Country: Taiwan\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Thailand\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Trinidad & Tobago\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Turkey\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Turkmenistan\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: US\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Ukraine\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: United Arab Emirates\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: United Kingdom\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Country: Uzbekistan\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Venezuela\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Vietnam\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Country: Western Africa\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "# Baseline Evaluate phase: Use the baseline classifier to predict levels of CN\n",
    "\n",
    "for index, country in enumerate(cotry_reg):\n",
    "    print(f\"Country: {country}\")\n",
    "    \n",
    "    clf: RandomForestClassifier = joblib.load(\"models/bal/learners/baseline.pkl\")\n",
    "    pred = clf.predict(forecasts[index])\n",
    "    print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
